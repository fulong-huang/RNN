{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f45fe0-6e50-4894-b79e-589023a3dd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b56d226-ac6f-42e5-8abe-cd0d6eed48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length, data=\"\"):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        self.data = \"Today we will be talking about RNN and LSTM. RNN stands for recurrent neural network, \" + \\\n",
    "                    \"the special property of RNN is its ability to use the results from the past to generate a new result. \" + \\\n",
    "                    \"LSTM stands for Long-Short Term Memory, \" + \\\n",
    "                    \"LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, \" + \\\n",
    "                    \"and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradients.\"\n",
    "        if len(data) != 0:\n",
    "            self.data = data\n",
    "        #self.fp = open(path, \"r\")\n",
    "        #self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        print(chars)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371906b3-1ba2-4292-902d-2a342ff63a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "            xs, hs, os, ycap = {}, {}, {}, {}\n",
    "            hs[-1] = np.copy(hprev)\n",
    "            for t in range(len(inputs)):\n",
    "                xs[t] = np.zeros((self.vocab_size,1))\n",
    "                xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "                hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "                os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "                ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "            return xs, hs, ycap\n",
    "        \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "            # backward pass: compute gradients going backwards\n",
    "            dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "            db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "            dhnext = np.zeros_like(hs[0])\n",
    "            for t in reversed(range(self.seq_length)):\n",
    "                dy = np.copy(ps[t])\n",
    "                #through softmax\n",
    "                dy[targets[t]] -= 1 # backprop into y\n",
    "                #calculate dV, dc\n",
    "                dV += np.dot(dy, hs[t].T)\n",
    "                dc += dc\n",
    "                #dh includes gradient from two sides, next cell and current output\n",
    "                dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "                # backprop through tanh non-linearity \n",
    "                dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "                db += dhrec\n",
    "                #calculate dU and dW\n",
    "                dU += np.dot(dhrec, xs[t].T)\n",
    "                dW += np.dot(dhrec, hs[t-1].T)\n",
    "                #pass the gradient from next cell to the next iteration.\n",
    "                dhnext = np.dot(self.W.T, dhrec)\n",
    "            # clip to mitigate exploding gradients\n",
    "            for dparam in [dU, dW, dV, db, dc]:\n",
    "                np.clip(dparam, -5, 5, out=dparam) \n",
    "            return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "            \"\"\"loss for a sequence\"\"\"\n",
    "            # calculate cross-entrpy loss\n",
    "            return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "                \n",
    "    def sample(self, h, seed_ix, n):\n",
    "            \"\"\"\n",
    "            sample a sequence of integers from the model\n",
    "            h is memory state, seed_ix is seed letter from the first time step\n",
    "            \"\"\"\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[seed_ix] = 1\n",
    "            ixes = []\n",
    "            for t in range(n):\n",
    "                h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "                y = np.dot(self.V, h) + self.c\n",
    "                p = np.exp(y)/np.sum(np.exp(y))\n",
    "                ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "                x = np.zeros((self.vocab_size,1))\n",
    "                x[ix] = 1\n",
    "                ixes.append(ix)\n",
    "            return ixes\n",
    "\n",
    "    def train(self, data_reader):\n",
    "            iter_num = 0\n",
    "            # threshold = 0.05 ####################### Thresholds #######################\n",
    "            threshold = 0.01 ####################### Thresholds #######################\n",
    "            smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "\n",
    "            ###########################################################\n",
    "            # converge_limit = 5000\n",
    "            # converge_count = 0\n",
    "            # min_loss = 100000\n",
    "            ###########################################################\n",
    "\n",
    "            while (smooth_loss > threshold):\n",
    "                # if smooth_loss < min_loss:\n",
    "                #     min_loss = smooth_loss\n",
    "                #     converge_count = 0\n",
    "                # else:\n",
    "                #     if converge_count >= converge_limit:\n",
    "                #         print(f\"Model seems to converge. Min loss: {min_loss}, Curr loss: {smooth_loss}\")\n",
    "                #         break \n",
    "                #     converge_count += 1\n",
    "                if data_reader.just_started():\n",
    "                    hprev = np.zeros((self.hidden_size,1))\n",
    "                inputs, targets = data_reader.next_batch()\n",
    "                xs, hs, ps = self.forward(inputs, hprev)\n",
    "                dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "                loss = self.loss(ps, targets)\n",
    "                self.update_model(dU, dW, dV, db, dc)\n",
    "                smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "                hprev = hs[self.seq_length-1]\n",
    "                if not iter_num%500:\n",
    "                    sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "                    print( ''.join(data_reader.ix_to_char[ix] for ix in sample_ix))\n",
    "                    print( \"\\n\\niter :%d, loss:%f\"%(iter_num, smooth_loss))\n",
    "                    #print( \"\\n\\niter :%d, loss:%f, best:%f\"%(iter_num, smooth_loss, min_loss))\n",
    "\n",
    "                iter_num += 1\n",
    "\n",
    "    def predict(self, data_reader, start, n):\n",
    "\n",
    "        #initialize input vector\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []        \n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            ixes.append(ix)\n",
    "            x[ix] = 0\n",
    "\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = np.zeros((self.vocab_size,1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529a4220-2fb7-4106-a5dc-a5ad535058e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', ' ', 'h', 't', 'M', 'e', 'o', 'x', 'y', 'i', 'S', 'v', 'u', 'n', 'm', 's', ',', 'g', 'l', 'c', 'k', 'f', 'd', 'N', '-', 'T', 'L', '/', 'p', 'r', 'b', 'w', '.', 'R']\n",
      "NrNuiw wy  wy weyi  al e dwyin nl ww lywlyp wnywbdwy Si aaw we ieyhyi,y ftyy wiyw  Neyysfwywl yv,n wywwvcLawLn  wdty  yyyccyy aneylyewwyewwyykyviwbva fbwwy  w ayynr ,yyy rcyxy eaylsnycstei  napiyiwyww\n",
      "\n",
      "\n",
      "iter :0, loss:88.158980\n",
      "ilagry ndeymre ataL andghg tre hitum by nohtind beu rngog atinkind rsRo nuaogset um tletitoai eeo. an. iha/R-y thadlnodt,t  bs inangSa g/u inN llko/ondite soltttagwealwutkrtswosgndv-t t tes liit eorue\n",
      "\n",
      "\n",
      "iter :500, loss:83.554907\n",
      "ongN i abd ierrtxeLSTM tes abk talhitg trm werorter. LSTMhm g lexul biling ds movery  kiinl be om RN nSluondlit t  fal lve ralofte monul lf mm iRNN RNmof foralney cspmhm m me pant. Lk, thtcht mom dsii\n",
      "\n",
      "\n",
      "iter :1000, loss:67.867849\n",
      "s and m pil LSTe pal ore wone te tvry lss r alv. . re fsmpm de latu m tit ger. rem RNN geres wey Rer. LSTM vo pavo RN tadilan nor rete loteshem a RNNoaty bil sN g gong mor m alM lormt be uatnerovfeRpa\n",
      "\n",
      "\n",
      "iter :1500, loss:57.761360\n",
      "opay, anding/ frm Mesulfto/ xer ve RNN nenling grad dy the ef lore rat rer ort wolk neurall caloRN, meme retw lsdkLenulsy LSTR tal ang-her/s frem res loe ranglong/teresing the the RNN les andue form m\n",
      "\n",
      "\n",
      "iter :2000, loss:46.822174\n",
      "re a nong LSdM is fsils. LSTM. hits a bemory, be w teplvuan be taving lofiexemorseseitiealourevanity hesmondty, L, g germ thaleRNN btng vandine te rerey rerpyfvarest tha g diting mopre m ilsy,erissti \n",
      "\n",
      "\n",
      "iter :2500, loss:41.916014\n",
      "omual RNN iss from mon abewisianad esplemor LSTRNits angw aoforual. LSTM isd abtse rb buleng/thmathe rest ladil nodit athehavk andl al bontg a Term mori fepr sutvere tre treory, LSTMM abdt ne tanpeng \n",
      "\n",
      "\n",
      "iter :3000, loss:32.705015\n",
      "alha halinalhath abvandity oe als and can alfor he cofre tadilgnand Landty orodin s foreplodg/s of tarky orodifg mathe me caspiteorpe grte remem ripand gcom trad thonat RNNl by, no its mom a morwor ae\n",
      "\n",
      "\n",
      "iter :3500, loss:23.314900\n",
      "om a RNN trmew resti g lopa perm tothe tanbecorpe bults frovint aesesore lfory te at RNN iSdum reinllb tandling lodils Rsthano g-term mes for ,e RNN btanaling ls handling iontill tsorult. LfM alsualdR\n",
      "\n",
      "\n",
      "iter :4000, loss:15.982693\n",
      "ane and aeories and can abs als from mondling andy ofvalox ability to ule grty winitanithem molking long-term meories and can frmom als aes from the pascitieyN and ffrre gro vandly tf RNN butyd nertse\n",
      "\n",
      "\n",
      "iter :4500, loss:11.170117\n",
      "omure pesuetking about RNN and LSTM. RNN stands fore res buthm tatk from mhe past to generathe pecmrom cf result. LSTM stands for Long-Short Term iest-tand hang/shing/shore thm to pandlibing/shitg tse\n",
      "\n",
      "\n",
      "iter :5000, loss:7.586560\n",
      "omure pent the erecufre tes als from the past to generate a fe bonplndl RNN is its about RNN and LSTM. RNN sthe ere tertre ne tands for Long-Short Term Memory, LSTM is a form of RNN but differs from t\n",
      "\n",
      "\n",
      "iter :5500, loss:5.228692\n",
      "sm tat angwlendll iemoriling lodiinl iSTom Rre sefrkeval ffort Temw forle frto techecmroe hetk, the special propert feories ands ioche talking about RNN and LSTM. RNN stands for recurrent neural netwo\n",
      "\n",
      "\n",
      "iter :6000, loss:3.664078\n",
      "omhare mem me ial preper yN italinaling long-term memory, and a more cRNNility to use the results from the past Long- anesw moreculop cort of pscilg thm to RNN bual xhm linend a nes ferm mo hecurrent \n",
      "\n",
      "\n",
      "iter :6500, loss:2.636038\n",
      "omhrerm memory, LSTM is alnofreore the tanRilndlb fal eseresty of RNN is its ability to use thuorth Tes frores and LSTM isivandstadifgorert memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :7000, loss:1.938739\n",
      "aan s ang analo aang-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can alform RNN stits ang anotherm ten Lest coNN \n",
      "\n",
      "\n",
      "iter :7500, loss:1.472836\n",
      "omure pesofterhe ralinanalinand hatitita fals ane alodilsnand LSTMes tecity to genertan s a foories avalinande complex algorithm to handle the long/short-term memories. LSTM is better at handliff .e t\n",
      "\n",
      "\n",
      "iter :8000, loss:1.155231\n",
      "omure ptck, anot/ com RNN but ding ss s pesplc. fewilse frspat te ther value representl LSTM anethe perifermhm Tom the pasita ne compls andl ne aands forert hinithong/she wewterkat ng long-the reprecl\n",
      "\n",
      "\n",
      "iter :8500, loss:0.935303\n",
      "omha halinaling long-term meories and can also resolve exploding/vanishing grads an also resolve exploding/vanishing grads abtm ieorty, and Memory, LSTM is a form of RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :9000, loss:0.777997\n",
      "omurerty of RNN iskitn RNN as. winat RNN stands for Lonpl RNN but differs from traditional RNN by having another value representing the long-term mom athe mepres bethaving vong-terky te ort  loplet ao\n",
      "\n",
      "\n",
      "iter :9500, loss:0.663831\n",
      "omurerty of RNN it mondl RNN bndit . for abot at handling long-term meories and can also resolve exploding/vanishing grads als andils frorty of RNN is its ability to use the results from the past to g\n",
      "\n",
      "\n",
      "iter :10000, loss:0.578133\n",
      "emorerm Memory, LSTM is a form of kre trt an orm of RNN but difxplepr pang-ghray wert memory, and a more complex algorithm to handle the long/short-th LSTM als aodinand the pectal property of RNN is i\n",
      "\n",
      "\n",
      "iter :10500, loss:0.512507\n",
      "omurers f om traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handlitha/ aon RNN but di\n",
      "\n",
      "\n",
      "iter :11000, loss:0.460702\n",
      "ang long-tarisnalibicithe merort net ore long/short-term memories. LSTM is better at handling long-term meoriesha handle tse ralinal RNN bbabity cor. hSTM seilsky, hat aonds for long-Short Term Memory\n",
      "\n",
      "\n",
      "iter :11500, loss:0.419541\n",
      "osuleuts tere term tadids frorue perpendils frort memore complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/ he ceor \n",
      "\n",
      "\n",
      "iter :12000, loss:0.384848\n",
      "omutremork, tandle tf orey, and chorty, and can.dy, and fal propllkiffemories. LSTM is better ats from the past thepletero cane can peum meor lod from the past to generate a new results ayf RNN nety g\n",
      "\n",
      "\n",
      "iter :12500, loss:4.551121\n",
      "ad a soN long Meural network, the special pecureplopere term of RNN but differs from tre lonT/shalg ande complertang term memory, LSTM is a form of RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :13000, loss:3.159648\n",
      "am meor fres inds fox ,s ispe hal be wNl bing lodif ess from thaditional Rcilil form of Rm mon abot nong-Short Term Memory, LSTM is a form sad about RNN and LSTM. RNN stands for recm memories. LSTM is\n",
      "\n",
      "\n",
      "iter :13500, loss:2.124113\n",
      "alo/uat thavanorithm to handle the long/short-term memories. LSTM is better andg Temmor at handling long-term meories and can also resolve exploding/vanishing grads along-term meories and can anfpres \n",
      "\n",
      "\n",
      "iter :14000, loss:1.453930\n",
      "ophe .N brents thmot eorieno value representing the long-term memory, and a mork, LSTM is a form of RNN but differs but diffesmom abs from the past to generate a new result. LSTM stands for Long-Short\n",
      "\n",
      "\n",
      "iter :14500, loss:1.024490\n",
      "am mrm uerm the long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle thory c\n",
      "\n",
      "\n",
      "iter :15000, loss:0.748657\n",
      "om arm uecoat ran be to tewory cor value representing the long-term memory, and a more complex algorithm to handly to als nellang andehat ands abe al the urepresefork, the soecial property of RNN is i\n",
      "\n",
      "\n",
      "iter :15500, loss:0.570050\n",
      "omplom ere long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads along-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :16000, loss:0.452977\n",
      "as aont ran be ererl bethe aen. he past te wene rte tong-term meories and can also resolve exploding/vanishing grads alon analing long-term meories and can also resolve exploding/vanishing grads along\n",
      "\n",
      "\n",
      "iter :16500, loss:0.374841\n",
      "omwt reprepecorurepleplpe meoriecradil the the long/short-term memories. LSTM is bettet ato herals rem willl thady to us g term Memory, LSTM is a form of RNN but differs frtm Linds the long/short-term\n",
      "\n",
      "\n",
      "iter :17000, loss:0.321393\n",
      "splsus it als from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM iong be rast to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN \n",
      "\n",
      "\n",
      "iter :17500, loss:0.283719\n",
      "om memorils bew rore cem teerm of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. L\n",
      "\n",
      "\n",
      "iter :18000, loss:0.256257\n",
      "alue represecofres erpeto rending iong- a LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and \n",
      "\n",
      "\n",
      "iter :18500, loss:0.235526\n",
      "omprrads analing valioanis alRNN urate tad the long/short-term memories. LSTM is better at handling to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs \n",
      "\n",
      "\n",
      "iter :19000, loss:0.219327\n",
      "omprsme but long/vang tong/va isal dsananitional RNN by having another value representing the long-term memory, and a more complex asm To handly to RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :19500, loss:0.206252\n",
      "omhan ofr at handling long-term meories and can also resolve exploding/vanishing grads along-term memory, and a more complex alsoriling long-term memory, LSTM is frw long-ta aew avs from the by nesran\n",
      "\n",
      "\n",
      "iter :20000, loss:0.195385\n",
      "osus dscuralking about RNN and LSTM. RNN stands for recurrent neural nalding abo gort feort memorine vanishing grads ale ranils form of RNN but differs from traditional RNN by having another value rep\n",
      "\n",
      "\n",
      "iter :20500, loss:0.186120\n",
      "omprrm bilils forpend at mondl RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term \n",
      "\n",
      "\n",
      "iter :21000, loss:0.178051\n",
      "os a morherm the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN itand calod forpe from theulils aong al fork, the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :21500, loss:0.170923\n",
      "osua ds forpat eresult. LSTM stae aong-Sfrm memory, and a more complex algoneranex reste RNN a RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use \n",
      "\n",
      "\n",
      "iter :22000, loss:0.164571\n",
      "omwxrm of RNN but diffrsat  forp, and a morm term meories and ral also res merm Memory, LSTM is a form of RNN but differs from traditional RNN by having abe wepleplopeerespand ffories and can also res\n",
      "\n",
      "\n",
      "iter :22500, loss:0.158704\n",
      "om fror l RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handlin\n",
      "\n",
      "\n",
      "iter :23000, loss:0.153328\n",
      "om xempress ty, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. L\n",
      "\n",
      "\n",
      "iter :23500, loss:0.148372\n",
      "osuat ne tadkinalibi is mexploding value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can al\n",
      "\n",
      "\n",
      "iter :24000, loss:0.143774\n",
      "osul RNN Lont neural nat RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSnM ability to use the results \n",
      "\n",
      "\n",
      "iter :24500, loss:0.139483\n",
      "omurert memory, LSTM is als morad to gondl us uts from th fort Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value represents Long-Short Term Memory, LSTM is a \n",
      "\n",
      "\n",
      "iter :25000, loss:0.135463\n",
      "osul RNN anotherevolg se ehe long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve exploding/van\n",
      "\n",
      "\n",
      "iter :25500, loss:0.131684\n",
      "om xem recus grd long/short-term memories. LSTM is better at handling long-term meories andican also resolve exploding/vanishing grads along-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :26000, loss:0.128121\n",
      "omhanethe ureerky the ere from the pasi Long-term meories and can also resolve exploding/vanishing grads along-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is\n",
      "\n",
      "\n",
      "iter :26500, loss:0.124760\n",
      "omw recult. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to ha\n",
      "\n",
      "\n",
      "iter :27000, loss:0.121584\n",
      "om frmp nal LSTM stands for Long-rerm meories and can ort bm generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another val\n",
      "\n",
      "\n",
      "iter :27500, loss:0.118619\n",
      "osule tresem te ue preslN tronit Tomre to tethals frorty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is for RN itg abl\n",
      "\n",
      "\n",
      "iter :28000, loss:0.115755\n",
      "oshity the lalua ieork, thes Lbitinabing/vanishing grads alon Lualkehe pacalve exploding/vanishing grads alon forpalg andicials form of RNN but differs from traditional RNN bo betchort res avm will be\n",
      "\n",
      "\n",
      "iter :28500, loss:0.113044\n",
      "opLevmerm long-term meories and can also resolve exploding/vanishing gradse ralgw result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :29000, loss:0.110413\n",
      "omparm wilil thaving grads tame respati Long-Short re pandl nes at  andl RNN vand and a mores and w to generate a new recurrent neuralnds beurerty Tivs mesorte t deng taeurerty of RNN is its abililina\n",
      "\n",
      "\n",
      "iter :29500, loss:0.107971\n",
      "osule bre res from traditional RNN by having another value representing the long-term memory, and a more complex ationdling and cul be terty term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :30000, loss:0.105601\n",
      "omurere feom mre ras avanaling LSTM. RNN stands for recurrent neural network, the special propecter ae RNN stands for recurrent neural network, the special property of RNN is its ability to use the re\n",
      "\n",
      "\n",
      "iter :30500, loss:0.103347\n",
      "osus abal new a moref use the results froprelelort, and cal bs ils isg taou aes fre tsepretlo Leni fe rseplep fepluthere pandl thaditional RNN by having another value representing the long-term memory\n",
      "\n",
      "\n",
      "iter :31000, loss:0.101173\n",
      "osul RNN indtandl fe ralks and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long\n",
      "\n",
      "\n",
      "iter :31500, loss:0.099099\n",
      "osusem the pasue peer Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle t\n",
      "\n",
      "\n",
      "iter :32000, loss:0.097106\n",
      "osus mom along-term meories and can also resolve exploding/vanishing grads anol a aonwing the long-term memory, and a more complex algorishing lodiindtTre thm tere meories and ranitye ato Teorist aong\n",
      "\n",
      "\n",
      "iter :32500, loss:0.095198\n",
      "osulenty Mewtad and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meoriefrt peurerm Lempresurtw can also res alen erad the long/short-term memor\n",
      "\n",
      "\n",
      "iter :33000, loss:0.093365\n",
      "omurecis better at or at handling long-term meories and can also resolve exploding/vanishing grads iofories. LSTM is better at handling long-term meories and can also resolve explon har NNN st conus l\n",
      "\n",
      "\n",
      "iter :33500, loss:0.091606\n",
      "oplelty having taning aang to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve e\n",
      "\n",
      "\n",
      "iter :34000, loss:0.089917\n",
      "omploere ciang lerm Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-tand ial ils mora, and a morex more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :34500, loss:0.088295\n",
      "opresentithe wils be als its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having\n",
      "\n",
      "\n",
      "iter :35000, loss:0.086737\n",
      "oshis terk torhes. LSTM is better at handling long-te ay of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN \n",
      "\n",
      "\n",
      "iter :35500, loss:0.085241\n",
      "orual. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anNling/vanishing grads alon ure tse rest the ris long we will be talking about RNN and LSTM. RNN sta\n",
      "\n",
      "\n",
      "iter :36000, loss:0.083804\n",
      "om memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads alon eravils andibuty hf use neulat hads from tha RNN M abst nes fropre ute te eNN but dinesps \n",
      "\n",
      "\n",
      "iter :36500, loss:0.082427\n",
      "emorere term rinitaninalinding g-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :37000, loss:0.081105\n",
      "omprert feort semory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the loplex algorithm to handle the long/short-term memerist te term to valofvalg gr as\n",
      "\n",
      "\n",
      "iter :37500, loss:0.079839\n",
      "osul RNN bettwolils fropresults frorils and a mory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a uo te use the results from t\n",
      "\n",
      "\n",
      "iter :38000, loss:0.078626\n",
      "oplet re con. cor also re can also resolve exploding/vanishing grads along-term meories and can also resolve exploding/vanishing grads als ional RNN by having another value representing the long-term \n",
      "\n",
      "\n",
      "iter :38500, loss:0.077467\n",
      "omurerky of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :39000, loss:0.076357\n",
      "omprere from thaditional RNN be hane hal dse tall form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :39500, loss:0.075294\n",
      "om iss its about RNN and LSTM. RNN stands fer Long-term meories and can also resolve exploding/vanishing grads fortes als frrm to us g-the will be talking about RNN and LSTM. RNN stands for recurrent \n",
      "\n",
      "\n",
      "iter :40000, loss:0.074267\n",
      "omurert forta form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to a adierexplong-term meories and can also ralenavan\n",
      "\n",
      "\n",
      "iter :40500, loss:0.073265\n",
      "salforo rert of tse rert of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is\n",
      "\n",
      "\n",
      "iter :41000, loss:0.072265\n",
      "osuse tands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more pfal x aong-Short Term Memory, LSTM\n",
      "\n",
      "\n",
      "iter :41500, loss:0.071259\n",
      "osusem rindling long-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve exploding/vanishing grads alon f risul Long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :42000, loss:0.070239\n",
      "omprem the pasue paew oor lcut tonil RNN long/short-term memories. LSTM is better at handling long-term meoriesul pangeres estilg more complex algorithm to handle the by having another value represent\n",
      "\n",
      "\n",
      "iter :42500, loss:0.069221\n",
      "osul RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory\n",
      "\n",
      "\n",
      "iter :43000, loss:0.068196\n",
      "osul RNN grm ianetories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads oorex ret to use the results frorty of RNN is its abililing term te palls angw renl\n",
      "\n",
      "\n",
      "iter :43500, loss:0.067194\n",
      "omureritine a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to he complex algorithm to handle the long/short-term\n",
      "\n",
      "\n",
      "iter :44000, loss:0.066233\n",
      "omurerty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term from the long/short-term memories. LSTM is better at handling long-term meorie\n",
      "\n",
      "\n",
      "iter :44500, loss:0.065266\n",
      "ostandty to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads onpl iSTong-term meories and can also resolve exploding/van\n",
      "\n",
      "\n",
      "iter :45000, loss:0.064349\n",
      "orial tands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :45500, loss:0.063467\n",
      "osulextands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :46000, loss:0.062582\n",
      "omurerty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :46500, loss:0.061737\n",
      "osusexrm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradshing byt neth rilgl ataof RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :47000, loss:0.060925\n",
      "os ist ay ha gondl RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm\n",
      "\n",
      "\n",
      "iter :47500, loss:0.060123\n",
      "osulents torhinds for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle t\n",
      "\n",
      "\n",
      "iter :48000, loss:0.059340\n",
      "omplom ere thm term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algoritherilt it urty of RNN is its ata\n",
      "\n",
      "\n",
      "iter :48500, loss:0.058579\n",
      "ompaemories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads ito ge ireciss te explgt and can be ter generate ats from the past to generate a newerengl te w\n",
      "\n",
      "\n",
      "iter :49000, loss:0.057837\n",
      "omurerty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :49500, loss:0.057109\n",
      "omparm of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more compl g meorr valioads frompl foris andty  a for sol erer ts ithe uem te special pr\n",
      "\n",
      "\n",
      "iter :50000, loss:0.056400\n",
      "osus mom along-term meories and cal Rrs alse rals natiay ofor LSTMN form ti ue willl the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from t\n",
      "\n",
      "\n",
      "iter :50500, loss:0.055729\n",
      "omparm winlt RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-the erult RNN by having another value represent\n",
      "\n",
      "\n",
      "iter :51000, loss:0.055129\n",
      "omprurtste toet recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form o\n",
      "\n",
      "\n",
      "iter :51500, loss:0.054480\n",
      "om memories. LSTM is better at handling long-term thaval network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term \n",
      "\n",
      "\n",
      "iter :52000, loss:0.053817\n",
      "om frmpor- ang grads be tre res from tre long- athe orte cathm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads of RN\n",
      "\n",
      "\n",
      "iter :52500, loss:0.053169\n",
      "om meort cianew result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :53000, loss:0.052552\n",
      "om memori se exelshing LNN alvexrian RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long\n",
      "\n",
      "\n",
      "iter :53500, loss:0.051984\n",
      "orial term memorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads bom the past to generate a new result. LSTM sta\n",
      "\n",
      "\n",
      "iter :54000, loss:0.051407\n",
      "om ferm Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. L\n",
      "\n",
      "\n",
      "iter :54500, loss:0.050824\n",
      "om frolgeraritinaling long-term thalt non. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads ormerm long-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :55000, loss:0.050259\n",
      "osusem tre RNN LSTMN complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads iom the pasuerereshandl ne tr\n",
      "\n",
      "\n",
      "iter :55500, loss:0.049717\n",
      "osulelty oaldlod abe rss tndils forilg-tathm to hanelvalinand a nong-term meories and cal Rrisus generate a new reng rerty of RNN is its abiling g-term memory, LSTM is a form of RNN but differs from t\n",
      "\n",
      "\n",
      "iter :56000, loss:0.049185\n",
      "osul moroperort, andialong-term meories and can also resolva monds LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads fort shat the long/short-term memories. L\n",
      "\n",
      "\n",
      "iter :56500, loss:0.048658\n",
      "osuse tands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle londl R\n",
      "\n",
      "\n",
      "iter :57000, loss:0.048142\n",
      "omparm mondling andls and adl be talking about RNN and LSTM. RNN stands for recurrent neural network, the special propert nesortf LSTM is a form of RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :57500, loss:0.047640\n",
      "osus Rbg tene ralt the past to generate a new renal newe complex algorithm to handle the long/shorta RNN Lont erm  es al RNN bethavingtalew a mory, and a more complex algoralkehaving another value rep\n",
      "\n",
      "\n",
      "iter :58000, loss:0.047148\n",
      "emprem te use the results from the past to generate a new recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands fo\n",
      "\n",
      "\n",
      "iter :58500, loss:0.046663\n",
      "omurevanaliof ral ehes atay  M gondl ng long-term meories and can also resolve exploding/vanishing grads borilg long-term memory, and a more complex atiodiling long-term meor efeorhe anetherevty, and \n",
      "\n",
      "\n",
      "iter :59000, loss:0.046189\n",
      "omparm me urecispmet orm of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is\n",
      "\n",
      "\n",
      "iter :59500, loss:0.045727\n",
      "ompfrm of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handlin\n",
      "\n",
      "\n",
      "iter :60000, loss:0.045273\n",
      "osual be tarkm toriy of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by \n",
      "\n",
      "\n",
      "iter :60500, loss:0.044822\n",
      "om mrm gonul RNN stands for Long-Shor rk, tandlod fofgererorit networ LSTM is als aong-Shoring/vanishing grads alon atands orm biniling long-term memory, and a more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :61000, loss:0.044389\n",
      "omparm wilul the long/short-term memories. LSTiris be tands fort re ors nethont e orty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term \n",
      "\n",
      "\n",
      "iter :61500, loss:0.043964\n",
      "osuall LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads orpecw semork, the special property of RNN is its ability to use the results from the past to generat\n",
      "\n",
      "\n",
      "iter :62000, loss:0.043537\n",
      "omprem the past to generate a new recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory\n",
      "\n",
      "\n",
      "iter :62500, loss:0.043134\n",
      "omparm of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at hanalin\n",
      "\n",
      "\n",
      "iter :63000, loss:0.042720\n",
      "osha itands andlioalb aSTM iort memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :63500, loss:0.042335\n",
      "om memories. LSTM is better at handling long-term meories and can also ers along-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term \n",
      "\n",
      "\n",
      "iter :64000, loss:0.041945\n",
      "ompfrm pitiane resty Tomplex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads along-term meories and can al\n",
      "\n",
      "\n",
      "iter :64500, loss:0.041550\n",
      "omurerty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a feories anal hall form of RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :65000, loss:0.041205\n",
      "andlertoeperereperepresenting the longng/thox at hanl sof result. LSTM s ands fox loni g memor syy Terorueurepresentindlding/vaninald folpecirson but ding/long/short-term memorm vang grandl for ecomrt\n",
      "\n",
      "\n",
      "iter :65500, loss:5.998636\n",
      "alspang beterthe past to generanexlong grads frm RNN stands for Long-Short memories. LSTM is better at handling long-term meories and can also resolve exploding/vaninand form of RNN but differs from t\n",
      "\n",
      "\n",
      "iter :66000, loss:4.236564\n",
      "aluprest network, the special property of RNN is its ability to use the results from tre long/short-term memories. LSTM is iong nesling nong-Short Term Memory, LSTM is a form of RNN but differs from t\n",
      "\n",
      "\n",
      "iter :66500, loss:2.721321\n",
      "orwe prot atn tarm g-overe te results from the past to generate a new result. LSTM syiod Ry, fnereor long/short-term memories. LSTM is better at handling long-term meories and can also resolvm RNN Lbo\n",
      "\n",
      "\n",
      "iter :67000, loss:1.751648\n",
      "boN LSTMwirading/lalinand a ofories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anothe als frovalinaling more al be talking about RNN and LSTM. RNN sta\n",
      "\n",
      "\n",
      "iter :67500, loss:1.141869\n",
      "bortetorherm of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM bs better at h\n",
      "\n",
      "\n",
      "iter :68000, loss:0.759768\n",
      "om mereN another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM vandly the long/short-term memories. LSTM is better at handling long\n",
      "\n",
      "\n",
      "iter :68500, loss:0.520054\n",
      "om memorilonis ano LSTM is better at had abfor morilvding/vang rend nefralinal RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term\n",
      "\n",
      "\n",
      "iter :69000, loss:0.369249\n",
      "om uerpandle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anot ran trepl LSTM stands for Long-Short Term Memory, LSTM is a \n",
      "\n",
      "\n",
      "iter :69500, loss:0.273796\n",
      "bot erecoresty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having\n",
      "\n",
      "\n",
      "iter :70000, loss:0.212802\n",
      "oriaving another value representing the long-term memory, and a more complex algorithm tori Londiby  ftpre RNN be berta new recurrent neural network, the special property of RNN is its ability to use \n",
      "\n",
      "\n",
      "iter :70500, loss:0.173290\n",
      "om prepls erepresenti Long-Short Term Memory, Ly LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads abity of RNN is its ability to use the results from the pas\n",
      "\n",
      "\n",
      "iter :71000, loss:0.147184\n",
      "orils tonds LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to ha\n",
      "\n",
      "\n",
      "iter :71500, loss:0.129452\n",
      "alkperty of RNN is its ability to use the results from the past to generate a new rong-term meories and can also resolve exploding/vanishing grads iofrer at handling long-term meories and can also res\n",
      "\n",
      "\n",
      "iter :72000, loss:0.116973\n",
      "oouere the panexreorilve exploding/vanishing grads anot rannds frore complex ang another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. L\n",
      "\n",
      "\n",
      "iter :72500, loss:0.107833\n",
      "aluls RNN andilg lalM atnor rong-trong-term meories and can also resolve exploding/vanishing grads anothe als frepre rent neural network, the special property of RNN is its ability to use the results \n",
      "\n",
      "\n",
      "iter :73000, loss:0.100854\n",
      "os a dsaandl LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anot ereprtl be teral the past to Lenul nSTMor grt at eralg another value representing the long\n",
      "\n",
      "\n",
      "iter :73500, loss:0.095313\n",
      "bot tong-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by can also reshaving another value representing the long-term memory, and a more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :74000, loss:0.090756\n",
      "als als f RNN but differs from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing th\n",
      "\n",
      "\n",
      "iter :74500, loss:0.086897\n",
      "ath realshand bet reepreplopertyforts from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form oorils nal for recurrent neural network, the special property of RN\n",
      "\n",
      "\n",
      "iter :75000, loss:0.083550\n",
      "alurals from the past to generate a new reng-term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm\n",
      "\n",
      "\n",
      "iter :75500, loss:0.080593\n",
      "als als fal pands for atw a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anong the lhe ral\n",
      "\n",
      "\n",
      "iter :76000, loss:0.077941\n",
      "alspres from the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads orpecial property of RNN is its ability to use the results from t\n",
      "\n",
      "\n",
      "iter :76500, loss:0.075537\n",
      "aluuret fror ere the tant nesvanishing grads anong re also reshaving groril to ge ust neural network, the special property of RNN is its ability to use the results from the past to generate a new resu\n",
      "\n",
      "\n",
      "iter :77000, loss:0.073337\n",
      "alupresland ae rats form mo perl be terusts from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another val\n",
      "\n",
      "\n",
      "iter :77500, loss:0.071310\n",
      "ories. LSTM is better at handling long-term meories and can also resolve exploding/vanishibe ast anndl RNN be alolgew at prouls form of RNN but differs from traditional RNN by having another value rep\n",
      "\n",
      "\n",
      "iter :78000, loss:0.069431\n",
      "als result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to ha\n",
      "\n",
      "\n",
      "iter :78500, loss:0.067681\n",
      "alupresefrals for aes and can also resolve exploding/vanishing grads aand a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also res\n",
      "\n",
      "\n",
      "iter :79000, loss:0.066045\n",
      "alue representing the long-term memory, and a more complex algorithm to handle the long/short-term memoriese rilonds anong-thetrecorte ciss fas ats moteral difgeral network, the special property of RN\n",
      "\n",
      "\n",
      "iter :79500, loss:0.064511\n",
      "olurent erlvecM anal oorils a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and \n",
      "\n",
      "\n",
      "iter :80000, loss:0.063069\n",
      "alulldiyf erm meor sndslit network, the spey, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle t\n",
      "\n",
      "\n",
      "iter :80500, loss:0.061710\n",
      "als recurrent neural network, the special propert neurits andifalRpl memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anoding/vanishing grads anoth\n",
      "\n",
      "\n",
      "iter :81000, loss:0.060425\n",
      "alur long/short-term memories. LSTM is better at handling long-term meories ana bat be talking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability t\n",
      "\n",
      "\n",
      "iter :81500, loss:0.059208\n",
      "als renlishing grads forty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :82000, loss:0.058052\n",
      "oluerenting theplong  analh and can also resolve exploding/vanishing grads anothe orkm Tovhy woriyform of RNN but differs from traditional RNN by having another value representing the long-term memory\n",
      "\n",
      "\n",
      "iter :82500, loss:0.056953\n",
      "alullRN bul be talking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands fr\n",
      "\n",
      "\n",
      "iter :83000, loss:0.055904\n",
      "os as for Leng-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :83500, loss:0.054903\n",
      "alus athones andl be tat the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding lodiene rils be talking about RNN and LSTM. RNN stands for recurrent \n",
      "\n",
      "\n",
      "iter :84000, loss:0.053945\n",
      "aloplep all be talking about RNN and LSTM. RNN stands for recurrent neural network, tho Lent Them cMlvalf result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditi\n",
      "\n",
      "\n",
      "iter :84500, loss:0.053027\n",
      "osul RNN another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodi\n",
      "\n",
      "\n",
      "iter :85000, loss:0.052146\n",
      "alullvilitaling long-term meories and can also resolve exploding/vanishing grads forty cior itsts iSTong-term meories and can also resolve exploding/vanishing grads anol LSTM ionul RNN by having anoth\n",
      "\n",
      "\n",
      "iter :85500, loss:0.051300\n",
      "alor atan s for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value represent network, the spectrso, but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :86000, loss:0.050485\n",
      "als als fermeries for renglre als and can also resolve exploding/vanishing grads anot ran also repalpl RNN vanul RNN a canu algprestht Torp fermerisont neural network, the special property of RNN is i\n",
      "\n",
      "\n",
      "iter :86500, loss:0.049700\n",
      "anals beTort calolong-vang re ato at handling long-term meories and can also resolve exploding/vanishing grads anong ghabyf RNN but differs from traditional RNN by having another value representing th\n",
      "\n",
      "\n",
      "iter :87000, loss:0.048944\n",
      "alcurept per LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory\n",
      "\n",
      "\n",
      "iter :87500, loss:0.048214\n",
      "aos ats from the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anot forpalt and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :88000, loss:0.047510\n",
      "als result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from trd ta urec espalking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RN\n",
      "\n",
      "\n",
      "iter :88500, loss:0.046828\n",
      "alspres from the past to generate a new renties. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :89000, loss:0.046170\n",
      "alspreploperecta o conelsut te aene aof RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memor\n",
      "\n",
      "\n",
      "iter :89500, loss:0.045532\n",
      "als als frem te uss from the past to generate a new renal torg-term meories and can also resolve exploding/vanishing grads fories al iN use the results from the past to generate a new result. LSTM sta\n",
      "\n",
      "\n",
      "iter :90000, loss:0.044915\n",
      "alue representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :90500, loss:0.044317\n",
      "aourerty of RNN is its abiliti iodulil network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a \n",
      "\n",
      "\n",
      "iter :91000, loss:0.043738\n",
      "anal cosul RNN grt om RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is bette\n",
      "\n",
      "\n",
      "iter :91500, loss:0.043176\n",
      "bst the rsuls frories and can sef pre rests from the past to generate a new ata form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more compl\n",
      "\n",
      "\n",
      "iter :92000, loss:0.042630\n",
      "als als from ti its from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long\n",
      "\n",
      "\n",
      "iter :92500, loss:0.042100\n",
      "osus RNN bedby ta RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having\n",
      "\n",
      "\n",
      "iter :93000, loss:0.041585\n",
      "aloprecor and chaval network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but di\n",
      "\n",
      "\n",
      "iter :93500, loss:0.041085\n",
      "bout RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representithe exploding/vanishing grads fortyfortand and a more\n",
      "\n",
      "\n",
      "iter :94000, loss:0.040598\n",
      "aluerewm Tore bot RNN ithe torty network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form o\n",
      "\n",
      "\n",
      "iter :94500, loss:0.040124\n",
      "orial be ralking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long\n",
      "\n",
      "\n",
      "iter :95000, loss:0.039663\n",
      "botrecorkuvang-term meories and can also resolve exploding/vanishing grads along-term meories and can alspand can also resolve exploding/vanishing grads abit ferm the panut rtands LSTM stands for Long\n",
      "\n",
      "\n",
      "iter :95500, loss:0.039213\n",
      "anerer craditional RNN vanuls LSTM stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short\n",
      "\n",
      "\n",
      "iter :96000, loss:0.038775\n",
      "ath reng rent nespal network, the special property of RNN is its ability to use the results from the las areom tremor Long-term meories and can also resolve exploding/vanishing grads anol neural Lfori\n",
      "\n",
      "\n",
      "iter :96500, loss:0.038348\n",
      "alur long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve exploding/vanishing grads anol neural\n",
      "\n",
      "\n",
      "iter :97000, loss:0.037932\n",
      "orial be talking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long\n",
      "\n",
      "\n",
      "iter :97500, loss:0.037526\n",
      "alul RNN andthecorte complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads avong-term meories and can al\n",
      "\n",
      "\n",
      "iter :98000, loss:0.037129\n",
      "orualking long-term meories and cad also resor comw recural bility to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs \n",
      "\n",
      "\n",
      "iter :98500, loss:0.036742\n",
      "alue representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :99000, loss:0.036364\n",
      "alullvalinaling more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/shiting g-term meories and can also resolve e\n",
      "\n",
      "\n",
      "iter :99500, loss:0.035994\n",
      "alul dboN Loding value representing the long-term memory, and a more complex algorithm to handle the long/short-tere rets f RNN but differs from traditiom abnite the urs from the past to generate a ne\n",
      "\n",
      "\n",
      "iter :100000, loss:0.035633\n",
      "alur long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads along-term memory, and a more complex algorithm to handle the long/short-term\n",
      "\n",
      "\n",
      "iter :100500, loss:0.035279\n",
      "alue representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :101000, loss:0.034934\n",
      "alul RNN iands be be terty of RNN is its ability to use the results from the past to generate a new result. LSTor at ore rilong tao property of RNN is its ability to use the results from the past to g\n",
      "\n",
      "\n",
      "iter :101500, loss:0.034596\n",
      "osual ter result. LSTM stands for Long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/van\n",
      "\n",
      "\n",
      "iter :102000, loss:0.034265\n",
      "orual be talking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long\n",
      "\n",
      "\n",
      "iter :102500, loss:0.033941\n",
      "osus RNN tadky Lforinetpal she ald LSTM aong-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :103000, loss:0.033623\n",
      "osus g talu. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anot ran bl t-. the past to generate a new recurrent neural network, the special property of RN\n",
      "\n",
      "\n",
      "iter :103500, loss:0.033312\n",
      "als als fropre rent ral bif rseto vang another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meorie\n",
      "\n",
      "\n",
      "iter :104000, loss:0.033008\n",
      "alullkme moris network, the special property of RNN is its ability to use the results from the past to generate a nem at mhe past to generate a new ren brty he use the results from the past to generat\n",
      "\n",
      "\n",
      "iter :104500, loss:0.032710\n",
      "alul RNN another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodi\n",
      "\n",
      "\n",
      "iter :105000, loss:0.032417\n",
      "orial be talking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long\n",
      "\n",
      "\n",
      "iter :105500, loss:0.032130\n",
      "ane renlis ang anotheculs frories and can also resolve exploding/vanishing grads along-term meorueralt aSTM is better at handling long-term meories and can also resolve exploding/vanishing grads ofreo\n",
      "\n",
      "\n",
      "iter :106000, loss:0.031849\n",
      "om RNNt Lut bef result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :106500, loss:0.031573\n",
      "analve ere long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anong-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :107000, loss:0.031302\n",
      "anullong-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term\n",
      "\n",
      "\n",
      "iter :107500, loss:0.031037\n",
      "alul RNN ial ionu-te resty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :108000, loss:0.030776\n",
      "am rerm meories and can also resolve exploding/vanishing grads anong-terty chalsd and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and \n",
      "\n",
      "\n",
      "iter :108500, loss:0.030520\n",
      "olue ehm term meoroe ermemorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anong grabsl ty to use the results \n",
      "\n",
      "\n",
      "iter :109000, loss:0.030269\n",
      "alus athinald nalibiTial the past tort ndlning about RNN and LSTM. RNN stands for Long-term meories and can also resolve exploding/vanishing grads ato gerevatinat RNN by having another value represent\n",
      "\n",
      "\n",
      "iter :109500, loss:0.030022\n",
      "alul RNN be bes and from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long\n",
      "\n",
      "\n",
      "iter :110000, loss:0.029780\n",
      "am preploperecals. LSTM. RNN stands LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :110500, loss:0.029542\n",
      "ads alting/she term meories and can also resolve exploding/vanishing grads fort form of RNN but differm meorty of RNN is its ability to use the results from the past to generate a new result. LSTM sta\n",
      "\n",
      "\n",
      "iter :111000, loss:0.029309\n",
      "alus atgon/vanishing grads anong-term fore ere tong-teom RNm tf use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from t\n",
      "\n",
      "\n",
      "iter :111500, loss:0.029079\n",
      "alurals b-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads fories als frm a form of RNN but differs from traditional RNN by having another val\n",
      "\n",
      "\n",
      "iter :112000, loss:0.028853\n",
      "alial the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anong grads\n",
      "\n",
      "\n",
      "iter :112500, loss:0.028632\n",
      "bort, the long-term meories and can also resolve exploding value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long\n",
      "\n",
      "\n",
      "iter :113000, loss:0.028414\n",
      "osus RNN rest neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but di\n",
      "\n",
      "\n",
      "iter :113500, loss:0.028199\n",
      "alullkiang/valior ashe RNN be alspermemom aSTore exploding/vanishing grads iofhe corpindsdilt a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :114000, loss:0.027988\n",
      "alul morherm of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at h\n",
      "\n",
      "\n",
      "iter :114500, loss:0.027781\n",
      "alul RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm\n",
      "\n",
      "\n",
      "iter :115000, loss:0.027577\n",
      "alue representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :115500, loss:0.027377\n",
      "alullkm Tial RNN snana RN ian be talking about RNN and LSTM. RNN stands for Long-Short chanenald LSTM. RNN stands for Long-Serale alng grm of RNN but differs from the past to generate a new result. LS\n",
      "\n",
      "\n",
      "iter :116000, loss:0.027179\n",
      "als als from the past to generate a new recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term \n",
      "\n",
      "\n",
      "iter :116500, loss:0.026985\n",
      "boure rtandling long-term meories and can also resolve exploding/vanishing grads iom aonul the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs \n",
      "\n",
      "\n",
      "iter :117000, loss:0.026794\n",
      "als als n uealies athe rsst to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term \n",
      "\n",
      "\n",
      "iter :117500, loss:0.026606\n",
      "alurlvs from the pasuete the pasue ralnd RNN bs be rets fne rul RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle t\n",
      "\n",
      "\n",
      "iter :118000, loss:0.026421\n",
      "anall LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle t\n",
      "\n",
      "\n",
      "iter :118500, loss:0.026239\n",
      "alul RNN andther value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodi\n",
      "\n",
      "\n",
      "iter :119000, loss:0.026059\n",
      "analve exploding/vanishing grads iofories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads orvecork. g-terey ofreso verprt st a fore complex algorithm to ha\n",
      "\n",
      "\n",
      "iter :119500, loss:0.025882\n",
      "alul RNN and LSTM. RNN stands at ort corslithe representing the long-term memory, and a more complex algorithm to handle the long/short-term te. LSTM. RNN stands for Long-Short Term Memory, LSTM is a \n",
      "\n",
      "\n",
      "iter :120000, loss:0.025708\n",
      "alul mom oresta and can biffersyf ut pending grads for che palt Leng al property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory\n",
      "\n",
      "\n",
      "iter :120500, loss:0.025536\n",
      "aox als fremathe mories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anodinald the long/short-term memories. LSTM is better at handling long-term meorie\n",
      "\n",
      "\n",
      "iter :121000, loss:0.025367\n",
      "alhe RNN isating/valinalding value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also res\n",
      "\n",
      "\n",
      "iter :121500, loss:0.025201\n",
      "alus athonerilts anal o a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. L\n",
      "\n",
      "\n",
      "iter :122000, loss:0.025036\n",
      "alul RNN iands and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :122500, loss:0.024875\n",
      "am preploperecals. LSTM. RNN stands for recurrent neuralnthe ral thm talilinalg nall RNN vang/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :123000, loss:0.024715\n",
      "osus ve ure pts RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at h\n",
      "\n",
      "\n",
      "iter :123500, loss:0.024558\n",
      "am rerectanting long-term meories and can also resolve exploding/vanishing grads along-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long\n",
      "\n",
      "\n",
      "iter :124000, loss:0.024403\n",
      "aeha RNN ls a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is bette\n",
      "\n",
      "\n",
      "iter :124500, loss:0.024250\n",
      "om rerty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :125000, loss:0.024099\n",
      "alorle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads fories al iSTopres ats frum to use the results from the past to generat\n",
      "\n",
      "\n",
      "iter :125500, loss:0.023950\n",
      "as along-term meories and can also resolve exploding/vanishing grads orpecial property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term \n",
      "\n",
      "\n",
      "iter :126000, loss:0.023803\n",
      "aloplep fal property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by \n",
      "\n",
      "\n",
      "iter :126500, loss:0.023658\n",
      "als res from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term lemories. LSTM is better at handling long-term meorie\n",
      "\n",
      "\n",
      "iter :127000, loss:0.023515\n",
      "alul ds from the past to generate rest netwerk, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a \n",
      "\n",
      "\n",
      "iter :127500, loss:0.023374\n",
      "om frolg-taris and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :128000, loss:0.023235\n",
      "alul koat nong-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :128500, loss:0.023098\n",
      "orial be talking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long\n",
      "\n",
      "\n",
      "iter :129000, loss:0.022962\n",
      "als ale caluperal . LSTMN but to kes. neories al iSTM s from the past to generate a new recurrent neural network, the special property of RNN is its ability to use the results from the past to generat\n",
      "\n",
      "\n",
      "iter :129500, loss:0.022829\n",
      "als als from the past to generate a new resulre te pres are tands at ralking abo the well bingals fropresults from tre rast to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a \n",
      "\n",
      "\n",
      "iter :130000, loss:0.022696\n",
      "osulevanexhpretre teruetterkertands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term \n",
      "\n",
      "\n",
      "iter :130500, loss:0.022566\n",
      "bom renulndining about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :131000, loss:0.022437\n",
      "osual the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads avong-term \n",
      "\n",
      "\n",
      "iter :131500, loss:0.022310\n",
      "osusty the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads iofewm uisul LSTM is a form of RNN but differs from traditional RNN by \n",
      "\n",
      "\n",
      "iter :132000, loss:0.022184\n",
      "alul RNN andther value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodi\n",
      "\n",
      "\n",
      "iter :132500, loss:0.022060\n",
      "ads an s and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm\n",
      "\n",
      "\n",
      "iter :133000, loss:0.021937\n",
      "als als frem tieury terore cont neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a \n",
      "\n",
      "\n",
      "iter :133500, loss:0.021816\n",
      "alul RNN andinex ang andther value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also res\n",
      "\n",
      "\n",
      "iter :134000, loss:0.021696\n",
      "bobeter fror tfeurist te tenul nSTM is better at handling long-term meories and can also resolve exploding/vanishing grads ormeshe past te treulLRNN bs from the past to generate a new recural be talki\n",
      "\n",
      "\n",
      "iter :134500, loss:0.021578\n",
      "os andtands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form o\n",
      "\n",
      "\n",
      "iter :135000, loss:0.021461\n",
      "alue represeres at RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and \n",
      "\n",
      "\n",
      "iter :135500, loss:0.021345\n",
      "alur loe ralg nald LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :136000, loss:0.021231\n",
      "ads alt rial lonal bits bt term meories and can also resolve ureresus at handling long-term meories and can also resolve exploding/vanishing grads along-term meor ef plcif ess from traditional RNN by \n",
      "\n",
      "\n",
      "iter :136500, loss:0.021118\n",
      "osusty the pase long-term meories and can also resels for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and \n",
      "\n",
      "\n",
      "iter :137000, loss:0.021007\n",
      "aluplep alle exploding/vanishing grads along-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodi\n",
      "\n",
      "\n",
      "iter :137500, loss:0.020896\n",
      "alupresefrads form of RNN but differs from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value rep\n",
      "\n",
      "\n",
      "iter :138000, loss:0.020787\n",
      "als als fremathe pasi long-term meories and can also resolve exploding/vanishing grads anes frorty, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long\n",
      "\n",
      "\n",
      "iter :138500, loss:0.020679\n",
      "alus atg the the lodgy halinalding about RNN and LSTM. RNN stands for Long-Shor frem ties it caal aal RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long\n",
      "\n",
      "\n",
      "iter :139000, loss:0.020573\n",
      "ane recult. LSTM alsolve wilvvands ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by \n",
      "\n",
      "\n",
      "iter :139500, loss:0.020467\n",
      "als als frem tieurecill ianet RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :140000, loss:0.020363\n",
      "alur long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads forty, at handling long-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :140500, loss:0.020260\n",
      "als res from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meorie\n",
      "\n",
      "\n",
      "iter :141000, loss:0.020158\n",
      "orial be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :141500, loss:0.020057\n",
      "orial be ralking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long\n",
      "\n",
      "\n",
      "iter :142000, loss:0.019957\n",
      "alullkianile thevlong/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anoling/vanishing grads orpecial property of RNN is its ability t\n",
      "\n",
      "\n",
      "iter :142500, loss:0.019858\n",
      "alur long/vang-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :143000, loss:0.019760\n",
      "aehal tands for Long-term meories and can also resolve exploding/vanishing grads or erialt the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vao\n",
      "\n",
      "\n",
      "iter :143500, loss:0.019664\n",
      "alul RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm\n",
      "\n",
      "\n",
      "iter :144000, loss:0.019568\n",
      "olut RNN ano LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads orpecial property of RNN is its ability to use the results from the past to generate a new resu\n",
      "\n",
      "\n",
      "iter :144500, loss:0.019473\n",
      "alul RNN grt of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at h\n",
      "\n",
      "\n",
      "iter :145000, loss:0.019379\n",
      "alul . LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads ormeries. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :145500, loss:0.019286\n",
      "osus LSTM a thm more complex algorithm to handle the long/short-term memories. LSTM is better at hial be talking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RN\n",
      "\n",
      "\n",
      "iter :146000, loss:0.019195\n",
      "als als frers from the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads bils from traditional RNN by having another value represent\n",
      "\n",
      "\n",
      "iter :146500, loss:0.019104\n",
      "alolte exploding/vanishing grads forty, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :147000, loss:0.019014\n",
      "ane reculrevalinat RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and \n",
      "\n",
      "\n",
      "iter :147500, loss:0.018924\n",
      "anulle he special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :148000, loss:0.018836\n",
      "alulldyy Tars from the pase ralil for Les LSTM stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to ge ers from the past to generate a ne\n",
      "\n",
      "\n",
      "iter :148500, loss:0.018749\n",
      "als als from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory\n",
      "\n",
      "\n",
      "iter :149000, loss:0.018662\n",
      "osul by having anovandl for result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN bym the the palt Long-term meories and can also resolve exploding/van\n",
      "\n",
      "\n",
      "iter :149500, loss:0.018577\n",
      "alul RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm\n",
      "\n",
      "\n",
      "iter :150000, loss:0.018492\n",
      "alul RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm\n",
      "\n",
      "\n",
      "iter :150500, loss:0.018408\n",
      "am xrmporus be retperalts from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing th\n",
      "\n",
      "\n",
      "iter :151000, loss:0.018324\n",
      "als ang bete te ses from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long\n",
      "\n",
      "\n",
      "iter :151500, loss:0.018242\n",
      "om xemdiy we will be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory\n",
      "\n",
      "\n",
      "iter :152000, loss:0.018160\n",
      "alul RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm\n",
      "\n",
      "\n",
      "iter :152500, loss:0.018079\n",
      "ane recult. LSTM stands for Long-term meories and can also resolve exploding/vanishing grads along-term meore restsofrories ands and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN \n",
      "\n",
      "\n",
      "iter :153000, loss:0.017999\n",
      "alul RNN and LSTM. RNN lts tr tandlodg bertandl be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value rep\n",
      "\n",
      "\n",
      "iter :153500, loss:0.017920\n",
      "om rerty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :154000, loss:0.017841\n",
      "ads annl he results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long\n",
      "\n",
      "\n",
      "iter :154500, loss:0.017763\n",
      "osus Rtw getorhe pate resul LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term\n",
      "\n",
      "\n",
      "iter :155000, loss:0.017686\n",
      "orial be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :155500, loss:0.017610\n",
      "anal RNN and LSTM. RNN btands for rent rest neuralnds Lua ands fort meorte complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve e\n",
      "\n",
      "\n",
      "iter :156000, loss:0.017534\n",
      "alus atgonerate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more compl\n",
      "\n",
      "\n",
      "iter :156500, loss:0.017459\n",
      "orualking long-term meories and can also resolve exploding/vanishing grads iofories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads along-term meor ere loe\n",
      "\n",
      "\n",
      "iter :157000, loss:0.017384\n",
      "alullRNN is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is bette\n",
      "\n",
      "\n",
      "iter :157500, loss:0.017310\n",
      "als ars forme herpat ort andl bul RNNilg/vanishing grads forkinds be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having\n",
      "\n",
      "\n",
      "iter :158000, loss:0.017237\n",
      "olurent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs \n",
      "\n",
      "\n",
      "iter :158500, loss:0.017165\n",
      "als ans frories at handling long-term meories and can also resolve exploding/vanishing grads forty, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long\n",
      "\n",
      "\n",
      "iter :159000, loss:0.017093\n",
      "alul RNN and LSTM. RNN sthe long-term meories and can also resolve exploding/vanishing grads iom andl be ralking/shitg walioreweral tho al RNN vanulleproperty of RNN is its ability to use the results \n",
      "\n",
      "\n",
      "iter :159500, loss:0.017021\n",
      "alul RNN iands foorte result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the lodg netrts at RNN by having anoth\n",
      "\n",
      "\n",
      "iter :160000, loss:0.016951\n",
      "als als frem tieurist core cemweral ding/shong/vanishing grads fortyfofor from the past to generate resefrom the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form o\n",
      "\n",
      "\n",
      "iter :160500, loss:0.016881\n",
      "osus LSwM and can also resolve exploding/vanishing grads along-term meorky of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM\n",
      "\n",
      "\n",
      "iter :161000, loss:0.016811\n",
      "aourevane henlithe rals be rals fremotiex ing long-term meories and can also resolve exploding/vanishing grads along-term memory, and a more complex algorithm to handle the long/short-term memories. L\n",
      "\n",
      "\n",
      "iter :161500, loss:0.016743\n",
      "anal nos fnolgehe tands for resul LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :162000, loss:0.016674\n",
      "om rerty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :162500, loss:0.016607\n",
      "anall Lewor athorthe tadbe tereprestathe rem remeries for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands fo\n",
      "\n",
      "\n",
      "iter :163000, loss:0.016539\n",
      "aor recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but di\n",
      "\n",
      "\n",
      "iter :163500, loss:0.016473\n",
      "osuere the panexrm mere ty, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term\n",
      "\n",
      "\n",
      "iter :164000, loss:0.016407\n",
      "aluprenlts atw oor LSdiling long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :164500, loss:0.016341\n",
      "als ats f use nesphetre terpathm te RNN is its ability to use the results from the past long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value represent\n",
      "\n",
      "\n",
      "iter :165000, loss:0.016277\n",
      "alul RNN and LSTM. RNN btay ne aSTories ands and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term \n",
      "\n",
      "\n",
      "iter :165500, loss:0.016212\n",
      "as asl iSTM as bertyr atal better at will iefrm te retplox abilg vatinaling long-term meories and can also resolve exploding/vanishing grads ane ands fofres its abou repls art alol itadiag along-term \n",
      "\n",
      "\n",
      "iter :166000, loss:0.016148\n",
      "alul ds frories ana having grads bethm we handle the shiting the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads orpecial property\n",
      "\n",
      "\n",
      "iter :166500, loss:0.016085\n",
      "alul RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm\n",
      "\n",
      "\n",
      "iter :167000, loss:0.016022\n",
      "alul RNN and LSTM. RNN btands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory\n",
      "\n",
      "\n",
      "iter :167500, loss:0.015960\n",
      "alur valonalng ge alox about  LSTM ssanfom the past to generate a new recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LS\n",
      "\n",
      "\n",
      "iter :168000, loss:0.015898\n",
      "oral RNN and LSTM. RNN bt ral be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long\n",
      "\n",
      "\n",
      "iter :168500, loss:0.015837\n",
      "alill term memor represefork, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands fork, the special property of RNN is its ability to use \n",
      "\n",
      "\n",
      "iter :169000, loss:0.015776\n",
      "as as be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :169500, loss:0.015716\n",
      "alul RNN andibf al iSTM is better at handling/long-term meories and can also resolve exploding/vanishing grads forty, LSTM is a form of RNN but differs from traditional RNN by having another value rep\n",
      "\n",
      "\n",
      "iter :170000, loss:0.015656\n",
      "osul RNN and LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads ormerm long-term meories and can also resolve exploding/vanishing grads along-term meories and \n",
      "\n",
      "\n",
      "iter :170500, loss:0.015596\n",
      "orial be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :171000, loss:0.015537\n",
      "anal chexralt having ands from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handlin\n",
      "\n",
      "\n",
      "iter :171500, loss:0.015479\n",
      "als als fresalinat coave exploding/vanishing grads anoding/vanishing grads fories als frrm of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :172000, loss:0.015421\n",
      "alur long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads fortyfofor from the past to generate a new recurrent neural network, the spec\n",
      "\n",
      "\n",
      "iter :172500, loss:0.015363\n",
      "aourert neslf urrectr at handling long-term meories and can also resolve exploding/vanishing grads along-term meort feories ands and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN \n",
      "\n",
      "\n",
      "iter :173000, loss:0.015306\n",
      "bor Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term\n",
      "\n",
      "\n",
      "iter :173500, loss:0.015249\n",
      "alul RNN iands and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :174000, loss:0.015193\n",
      "olut etmorkvendint winal RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meorie\n",
      "\n",
      "\n",
      "iter :174500, loss:0.015137\n",
      "alux alsorilg neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but di\n",
      "\n",
      "\n",
      "iter :175000, loss:0.015082\n",
      "als avs from the pas better at handling long-term meories and can also resolve exploding/vanishing grads anothe us g term memory, and a more complex algorithm to handle the long/short-term memories. L\n",
      "\n",
      "\n",
      "iter :175500, loss:0.015027\n",
      "osul tbe talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :176000, loss:0.014972\n",
      "alus athonelong/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads forty, and a more complex algorithm to handle the long/short-term memor\n",
      "\n",
      "\n",
      "iter :176500, loss:0.014918\n",
      "alulldyy ne RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :177000, loss:0.014864\n",
      "aor re pNN lte aem wenevate alking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term \n",
      "\n",
      "\n",
      "iter :177500, loss:0.014811\n",
      "am memories and/hre ral bl tertands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term \n",
      "\n",
      "\n",
      "iter :178000, loss:0.014758\n",
      "ane result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to ha\n",
      "\n",
      "\n",
      "iter :178500, loss:0.014705\n",
      "orial be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :179000, loss:0.014653\n",
      "aor recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but di\n",
      "\n",
      "\n",
      "iter :179500, loss:0.014601\n",
      "alue representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :180000, loss:0.014550\n",
      "oraave the loew recural ding value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also res\n",
      "\n",
      "\n",
      "iter :180500, loss:0.014499\n",
      "ads als frepale aes from the past tort chang grads fore farm of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle t\n",
      "\n",
      "\n",
      "iter :181000, loss:0.014448\n",
      "alul RNN wets from the past to generate a new recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short\n",
      "\n",
      "\n",
      "iter :181500, loss:0.014397\n",
      "alul RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm\n",
      "\n",
      "\n",
      "iter :182000, loss:0.014347\n",
      "ories. LSTM. RNN stands he ureplopecils avalnor a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :182500, loss:0.014298\n",
      "alul RNN and LSTM. RNN /ts aneta kend neuralithang grads iofres and can also resolve exploding/vanishing grads ortecill property of RNN is its ability to use the results from the past to generate a ne\n",
      "\n",
      "\n",
      "iter :183000, loss:0.014249\n",
      "aor recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but di\n",
      "\n",
      "\n",
      "iter :183500, loss:0.014200\n",
      "aoxerecurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but di\n",
      "\n",
      "\n",
      "iter :184000, loss:0.014151\n",
      "alul RNN ands fooprem long-term meories and can also resolve exploding/vanishing grads orpeciss be cempresestinds for recurrent neural network, the special property of RNN is its ability to use tre re\n",
      "\n",
      "\n",
      "iter :184500, loss:0.014103\n",
      "alue representing tse renlg/wiog Me bte atkinds bet result. LSTM stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new resu\n",
      "\n",
      "\n",
      "iter :185000, loss:0.014055\n",
      "om resty oarils als frrm of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is\n",
      "\n",
      "\n",
      "iter :185500, loss:0.014007\n",
      "am pres ato tosk, the sN generate arn bute we ureerk, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM\n",
      "\n",
      "\n",
      "iter :186000, loss:0.013960\n",
      "alul RNN and LSTM. RNN btandly tf RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. L\n",
      "\n",
      "\n",
      "iter :186500, loss:0.013913\n",
      "alul RNN and LSTM. RNN btands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory\n",
      "\n",
      "\n",
      "iter :187000, loss:0.013866\n",
      "alux alsol nong andl ermepr lopy talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long\n",
      "\n",
      "\n",
      "iter :187500, loss:0.013820\n",
      "alul RNN and LSTM. RNN btands for recurrent neural network, the special property of RNN is its ability to use the rts LSTM is a form of RNN but differs from traditional RNN by having another value rep\n",
      "\n",
      "\n",
      "iter :188000, loss:0.013774\n",
      "alul RNN and LSTM. RNN btanalibi ind gandling long-term meories and can also resolvs it he explsN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :188500, loss:0.013728\n",
      "osus LSTM a theral the pasi nes frtvanishing grads for, neorterhands another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is bette\n",
      "\n",
      "\n",
      "iter :189000, loss:0.013683\n",
      "osus LSTM andicses ands for reshindl ing about RNN lodits al iNN RNture tero thm te RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory\n",
      "\n",
      "\n",
      "iter :189500, loss:0.013638\n",
      "ads annll henlby torm to generate a newrame ralitionanali aong-Short network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long\n",
      "\n",
      "\n",
      "iter :190000, loss:0.013593\n",
      "alul RNN andiLSTM is better at handling long-term meories and can also resolve exploding/vanishing grads anol neural thetcorpreet re ufrolt neural network, the soecits is better at cha RNN ls a form o\n",
      "\n",
      "\n",
      "iter :190500, loss:0.013549\n",
      "an-terty ofrtcorulty tariing/lsof reprils. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads orpecial property of RNN is its ability to use the results from t\n",
      "\n",
      "\n",
      "iter :191000, loss:0.013505\n",
      "alul RNN grorth at handling long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :191500, loss:0.013461\n",
      "bot result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to ha\n",
      "\n",
      "\n",
      "iter :192000, loss:0.013417\n",
      "alul RNN and LSTM. RNN btanaving andthe urs from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another val\n",
      "\n",
      "\n",
      "iter :192500, loss:0.013374\n",
      "aourevanexrang-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also rese ses from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :193000, loss:0.013331\n",
      "bot Long-term meories and can also resolve exploding/vanishing grads iofhe corpandle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :193500, loss:0.013288\n",
      "anelrepertalt harials het frem to ith tad for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more compl\n",
      "\n",
      "\n",
      "iter :194000, loss:0.013246\n",
      "alul RNN and LSTM. RNN btanalibi iform te RNN is its about of RNN buata new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another val\n",
      "\n",
      "\n",
      "iter :194500, loss:0.013204\n",
      "als als fremathe pas for aad abot and cal be resting. fofrers from the past to generate alng-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value represent\n",
      "\n",
      "\n",
      "iter :195000, loss:0.013162\n",
      "alul RNN and LSTM. RNN btands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory\n",
      "\n",
      "\n",
      "iter :195500, loss:0.013120\n",
      "alul RNN and LSTM. RNN btandlifor renobr thm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads orpecial property of RN\n",
      "\n",
      "\n",
      "iter :196000, loss:0.013079\n",
      "anall bew are from the past to generate a new recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short\n",
      "\n",
      "\n",
      "iter :196500, loss:0.013038\n",
      "as as the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :197000, loss:0.012997\n",
      "anall iew are from the past to generate a new renal te wong-te rethe complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodi\n",
      "\n",
      "\n",
      "iter :197500, loss:0.012957\n",
      "omurereciavs era ty having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can al\n",
      "\n",
      "\n",
      "iter :198000, loss:0.012916\n",
      "ads ane cal property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by \n",
      "\n",
      "\n",
      "iter :198500, loss:0.012876\n",
      "aor recurrent neural network, the special property of RNN is its abilil bif sealinaling long-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve explodi\n",
      "\n",
      "\n",
      "iter :199000, loss:0.012837\n",
      "orial be talking about RNN and LSTM. RNNils LSTMNN lom annulb LSTM ioperstf ue recurs from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs \n",
      "\n",
      "\n",
      "iter :199500, loss:0.012797\n",
      "orse . LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads forty, LSTM is a form of RNN but differs from traditional RNN by having another value representing th\n",
      "\n",
      "\n",
      "iter :200000, loss:0.012758\n",
      "alul RNN and LSTM. RNN btanadianf ess from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is bette\n",
      "\n",
      "\n",
      "iter :200500, loss:0.012719\n",
      "alue representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-teray outhm to handle the long/short-term memories. LSTM is\n",
      "\n",
      "\n",
      "iter :201000, loss:0.012680\n",
      "orm long-term meories and can also resolve exploding/vanishing grads along-term meorils and aad anal loluerecor at of RNN but differs from traditional RNN by having another value representing the long\n",
      "\n",
      "\n",
      "iter :201500, loss:0.012642\n",
      "om rectands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :202000, loss:0.012603\n",
      "onul nSTM morult. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm\n",
      "\n",
      "\n",
      "iter :202500, loss:0.012565\n",
      "alor atno vang grong-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :203000, loss:0.012528\n",
      "als als fremathe los is blt and can sefralsefor at ortkt aond neurats aenehaditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :203500, loss:0.012490\n",
      "osus LSTM asti gemories. LSTM is al be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing th\n",
      "\n",
      "\n",
      "iter :204000, loss:0.012453\n",
      "aloplep orls corhat n ulal halking about RNN and LSTM. RNN stands for Long-Short Term iesortf RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :204500, loss:0.012415\n",
      "ads ands forils ands anothe pes frurals and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :205000, loss:0.012379\n",
      "aloll tero ta neories at handling long-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve exploding/vanishing grads of RNN is its ability to use the re\n",
      "\n",
      "\n",
      "iter :205500, loss:0.012342\n",
      "orial be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :206000, loss:0.012306\n",
      "alue representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :206500, loss:0.012269\n",
      "alur long/vang-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :207000, loss:0.012233\n",
      "alul RNN and LSTM. RNN btanand LSTM alg aong-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodi\n",
      "\n",
      "\n",
      "iter :207500, loss:0.012198\n",
      "alul RNN and LSTM. RNN btal tha iaa for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :208000, loss:0.012162\n",
      "bor Long-term meories and can also resolve exploding/vanishing grads fortyf ut pfeurist Long-term meories and can also resolve exploding/vanishing grads orvecm wores Lenabd RNN and LSTM. RNN stands fo\n",
      "\n",
      "\n",
      "iter :208500, loss:0.012127\n",
      "alul RNN ifort Te als and can alspang balitional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at h\n",
      "\n",
      "\n",
      "iter :209000, loss:0.012092\n",
      "als als from the past to generate a new renal tong-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more compl\n",
      "\n",
      "\n",
      "iter :209500, loss:0.012057\n",
      "olul not- Long-thory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memor\n",
      "\n",
      "\n",
      "iter :210000, loss:0.012022\n",
      "om xe rtecorty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having\n",
      "\n",
      "\n",
      "iter :210500, loss:0.011987\n",
      "als als frem tieurestandty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :211000, loss:0.011953\n",
      "oshalhing loding/value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodi\n",
      "\n",
      "\n",
      "iter :211500, loss:0.011919\n",
      "beutere term oorisning abouterty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditi\n",
      "\n",
      "\n",
      "iter :212000, loss:0.011885\n",
      "als als fremathe rasus abing abol RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. L\n",
      "\n",
      "\n",
      "iter :212500, loss:0.011851\n",
      "oluere exploding/vanishing grads anong anta form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :213000, loss:0.011818\n",
      "alus atgonerate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more compl\n",
      "\n",
      "\n",
      "iter :213500, loss:0.011784\n",
      "als als frepale als frorty, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term\n",
      "\n",
      "\n",
      "iter :214000, loss:0.011751\n",
      "aourevty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anoth\n",
      "\n",
      "\n",
      "iter :214500, loss:0.011718\n",
      "alur long/vang-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :215000, loss:0.011686\n",
      "als als from tho ls a senwing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/van\n",
      "\n",
      "\n",
      "iter :215500, loss:0.011653\n",
      "alul RNN and LSTM. RNN btanalibi iadils and from the prst to gendity talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having\n",
      "\n",
      "\n",
      "iter :216000, loss:0.011621\n",
      "am pres anal RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can al\n",
      "\n",
      "\n",
      "iter :216500, loss:0.011589\n",
      "alur lt having anothe long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :217000, loss:0.011557\n",
      "alill termemorils anoxhecthe past re will be talking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generat\n",
      "\n",
      "\n",
      "iter :217500, loss:0.011525\n",
      "alierecor RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handlin\n",
      "\n",
      "\n",
      "iter :218000, loss:0.011493\n",
      "als als frem ta ithol thang aong-Short Ters from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another val\n",
      "\n",
      "\n",
      "iter :218500, loss:0.011462\n",
      "als als from the past to generate a new renal tong-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more compl\n",
      "\n",
      "\n",
      "iter :219000, loss:0.011430\n",
      "orial be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :219500, loss:0.011399\n",
      "alul RNN and LSTM. RNN btanand LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads ormeri long-term meories and can also resolve exploding/vanishing grads along\n",
      "\n",
      "\n",
      "iter :220000, loss:0.011368\n",
      "alul RNN and LSTM. RNN btanand LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads orvaniliof ral bif sse the results frem rieurals ands foris network, the spec\n",
      "\n",
      "\n",
      "iter :220500, loss:0.011338\n",
      "ane result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to ha\n",
      "\n",
      "\n",
      "iter :221000, loss:0.011307\n",
      "alul RNN iands and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :221500, loss:0.011277\n",
      "olul nt hal aSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to ha\n",
      "\n",
      "\n",
      "iter :222000, loss:0.011246\n",
      "alolve exploding/vanishing grads anong-terehe als frepals ands and LSTM. RNN stands LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another val\n",
      "\n",
      "\n",
      "iter :222500, loss:0.011216\n",
      "alur long-term memories ashal al RNNling/lodieto generathe rals ang bal be talking about RNN and LSTM. RNN stands for Long-term meories and can also resolve exploding/vanishing grads orpecial property\n",
      "\n",
      "\n",
      "iter :223000, loss:0.011186\n",
      "olueerty, and a and cal property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditi\n",
      "\n",
      "\n",
      "iter :223500, loss:0.011157\n",
      "alue representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :224000, loss:0.011127\n",
      "aloll terere uerulte tang bal be talking about RNN and LSTM. RNN stands for Long-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :224500, loss:0.011097\n",
      "alupreteral ding/vanishing grads orvane pest nemror dt hanaling long-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve exploding/vanishing grads iorpt\n",
      "\n",
      "\n",
      "iter :225000, loss:0.011068\n",
      "alul RNN and LSTM. RNN btanand the pa m more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads iofRN\n",
      "\n",
      "\n",
      "iter :225500, loss:0.011039\n",
      "aloll Texplod frem rieurhm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads forty, it hane blt- ang- ers from traditi\n",
      "\n",
      "\n",
      "iter :226000, loss:0.011010\n",
      "anal nos fre sillt the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads bothe aettere mepresett netrork, the special property of RN\n",
      "\n",
      "\n",
      "iter :226500, loss:0.010981\n",
      "adielpl i to generaliof RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by \n",
      "\n",
      "\n",
      "iter :227000, loss:0.010953\n",
      "ads ane ral bingesng trong-term meories and can also resolve exploding/vanishing grads anes fbepresultp ciropt tene res als aong-Short Term Memory, LSTM is a form of RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :227500, loss:0.010924\n",
      "alierecufre rte erecravitional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term \n",
      "\n",
      "\n",
      "iter :228000, loss:0.010896\n",
      "alul RNN and LSTM. RNN btanalibi ianalinand ial property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN \n",
      "\n",
      "\n",
      "iter :228500, loss:0.010868\n",
      "als als from the past to generate a new renal tong-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more compl\n",
      "\n",
      "\n",
      "iter :229000, loss:0.010840\n",
      "alur long/vang-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :229500, loss:0.010812\n",
      "alul RNN and LSTM. RNN bt ralining lals ant cal property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN \n",
      "\n",
      "\n",
      "iter :230000, loss:0.010784\n",
      "alul RNN and LSTM. RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and \n",
      "\n",
      "\n",
      "iter :230500, loss:0.010756\n",
      "als als frem tieurhe comp nespalinald gropaling long/vanishing grads bom the past vecurrent neural network, the special property of RNN is its ability to use the results from the past to generate a ne\n",
      "\n",
      "\n",
      "iter :231000, loss:0.010729\n",
      "eor lopifgrmeorr vanishing grads along-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve exploding/vanishing grads along-term meories and can also res\n",
      "\n",
      "\n",
      "iter :231500, loss:0.010702\n",
      "aluulepl bertand ang rem rieurilt and aavials andl re tof RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :232000, loss:0.010674\n",
      "alue representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :232500, loss:0.010647\n",
      "osus RNN the lex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve e\n",
      "\n",
      "\n",
      "iter :233000, loss:0.010620\n",
      "ads ands and LSTM. RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and \n",
      "\n",
      "\n",
      "iter :233500, loss:0.010594\n",
      "alureltecorve urem tx ato Metor ,e us from tr ut pending grads betrts at will be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional R\n",
      "\n",
      "\n",
      "iter :234000, loss:0.010567\n",
      "als als frem tieurest-tort cerm memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads\n",
      "\n",
      "\n",
      "iter :234500, loss:0.010541\n",
      "als als frelsyf RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at h\n",
      "\n",
      "\n",
      "iter :235000, loss:0.010514\n",
      "ories. LSTM. RNN stands LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :235500, loss:0.010488\n",
      "orial be talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :236000, loss:0.010462\n",
      "orueerals aris adm the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads along-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :236500, loss:0.010436\n",
      "osusty te opres Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :237000, loss:0.010410\n",
      "alur long/vang-term meories lodiy ne winll RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handlin\n",
      "\n",
      "\n",
      "iter :237500, loss:0.010385\n",
      "am rand nesparisu. ionul the past to generate res from the las at ore special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Shralinalin\n",
      "\n",
      "\n",
      "iter :238000, loss:0.010359\n",
      "als ats f RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handlin\n",
      "\n",
      "\n",
      "iter :238500, loss:0.010334\n",
      "olulllitha RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex alg\n",
      "\n",
      "\n",
      "iter :239000, loss:0.010308\n",
      "olute tands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the lon\n",
      "\n",
      "\n",
      "iter :239500, loss:0.010283\n",
      "als als frem tieures and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more compl\n",
      "\n",
      "\n",
      "iter :240000, loss:0.010258\n",
      "als als from the past to generate a new renal te will be rals ng/valioadinf result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another val\n",
      "\n",
      "\n",
      "iter :240500, loss:0.010233\n",
      "ors se ere lom Memory ty LSTM ise rore term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/van\n",
      "\n",
      "\n",
      "iter :241000, loss:0.010208\n",
      "alur value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :241500, loss:0.010184\n",
      "als avs from the past to generate ann nes. LSTM is better at handling long-term meories and can also resolve exploding/vanishing grads forkinds fox and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :242000, loss:0.010159\n",
      "bm to ereprecof RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at h\n",
      "\n",
      "\n",
      "iter :242500, loss:0.010135\n",
      "alul RNN bettor at hand iem avilinal iSforiss aading about RNN and LSTM. RNN stands for Long-term meories and can also resolve exploding/vanishing grads of RNN is its ability to use the results from t\n",
      "\n",
      "\n",
      "iter :243000, loss:0.010110\n",
      "als als frem ties LSTM stands forererm memories and and bults from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by \n",
      "\n",
      "\n",
      "iter :243500, loss:0.010086\n",
      "alolve exploding/vanishing grads iofwes and can also resolve exploding/vanishing grads forty, fong-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value rep\n",
      "\n",
      "\n",
      "iter :244000, loss:0.010062\n",
      "als ass fats ienel ger value representin/vands ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditi\n",
      "\n",
      "\n",
      "iter :244500, loss:0.010038\n",
      "als als frem tieurecane and ands fories als frrm of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/shor\n",
      "\n",
      "\n",
      "iter :245000, loss:0.010014\n",
      "Prediction 1 (memory): memory . LSTM is better at handling long-term meories and\n",
      "Prediction 2 (LSTM): LSTM re andling tore heralew ato LSTon but differs from\n",
      "Prediction 3 (complex): complex rs better at handling long-term meories and can al\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"input.txt\", seq_length)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "rnn.train(data_reader)\n",
    "\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'memory ', 50)\n",
    "print(f\"Prediction 1 (memory): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'LSTM ', 50)\n",
    "print(f\"Prediction 2 (LSTM): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'complex ', 50)\n",
    "print(f\"Prediction 3 (complex): {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b558ab-c271-4c98-ba87-675dde6faf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Special: LSTM is bet cilo preeterere halting about RNN and LSTM. RNN s\n"
     ]
    }
   ],
   "source": [
    "predicted = rnn.predict(data_reader, 'LSTM is bet', 50)\n",
    "print(f\"Prediction Special: {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e3ea0-3985-414b-bea1-ae3e58a01e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e3ef8b-dfe7-4587-99c0-5ece9941be5f",
   "metadata": {},
   "source": [
    "Train model with even easier input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546a6645-c2f0-4cd7-9933-65a4422ca904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', ' ', 'h', 't', 'e', 'o', 'y', 'i', 'v', 'u', 'n', 'm', 's', ',', 'g', 'l', 'c', 'f', 'd', 'T', 'p', 'r', 'w', '.']\n",
      "ofmsgs pscep,suopapu, rstfsTm Ttruuwrcdfh aslmt sfw.rsvestguThidlontogennnlopTrrmfusrel svys s hnfddhgccisvipp,rwulsmvghsn,nrT.evis rvuovTh,fwlu.dhpsdwgvol,pm,l..hslwh vnh ln ,vgevnpv.guvhagfevlngmcp \n",
      "\n",
      "\n",
      "iter :0, loss:79.451715\n",
      "withwithg, sopefully the model sirther simpler string, hopefull wtthg, simpler strinow  erfwrm well withg, hopefully ihe model simodel can now perform well withgipeimod t e model simpler string, hopef\n",
      "\n",
      "\n",
      "iter :500, loss:57.641133\n",
      "can now perform well withgrihg, hopefully the model can now perfors string, hopefully the model now perform well withan now perform well withg, hopefully the model can now perform welly the model can \n",
      "\n",
      "\n",
      "iter :1000, loss:35.364160\n",
      "ing, hopefully the model can now perform well within strinrm perform well withwithen now perform well withodefully the model can now perform well withg, hopefully the model can now perform well withg,\n",
      "\n",
      "\n",
      "iter :1500, loss:21.599300\n",
      "an now perform well wtthg, hopefuley the model can now perform well within hopefully the model can now perfsrm well withopefully the model can now perform well withg, hopefully the model can now perfo\n",
      "\n",
      "\n",
      "iter :2000, loss:13.191806\n",
      "can now perform well withan now perform well within now perform well within simpler string, hopefully the model can now perform well within now perform well withopefully the model can now perform well\n",
      "\n",
      "\n",
      "iter :2500, loss:8.066891\n",
      "ing, hopefully the mtdel can now perform well withimpler string, hopefully the model can now perform well within strinow perform well wwing, hopefully the model can now perform well within hopefully t\n",
      "\n",
      "\n",
      "iter :3000, loss:4.944064\n",
      "in now perform well withg, hopefully the model can now perform well withg, hopefully the model can nopeler string, hopefully the model can now perform well witho,efully the model can now perform well \n",
      "\n",
      "\n",
      "iter :3500, loss:3.040618\n",
      " within now perform well within now perform well withg, hopefully the model can now perhdrm well withg, hopefully the model can now perform well withopefullcall withimpler string, hopefully the model \n",
      "\n",
      "\n",
      "iter :4000, loss:1.879524\n",
      "ing, hopefully the model can now perform well withiming, hopefully the model can now perform well within string, hopefully the model can now perform well withon perform well within simpler string, hop\n",
      "\n",
      "\n",
      "iter :4500, loss:1.170408\n",
      "withun now perform well withimiserfurm well withg,ihimolel can now perform well within hopefully the model can now perform well withopefully the model can now perform well within now perform well with\n",
      "\n",
      "\n",
      "iter :5000, loss:0.736516\n",
      "can now perform well within now perform well within hopefully the model can nod perform well withwnodel can now perform well withg, hopefully the model can now perform well withg, hopefully the model \n",
      "\n",
      "\n",
      "iter :5500, loss:0.470308\n",
      "ing, hopefully the model can now perform well withg,ihim well withopefully the model can now perform well within now perform well withopefully the model can now perform well withimpler string, hopeful\n",
      "\n",
      "\n",
      "iter :6000, loss:0.306401\n",
      "an now perform well withg, hopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well \n",
      "\n",
      "\n",
      "iter :6500, loss:0.204971\n",
      "can now perform well witho,efully the model can now perform well withg, hopefully the model can now perform well withimodel can now perform well withg, hopefully the model can now perform well within \n",
      "\n",
      "\n",
      "iter :7000, loss:0.141757\n",
      "ing, hopefully the model can now perform well withimpler string, hopefullv is withun now perform well withg, hopefully the model cauen m well within now perform well withimpler string, hopefu ly ihe m\n",
      "\n",
      "\n",
      "iter :7500, loss:0.101999\n",
      "in now perform well withopefully the model can now perform well within nopefully the model can now perform well withan now perform well within now perform well withan now perform well withg,lhopefully\n",
      "\n",
      "\n",
      "iter :8000, loss:0.076668\n",
      "cwithan now perform well withan now perform well withg, hopefully the model can now perform well withg, hopefully the model can now perform well withwitfun now perform well withimpler string, hopefull\n",
      "\n",
      "\n",
      "iter :8500, loss:0.060246\n",
      "ing, hopefully the model can now perform well withim nell withg, hopefully the model can now perform well withg, hopefully the model can now perform well withg, hopefully the model can now perform wel\n",
      "\n",
      "\n",
      "iter :9000, loss:0.049376\n",
      "in ng, hopefully the model can now perform well withopefully the model can now perform well within now perform well withg, hopefully the model can now perform well withimpler string, hopefully the mod\n",
      "\n",
      "\n",
      "iter :9500, loss:0.041984\n",
      "can now perform well within now perform well withimpler string, hopefully the model can now perform well withimpler string, hopefully the model can now perform well within now perform well within now \n",
      "\n",
      "\n",
      "iter :10000, loss:0.036788\n",
      "ing, hopefully the model can now perform well within now perform well within now perform well withg, hopefully the model can now perform well withopefully the model can now perform well withimodel can\n",
      "\n",
      "\n",
      "iter :10500, loss:0.033010\n",
      "en now perform well within now perform well within now perform well withopefully the model can now perform well within now perform well withopefully the model can now perfurm well withg, hopefully the\n",
      "\n",
      "\n",
      "iter :11000, loss:0.030158\n",
      " smpler string, hopefully the model can now perform well within now perform well withowefully the model can now perform well within now perform well withan now perform well withopefully the model can \n",
      "\n",
      "\n",
      "iter :11500, loss:0.027917\n",
      "ing, hopefully the model can now perform well within now perform well within now perform well withg,phopefully the model can now perform well withimpler string, hopefully the model can now perform wel\n",
      "\n",
      "\n",
      "iter :12000, loss:0.026100\n",
      "in now perform well withon perform well withg, hopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well withg, hopefully the mod\n",
      "\n",
      "\n",
      "iter :12500, loss:0.024582\n",
      "can now perform well withcwefullv the model can now perform well withg, hopefully the model can now perform well withopefully the model can now perform well within hopefully the model can now perform \n",
      "\n",
      "\n",
      "iter :13000, loss:0.023275\n",
      "ing, hopefully the model can now perform well withimpler string, hopefully the model can now perform well withopefully the model can now perform well withg, hopefully the model can now perform well wi\n",
      "\n",
      "\n",
      "iter :13500, loss:0.022133\n",
      "in now perform well withimpler string, hopefully the model can now perform well within now perform well within now perform well withopefully the model can now perform well within now perform well with\n",
      "\n",
      "\n",
      "iter :14000, loss:0.021118\n",
      "can now perform well withg, hopefully the model can now perform well within now perform well withg, hopefully the model can now perform well within now perform well withg,dhopefully the model can now \n",
      "\n",
      "\n",
      "iter :14500, loss:0.020201\n",
      "ing, hopefully the model can now perform well within now perform well withg, hopefully the model can now perform well withopefully the model can now perform well withg, hopefully the model can now per\n",
      "\n",
      "\n",
      "iter :15000, loss:0.019369\n",
      "in now perform well withon nopefully the model can now perform well within now perform well withg, hopefully the model can now perform well withopefully the model can now perform well withan now perfo\n",
      "\n",
      "\n",
      "iter :15500, loss:0.018608\n",
      "can now perform well withg, hopefully the model can now perform well withg, hopefully the model can now perform well within now perform well withg, hopefully the model can now perform well withg, hope\n",
      "\n",
      "\n",
      "iter :16000, loss:0.017903\n",
      "ing, hopefully the model can now perform well within now perform well withg, hopefully the model can now perform well withan now perform well within now perform well within string, hopefully the model\n",
      "\n",
      "\n",
      "iter :16500, loss:0.017254\n",
      "an now perform well within now perform well withowefully the model can now perform well withg, hopefully the model can now perform well withg, hopefully the model can now perform well within ng, hopef\n",
      "\n",
      "\n",
      "iter :17000, loss:0.016652\n",
      "can now perform well within now perform well withg, hopefully the model can now perform well withun now perform well within now perform well within now perform well withg,ihim perform well within now \n",
      "\n",
      "\n",
      "iter :17500, loss:0.016088\n",
      "ing, hopefully the model can now perform well withan now perform well within now perform well withimpler string, hopefully the model can now perform well withg, hopefully the model can now perform wel\n",
      "\n",
      "\n",
      "iter :18000, loss:0.015563\n",
      "an now perform well within simpler string, hopefully the model can now perform well withg, hopefully the model can now perform well withg, hopefully the model can now perform well withopefully the mod\n",
      "\n",
      "\n",
      "iter :18500, loss:0.015073\n",
      " withg, hopefully the model can now perform well within now perform well within now perform well withg, hopefully the model can now perform well withg, hopefully the model can now perform well within \n",
      "\n",
      "\n",
      "iter :19000, loss:0.014611\n",
      "ing, hopefully the model can now perform well withg, hopefully the model can now perform well withopefully the model can now perform well withg, hopefully the model can now perform well withimpler str\n",
      "\n",
      "\n",
      "iter :19500, loss:0.014178\n",
      "in now perform well within now perform well withopefully the model can now perform well within node erform well within now perform well within now perform well withg, hopefully the model can now perfo\n",
      "\n",
      "\n",
      "iter :20000, loss:0.013771\n",
      "can now perform well within now perform well within now perform well withg, hopefully the model can now perform well withan now perform well within now perform well withg, hopefully the model can now \n",
      "\n",
      "\n",
      "iter :20500, loss:0.013385\n",
      "ing, hopefully the model can now perform well within now perform well withg, hopefully the model can now perform well withopefully the model can now perform well within now perform well withan now per\n",
      "\n",
      "\n",
      "iter :21000, loss:0.013021\n",
      "an now perform well within nopefully the model can now perform well withan now perform well within now perform well within ng, hopefully the model can now perform well withan now perform well within n\n",
      "\n",
      "\n",
      "iter :21500, loss:0.012677\n",
      "can now perform well withg, hopefully the model can now perform well withg, hopefully the model can now perform well withg, hopefully the model can now perform well withimpler string, hopefully the mo\n",
      "\n",
      "\n",
      "iter :22000, loss:0.012349\n",
      "ing, hopefully the model can now perform well withg, hopefully the model can now perform well withg, hopefully the model can now perform well within now perform well within now perform well withg, hop\n",
      "\n",
      "\n",
      "iter :22500, loss:0.012039\n",
      "g, hopefully the model can now perform well withg, hopefully the model can now perform well withopefully the model can now der simpler string, hopefully the model can now perform well withu, now perfo\n",
      "\n",
      "\n",
      "iter :23000, loss:0.011745\n",
      "can now perform well withg, hopefully the model can now perform well withopefully the model can now perform well within now perform well with,n now perform well withg, hopefully the model can now perf\n",
      "\n",
      "\n",
      "iter :23500, loss:0.011463\n",
      "ing, hopefully the model can now perform well within now perform well withopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform wel\n",
      "\n",
      "\n",
      "iter :24000, loss:0.011195\n",
      "in now perform well withg, hopefully the model can now perform well withg, hopefully the model can now perform well within now perform well withg, hopefully the model can now perform well withg, hopef\n",
      "\n",
      "\n",
      "iter :24500, loss:0.010940\n",
      "can now perform well within now perform well withopefully the model can now perform well withg, hopefully the model can now perform well withg, hopefully the model can now perform well withopefully th\n",
      "\n",
      "\n",
      "iter :25000, loss:0.010695\n",
      "ing, hopefully the model can now perform well withan now perform well withg, homeler string, hopefully the model can now perform well withopefully the model can now perform well withopefully the model\n",
      "\n",
      "\n",
      "iter :25500, loss:0.010461\n",
      "in now perform well withopefully the model can now perform well within now perform well within now perform well withg, hopefully the model can now perform well within nopefully the model can now perfo\n",
      "\n",
      "\n",
      "iter :26000, loss:0.010238\n",
      "can now perform well withg, hopefully the model can now perform well withg, hopefully thegmodel can now perform well withopefully the model can now perform well withg, hopefully the model can now perf\n",
      "\n",
      "\n",
      "iter :26500, loss:0.010022\n",
      "Prediction 1 (This): This s a even simpler string, hopefully the model can n\n",
      "Prediction 2 (simpler): simpler mpler string, hopefully the model can now perform \n",
      "Prediction 3 (perform): perform rform well withg, hopefully the model can now perf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"input.txt\", seq_length, \\\n",
    "                         data=\"This is a even simpler string, hopefully the model can now perform well with predicting the outputs.\")\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "rnn.train(data_reader)\n",
    "\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'This ', 50)\n",
    "print(f\"Prediction 1 (This): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'simpler ', 50)\n",
    "print(f\"Prediction 2 (simpler): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'perform ', 50)\n",
    "print(f\"Prediction 3 (perform): {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bea65-bfe7-4a69-a108-0f04f78726f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
