{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f45fe0-6e50-4894-b79e-589023a3dd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b56d226-ac6f-42e5-8abe-cd0d6eed48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length, data=\"\", progress_len=5, max_trace_len = 200):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        self.data = \"Today we will be talking about RNN and LSTM. RNN stands for recurrent neural network, \" + \\\n",
    "                    \"the special property of RNN is its ability to use the results from the past to generate a new result. \" + \\\n",
    "                    \"LSTM stands for Long-Short Term Memory, \" + \\\n",
    "                    \"LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, \" + \\\n",
    "                    \"and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradients.\"\n",
    "        if len(data) != 0:\n",
    "            self.data = data\n",
    "        #self.fp = open(path, \"r\")\n",
    "        #self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        print(chars)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "        self.progress_len = progress_len\n",
    "        self.max_trace_len = max_trace_len\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.progress_len\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer, start new epoch\n",
    "            self.pointer = -1\n",
    "        trace_start = 0\n",
    "        if self.pointer > self.max_trace_len:\n",
    "            trace_start = self.pointer - self.max_trace_len\n",
    "        prev_input = [self.char_to_ix[ch] for ch in self.data[trace_start:input_start]]\n",
    "        return prev_input, inputs, targets\n",
    "\n",
    "    def new_epoch(self):\n",
    "        return self.pointer == -1\n",
    "    def start_epoch(self):\n",
    "        self.pointer = 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371906b3-1ba2-4292-902d-2a342ff63a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, start, inputs):\n",
    "        xs, hs, os, ycap = {}, {}, {}, {}\n",
    "\n",
    "        oh_start = np.zeros((self.vocab_size, 1))\n",
    "        hs_start = np.zeros((self.hidden_size, 1))\n",
    "        for c in start:\n",
    "            oh_start[c] = 1\n",
    "            hs_start = np.tanh(np.dot(self.U, oh_start) + np.dot(self.W, hs_start)+self.b)\n",
    "            oh_start[c] = 0\n",
    "        \n",
    "            \n",
    "        hs[-1] = hs_start\n",
    "\n",
    "        \n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((self.vocab_size,1))\n",
    "            xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "            hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "            os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "            ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "        return xs, hs, ycap\n",
    "        \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "            # backward pass: compute gradients going backwards\n",
    "            dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "            db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "            dhnext = np.zeros_like(hs[0])\n",
    "            for t in reversed(range(self.seq_length)):\n",
    "                dy = np.copy(ps[t])\n",
    "                #through softmax\n",
    "                dy[targets[t]] -= 1 # backprop into y\n",
    "                #calculate dV, dc\n",
    "                dV += np.dot(dy, hs[t].T)\n",
    "                dc += dc\n",
    "                #dh includes gradient from two sides, next cell and current output\n",
    "                dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "                # backprop through tanh non-linearity \n",
    "                dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "                db += dhrec\n",
    "                #calculate dU and dW\n",
    "                dU += np.dot(dhrec, xs[t].T)\n",
    "                dW += np.dot(dhrec, hs[t-1].T)\n",
    "                #pass the gradient from next cell to the next iteration.\n",
    "                dhnext = np.dot(self.W.T, dhrec)\n",
    "            # clip to mitigate exploding gradients\n",
    "            for dparam in [dU, dW, dV, db, dc]:\n",
    "                np.clip(dparam, -5, 5, out=dparam) \n",
    "            return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"loss for a sequence\"\"\"\n",
    "        # calculate cross-entrpy loss\n",
    "        return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "                \n",
    "    def sample(self, h, seed_ix, n):\n",
    "            \"\"\"\n",
    "            sample a sequence of integers from the model\n",
    "            h is memory state, seed_ix is seed letter from the first time step\n",
    "            \"\"\"\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[seed_ix] = 1\n",
    "            ixes = []\n",
    "            for t in range(n):\n",
    "                h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "                y = np.dot(self.V, h) + self.c\n",
    "                p = np.exp(y)/np.sum(np.exp(y))\n",
    "                ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "                x = np.zeros((self.vocab_size,1))\n",
    "                x[ix] = 1\n",
    "                ixes.append(ix)\n",
    "            return ixes\n",
    "\n",
    "    def train(self, data_reader, threshold = 0.01, max_epoch = 100, print_string = \"\"):\n",
    "        epoches = 0\n",
    "        loss = threshold + 1\n",
    "            \n",
    "        while (loss > threshold and max_epoch > epoches):\n",
    "            data_reader.start_epoch()\n",
    "            exec(print_string)\n",
    "            while not data_reader.new_epoch():\n",
    "                start, inputs, targets = data_reader.next_batch()\n",
    "                for i in range(0, len(start), 15):\n",
    "                    xs, hs, ps = self.forward(start[i:], inputs)\n",
    "                    dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "                    self.update_model(dU, dW, dV, db, dc)\n",
    "\n",
    "                # xs, hs, ps = self.forward(start, inputs)\n",
    "                # dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "                # self.update_model(dU, dW, dV, db, dc)\n",
    "                \n",
    "            epoches += 1            \n",
    "            loss = self.loss(ps, targets)\n",
    "            print(f\"Finished {epoches} epoches, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, data_reader, start, n):\n",
    "\n",
    "        #initialize input vector\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []        \n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "\n",
    "        for i in range(len(chars) - 1):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            ixes.append(ix)\n",
    "            x[ix] = 0\n",
    "\n",
    "        ix = data_reader.char_to_ix[chars[-1]]\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            x[ix] = 0\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529a4220-2fb7-4106-a5dc-a5ad535058e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'a', 'i', 'N', 's', 'L', 'b', 'u', 'w', '/', 'T', 'o', 'M', ' ', 'f', 'x', 'y', 'c', 'k', 'n', 't', 'S', 'h', ',', 'R', '.', 'l', '-', 'p', 'v', 'm', 'e', 'g', 'r']\n",
      "starting epoch: 0\n",
      "Prediction 1 (memory): memory LfscLnTRo -nrklktkR-uc -ttLc-Mff-bex,savtcTkduf.si\n",
      "Prediction 2 (LSTM): LSTM aeLhfc,ldTbernciRko doRahmv/M/xfi,dTvmTysolkkns.vm\n",
      "Prediction 3 (complex): complex .lsaoRd/ovSs.uN.hnh-wyoLplprrLMR.dN,e-rN,uTRmaegfi\n",
      "Finished 1 epoches, Loss: 23.181388986584807\n",
      "starting epoch: 1\n",
      "Prediction 1 (memory): memory he vaninrxnisg-vlcding/viadoiandsgandaorieie explo\n",
      "Prediction 2 (LSTM): LSTM inN ang ggkms andshind grtdieng-vandsging-annusesn\n",
      "Prediction 3 (complex): complex ani ga/g.esong gtiha taidatand ing ing stinisg/vin\n",
      "Finished 2 epoches, Loss: 2.185397328809108\n",
      "starting epoch: 2\n",
      "Prediction 1 (memory): memory hestand candshandseng ang andisndshindsorisslvang \n",
      "Prediction 2 (LSTM): LSTM ing gandam ong-gexploding/vonishing gradie handsha\n",
      "Prediction 3 (complex): complex and hangstsong/sonies. vandsexpandshand handananda\n",
      "Finished 3 epoches, Loss: 1.3681794739571633\n",
      "starting epoch: 3\n",
      "Prediction 1 (memory): memory m tererm me enm ingdangplsoggashandlisnggtshang/sh\n",
      "Prediction 2 (LSTM): LSTM ing erps and gangalse vandaereso vendasexgandilg/g\n",
      "Prediction 3 (complex): complex andiantthexplsding lan aliorivaning gradient-neng-\n",
      "Finished 4 epoches, Loss: 0.8331925642587814\n",
      "starting epoch: 4\n",
      "Prediction 1 (memory): memory orishing aandhang al nanthes ing graditingosg/pres\n",
      "Prediction 2 (LSTM): LSTM itN the lyphantomemeorererorieient-l anggglvdneriv\n",
      "Prediction 3 (complex): complex resoiterm,seng-tsting/vanishing gradients feneitin\n",
      "Finished 5 epoches, Loss: 0.7506776948715307\n",
      "starting epoch: 5\n",
      "Prediction 1 (memory): memory eraditients andting, L-/sting grtdalitingvanhting/\n",
      "Prediction 2 (LSTM): LSTM if dangtse gris ing gradientsting gradientshing an\n",
      "Prediction 3 (complex): complex bengaving ang bandientsheriving/vang-ses ing cradi\n",
      "Finished 6 epoches, Loss: 0.5186822669410087\n",
      "starting epoch: 6\n",
      "Prediction 1 (memory): memory ces andsentsng LSTM isn seing/sealso resortererm m\n",
      "Prediction 2 (LSTM): LSTM it dialgdaes ang scandienuere candsenting/vandienm\n",
      "Prediction 3 (complex): complex aes and can andte mooringereneraeing gan ientanivg\n",
      "Finished 7 epoches, Loss: 0.4265782018214681\n",
      "starting epoch: 7\n",
      "Prediction 1 (memory): memory eropaltone hem be hentaling/song-term meoeradiding\n",
      "Prediction 2 (LSTM): LSTM it mwm moring and hendiss ge/palgom orving ano hen\n",
      "Prediction 3 (complex): complex aespeconres al onithe long/seal nexhand gradienos \n",
      "Finished 8 epoches, Loss: 0.6271297872243\n",
      "starting epoch: 8\n",
      "Prediction 1 (memory): memory candbent-torg, Lhd horient nes ind ding vanishing \n",
      "Prediction 2 (LSTM): LSTM it m mecorishorterenm memererorierera, the long/sh\n",
      "Prediction 3 (complex): complex gen ats anishe remories. LSTM is bes and cing/gs g\n",
      "Finished 9 epoches, Loss: 0.2791941927057241\n",
      "starting epoch: 9\n",
      "Prediction 1 (memory): memory msefore, and ab bev aby ding and alishe fenp, bon \n",
      "Prediction 2 (LSTM): LSTM if hexthe loe a ding/vanishing gradientste handse \n",
      "Prediction 3 (complex): complex ioning lnathenl-tories. LSTM is ing anithe long/sp\n",
      "Finished 10 epoches, Loss: 0.2637670647208652\n",
      "starting epoch: 10\n",
      "Prediction 1 (memory): memory erom lradits abith feorithe long/sex gong-Sho benh\n",
      "Prediction 2 (LSTM): LSTM it m metee ang sofploding/ving lso ies.lte aem if \n",
      "Prediction 3 (complex): complex loning ann angtre candle mnorluetl alodite meorith\n",
      "Finished 11 epoches, Loss: 0.23386298660801572\n",
      "starting epoch: 11\n",
      "Prediction 1 (memory): memory eralodiesenting the long/shorteperalitan alionithe\n",
      "Prediction 2 (LSTM): LSTM it handsteeres alvaliese LcTM.iss. LSTM is better \n",
      "Prediction 3 (complex): complex algerm ormory, and al and complex ae ori, meoriesu\n",
      "Finished 12 epoches, Loss: 0.3021617039744865\n",
      "starting epoch: 12\n",
      "Prediction 1 (memory): memory ecalto iloding/vanishing gradientsherieno Long alg\n",
      "Prediction 2 (LSTM): LSTM if ha/thew Lclls. resules gen-ies fnds forkval ali\n",
      "Prediction 3 (complex): complex algorishm memories. LSTM is better at handlinda re\n",
      "Finished 13 epoches, Loss: 0.22112411641941399\n",
      "starting epoch: 13\n",
      "Prediction 1 (memory): memory erom lha ies for LSTM. song-term memories. LSTM is\n",
      "Prediction 2 (LSTM): LSTM is atstane meorie meorithe paspeceiesolteeeresulor\n",
      "Prediction 3 (complex): complex iesolve exploding/vanishing gradientste o iong aon\n",
      "Finished 14 epoches, Loss: 0.16636327956608424\n",
      "starting epoch: 14\n",
      "Prediction 1 (memory): memory exo aes and gSTMentandne pforinentresents ne long-\n",
      "Prediction 2 (LSTM): LSTM Ls LSTM is fxmwmrty oro restlvalt nespandlentandse\n",
      "Prediction 3 (complex): complex genhis new result. LSTMdifforinebur nan alits abe \n",
      "Finished 15 epoches, Loss: 0.164378575301996\n",
      "starting epoch: 15\n",
      "Prediction 1 (memory): memory eralodies and hentand panta ien also reneresonvese\n",
      "Prediction 2 (LSTM): LSTM it hextne lke recustaner romithe long/short-eerere\n",
      "Prediction 3 (complex): complex gonererecal hor vesoaninerave a ne hexpas alte feo\n",
      "Finished 16 epoches, Loss: 0.14928611382734633\n",
      "starting epoch: 16\n",
      "Prediction 1 (memory): memory caaber he long/sexploding/vanishing gradientste al\n",
      "Prediction 2 (LSTM): LSTM nex ao tem mrteralodienm the long-term meories and\n",
      "Prediction 3 (complex): complex gesult. LSTM. RNN st nentanew reshe pero resulte m\n",
      "Finished 17 epoches, Loss: 0.17054163922813795\n",
      "starting epoch: 17\n",
      "Prediction 1 (memory): memory eralodiabu RN tind Lbo its fet geNhand cxnRN RNN l\n",
      "Prediction 2 (LSTM): LSTM it hex co pe m more efmoring lforian aloting and h\n",
      "Prediction 3 (complex): complex gendinstity or genl nenpaes lof alot len and cing \n",
      "Finished 18 epoches, Loss: 0.17712820958669415\n",
      "starting epoch: 18\n",
      "Prediction 1 (memory): memory memeries. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM it m mem.ries instity handling long-term meories a\n",
      "Prediction 3 (complex): complex itse tong aloults anithe long-term meories and can\n",
      "Finished 19 epoches, Loss: 0.19811542307284033\n",
      "starting epoch: 19\n",
      "Prediction 1 (memory): memory cexpalies and a nexpreding/vanishing gradients for\n",
      "Prediction 2 (LSTM): LSTM is aestand LSlving gradients for recues aling anot\n",
      "Prediction 3 (complex): complex ienor RNN is gesulof LSTM. RNN standsenor Long-Sho\n",
      "Finished 20 epoches, Loss: 0.1509832745153883\n",
      "starting epoch: 20\n",
      "Prediction 1 (memory): memory eralodieneta feopies. remories. LSTM is better at \n",
      "Prediction 2 (LSTM): LSTM ity the lremoriepl Long-ree long-term meories and \n",
      "Prediction 3 (complex): complex algorithm tonhes andsenmplex ion long-teraloding/v\n",
      "Finished 21 epoches, Loss: 0.11967848645562386\n",
      "starting epoch: 21\n",
      "Prediction 1 (memory): memory orithe pands aon ani gendialodity ur m meories and\n",
      "Prediction 2 (LSTM): LSTM it hextorerteneralhe paedling/short-term meories a\n",
      "Prediction 3 (complex): complex algerato aeneroralitopa form chenlonow remorithe l\n",
      "Finished 22 epoches, Loss: 0.10467304039576932\n",
      "starting epoch: 22\n",
      "Prediction 1 (memory): memory ceneal ndspand ae tan abu hentandsents for Lenm to\n",
      "Prediction 2 (LSTM): LSTM it Lvathe the long-term meories and can also resol\n",
      "Prediction 3 (complex): complex genorm memories. LSTM is better at handling long-t\n",
      "Finished 23 epoches, Loss: 0.1132140446821705\n",
      "starting epoch: 23\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM it heftrerories and can also iesolvt eraditio ilgo\n",
      "Prediction 3 (complex): complex algorithm to hentore Ronpl bority of RNN is its. L\n",
      "Finished 24 epoches, Loss: 0.11272882326222554\n",
      "starting epoch: 24\n",
      "Prediction 1 (memory): memory cexpae ermore cand-can also resolve exploding/vani\n",
      "Prediction 2 (LSTM): LSTM it hes fem the pastat. meom the long/short-term me\n",
      "Prediction 3 (complex): complex result. LSTM standing gha nethtenm LSTM is better \n",
      "Finished 25 epoches, Loss: 0.11030238575215034\n",
      "starting epoch: 25\n",
      "Prediction 1 (memory): memory chang remories. LSTM is better at handling long-te\n",
      "Prediction 2 (LSTM): LSTM it .tatm Lul RNN and LSTM. RNN stands for Long-Sho\n",
      "Prediction 3 (complex): complex result. RNN but differs from the fone henting the \n",
      "Finished 26 epoches, Loss: 0.10974938603748462\n",
      "starting epoch: 26\n",
      "Prediction 1 (memory): memory caabert Term Memories. LSTM is better at handling \n",
      "Prediction 2 (LSTM): LSTM ie the lhe leecalkend alodie tondspa forurecorm me\n",
      "Prediction 3 (complex): complex gemories. LSTM is better at handling long-term meo\n",
      "Finished 27 epoches, Loss: 0.12289542263150938\n",
      "starting epoch: 27\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM it hextte ls lenerm oer generadient nes and canu l\n",
      "Prediction 3 (complex): complex result. LSTM stands for recuite tands for recurren\n",
      "Finished 28 epoches, Loss: 0.38487002931222475\n",
      "starting epoch: 28\n",
      "Prediction 1 (memory): memory meories and can also resolve exploding/vanishing g\n",
      "Prediction 2 (LSTM): LSTM nex Lot Lonm thewiand neu-alse the long-term meori\n",
      "Prediction 3 (complex): complex result. LSTM is a Ta tenera eet nories. LSTM is be\n",
      "Finished 29 epoches, Loss: 0.31629371750838786\n",
      "starting epoch: 29\n",
      "Prediction 1 (memory): memory wexpalingw remorrestories abut Tr hent ndm torilin\n",
      "Prediction 2 (LSTM): LSTM it hextorm te m memories. LSTM is better at handli\n",
      "Prediction 3 (complex): complex resultian tretanee tong aong seneraverresealong ah\n",
      "Finished 30 epoches, Loss: 1.2930857298684666\n",
      "starting epoch: 30\n",
      "Prediction 1 (memory): memory orita gencinso alspeopepoilsd LSTM. RNN stands for\n",
      "Prediction 2 (LSTM): LSTM it hext netwits alitat sex aonu-te mooerm meories \n",
      "Prediction 3 (complex): complex result. LSTM is nen and feorits abils frocing lrom\n",
      "Finished 31 epoches, Loss: 0.10896938639914683\n",
      "starting epoch: 31\n",
      "Prediction 1 (memory): memory ere a ner volheries. LSTM is better at handling lo\n",
      "Prediction 2 (LSTM): LSTM it hiy hentetm LSTM us aenera eemory, and a more c\n",
      "Prediction 3 (complex): complex res anitita hendst an the long/song-shind abult no\n",
      "Finished 32 epoches, Loss: 0.17357430423859968\n",
      "starting epoch: 32\n",
      "Prediction 1 (memory): memory meories and can also resolve exploding/vanishing g\n",
      "Prediction 2 (LSTM): LSTM it hextorertet al orithe rorm remory, and a more c\n",
      "Prediction 3 (complex): complex result. LSTM is a form of RNN but differs from tra\n",
      "Finished 33 epoches, Loss: 0.11610170931509116\n",
      "starting epoch: 33\n",
      "Prediction 1 (memory): memory wert. RNN at handle the long/shert-term meories an\n",
      "Prediction 2 (LSTM): LSTM ity tandstl fet re alo ge hent neutands for Long-S\n",
      "Prediction 3 (complex): complex results from traditional RNN by having gro al oet \n",
      "Finished 34 epoches, Loss: 2.5105372301872055\n",
      "starting epoch: 34\n",
      "Prediction 1 (memory): memory orithe angdl for Long-Shor senting aantstina faor \n",
      "Prediction 2 (LSTM): LSTM it hexpaew remories. LSTM is better at handling lo\n",
      "Prediction 3 (complex): complex res anits f ferm meories and can also resolve expl\n",
      "Finished 35 epoches, Loss: 0.23584189598325653\n",
      "starting epoch: 35\n",
      "Prediction 1 (memory): memory tenm LfN bethes and can also resolve exploding/van\n",
      "Prediction 2 (LSTM): LSTM it htytoreptrt torm the long/short-te mNndiesol Rh\n",
      "Prediction 3 (complex): complex al handling long-term meories and can also resolve\n",
      "Finished 36 epoches, Loss: 3.569436836318259\n",
      "starting epoch: 36\n",
      "Prediction 1 (memory): memory wem long-STShant LSTM is better at handling long-t\n",
      "Prediction 2 (LSTM): LSTM it hextorettetrera ae -arkersealiti galut repm ren\n",
      "Prediction 3 (complex): complex result. LSTM stands for recurrent neural network, \n",
      "Finished 37 epoches, Loss: 0.06607519660820187\n",
      "starting epoch: 37\n",
      "Prediction 1 (memory): memory ore longalso rtsd fforiestanopr or RNN but differs\n",
      "Prediction 2 (LSTM): LSTM it hextorerilg alits fremothe long/short-term meor\n",
      "Prediction 3 (complex): complex geroreeorepa deplerermemories. LSTM is better at h\n",
      "Finished 38 epoches, Loss: 0.11563278928233\n",
      "starting epoch: 38\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM it hestanetweut Te RNN but differs from traditing \n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term memories. \n",
      "Finished 39 epoches, Loss: 0.11533275196642162\n",
      "starting epoch: 39\n",
      "Prediction 1 (memory): memory meories and can also resolve exploding/vanishing g\n",
      "Prediction 2 (LSTM): LSTM it LSTM. LcTdity mone LSTM from traditional Ron al\n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term meories an\n",
      "Finished 40 epoches, Loss: 4.3982469634845005\n",
      "starting epoch: 40\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM it hextorerity thor RNN be handle lands for recurr\n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term meories an\n",
      "Finished 41 epoches, Loss: 1.4884152993778916\n",
      "starting epoch: 41\n",
      "Prediction 1 (memory): memory wert hendis a form of RNN but differs from traditi\n",
      "Prediction 2 (LSTM): LSTM it hextorm te orm te ort Term Memore craditiotalti\n",
      "Prediction 3 (complex): complex result. RNN but differs from traditional RNN by ha\n",
      "Finished 42 epoches, Loss: 0.5537469880807074\n",
      "starting epoch: 42\n",
      "Prediction 1 (memory): memory orithe tong/short-term memories. LSTM is better at\n",
      "Prediction 2 (LSTM): LSTM it hty wesults abelute ahm to handle the long/shor\n",
      "Prediction 3 (complex): complex genora meories and can also resolve exploding/vani\n",
      "Finished 43 epoches, Loss: 0.19558460492461766\n",
      "starting epoch: 43\n",
      "Prediction 1 (memory): memory memory, and a more complex algorithm to handle the\n",
      "Prediction 2 (LSTM): LSTM it hty band LSTM. RNN but differs from traditional\n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term memories. \n",
      "Finished 44 epoches, Loss: 0.27767016694208396\n",
      "starting epoch: 44\n",
      "Prediction 1 (memory): memory orithe pasta results from traditional RNN by havin\n",
      "Prediction 2 (LSTM): LSTM it hextorm te orm bul differs from traditional RNN\n",
      "Prediction 3 (complex): complex al handling long-term meories and can also resolve\n",
      "Finished 45 epoches, Loss: 0.7629846452880731\n",
      "starting epoch: 45\n",
      "Prediction 1 (memory): memory werthe foreroreva Me rane results from traditional\n",
      "Prediction 2 (LSTM): LSTM it hextondshg RN RNN fst LSTM is a form of RNN but\n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term memories. \n",
      "Finished 46 epoches, Loss: 0.19101722298003593\n",
      "starting epoch: 46\n",
      "Prediction 1 (memory): memory memory, and a more couting the long-term meories a\n",
      "Prediction 2 (LSTM): LSTM it hexplod net an having another value aopers frew\n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term meories an\n",
      "Finished 47 epoches, Loss: 0.14751187334908733\n",
      "starting epoch: 47\n",
      "Prediction 1 (memory): memory meories. LSTM is better at handling long-term meor\n",
      "Prediction 2 (LSTM): LSTM ientreeterereremomitha form meories and can also r\n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term meories an\n",
      "Finished 48 epoches, Loss: 0.09006746760043369\n",
      "starting epoch: 48\n",
      "Prediction 1 (memory): memory meories and can also resolve exploding/vanishing g\n",
      "Prediction 2 (LSTM): LSTM it hextormeterm cent die vands for Lendityeory, an\n",
      "Prediction 3 (complex): complex algorithm to handle the long-term memory, and a mo\n",
      "Finished 49 epoches, Loss: 0.052062402586347895\n",
      "starting epoch: 49\n",
      "Prediction 1 (memory): memory tenm Mrm term meories and can also resolve explodi\n",
      "Prediction 2 (LSTM): LSTM it hiy hentenew rem lot tt. RNN but differs frorie\n",
      "Prediction 3 (complex): complex reseloreds for recurrent neural pestands for Long-\n",
      "Finished 50 epoches, Loss: 0.15567909313358252\n",
      "starting epoch: 50\n",
      "Prediction 1 (memory): memory pent hent nk resuats from traditional RNN by havin\n",
      "Prediction 2 (LSTM): LSTM it hextoeult. LSTM is a form of RNN but differs fr\n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term memories. \n",
      "Finished 51 epoches, Loss: 0.229107728814022\n",
      "starting epoch: 51\n",
      "Prediction 1 (memory): memory memory, and a more complex algorithm to handle the\n",
      "Prediction 2 (LSTM): LSTM ity ting LeTM arother vanu-erom the plot  rem long\n",
      "Prediction 3 (complex): complex res abiud fers aluds lon s nes and a form thm toth\n",
      "Finished 52 epoches, Loss: 0.2965283824057638\n",
      "starting epoch: 52\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM itf tew re wet nes and cong-term meories and can a\n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term memories. \n",
      "Finished 53 epoches, Loss: 0.09514002914416951\n",
      "starting epoch: 53\n",
      "Prediction 1 (memory): memory wextork, the long/short-term memories. LSTM is bet\n",
      "Prediction 2 (LSTM): LSTM it hestyneworalitont-te candle the long/short-term\n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term memories. \n",
      "Finished 54 epoches, Loss: 0.0793428520225829\n",
      "starting epoch: 54\n",
      "Prediction 1 (memory): memory wert Tent nem be hent neurands fer iesults from tr\n",
      "Prediction 2 (LSTM): LSTM is feterm tenorite ano be thm tongsrse tong-term m\n",
      "Prediction 3 (complex): complex benplt. LSTM stands for Long-Short Term Memory, LS\n",
      "Finished 55 epoches, Loss: 0.06502054069579981\n",
      "starting epoch: 55\n",
      "Prediction 1 (memory): memory momplex and endvandshand cx RNN but hex ang sexthe\n",
      "Prediction 2 (LSTM): LSTM it hexpaerlre feorithe aong-term memory, and a mor\n",
      "Prediction 3 (complex): complex al orithm to handle the long/short-term memories. \n",
      "Finished 56 epoches, Loss: 0.05886767428886851\n",
      "starting epoch: 56\n",
      "Prediction 1 (memory): memory memory, and a more complex algorithm to handle the\n",
      "Prediction 2 (LSTM): LSTM it hestynewore wexoree and resules anits from trad\n",
      "Prediction 3 (complex): complex al handling long-term meories and can also resolve\n",
      "Finished 57 epoches, Loss: 0.0554687433653789\n",
      "starting epoch: 57\n",
      "Prediction 1 (memory): memory wert Te rad ing about RNN and LSTM. RNN stands for\n",
      "Prediction 2 (LSTM): LSTM it hextormemory, and a more complex algorithm to h\n",
      "Prediction 3 (complex): complex algerate ang-the long-term meories and can also re\n",
      "Finished 58 epoches, Loss: 0.053313656392848616\n",
      "starting epoch: 58\n",
      "Prediction 1 (memory): memory wert Te RNN but differs from traditional RNN by ha\n",
      "Prediction 2 (LSTM): LSTM it hextormere wopes. ton resortsel pa tenm ving le\n",
      "Prediction 3 (complex): complex res and can also resolve exploding/vanishing gradi\n",
      "Finished 59 epoches, Loss: 0.051524125187426814\n",
      "starting epoch: 59\n",
      "Prediction 1 (memory): memory wevtatkf al onplor dienal RNN by having another va\n",
      "Prediction 2 (LSTM): LSTM it hesteneworeciling long-term meories and can als\n",
      "Prediction 3 (complex): complex algeresents fore com torm Memoty, LSTM is a form o\n",
      "Finished 60 epoches, Loss: 0.04968842538005181\n",
      "starting epoch: 60\n",
      "Prediction 1 (memory): memory meories and centalond so generate a new redilgotit\n",
      "Prediction 2 (LSTM): LSTM it hesteneratuer value representing the long-term \n",
      "Prediction 3 (complex): complex alshand canding van also torien. Long-Short Term M\n",
      "Finished 61 epoches, Loss: 0.04764710825170706\n",
      "starting epoch: 61\n",
      "Prediction 1 (memory): memory wemtor RNN be handling long-term meories and can a\n",
      "Prediction 2 (LSTM): LSTM iLl Long-tew remories lto g ng she rene conds for \n",
      "Prediction 3 (complex): complex als. the past to generate a new result. LSTM stand\n",
      "Finished 62 epoches, Loss: 0.04538368366596757\n",
      "starting epoch: 62\n",
      "Prediction 1 (memory): memory orith. torm oem te handle the long/short-term memo\n",
      "Prediction 2 (LSTM): LSTM it hesteneworalis a formoorm memorererom mom resul\n",
      "Prediction 3 (complex): complex res and can also resolve exploding/vanishing gradi\n",
      "Finished 63 epoches, Loss: 0.04305743865511895\n",
      "starting epoch: 63\n",
      "Prediction 1 (memory): memory wextal f als l fong-Shortse tong/shorts from tradi\n",
      "Prediction 2 (LSTM): LSTM it hestanewuremorithe long-term meories and can al\n",
      "Prediction 3 (complex): complex res and can also resolve exploding/vanishing gradi\n",
      "Finished 64 epoches, Loss: 0.041210720087906494\n",
      "starting epoch: 64\n",
      "Prediction 1 (memory): memory memory, LSTM is a form of RNN but differs from tra\n",
      "Prediction 2 (LSTM): LSTM it hesthewlre rene cont-t. RNNNNithe pant-te momem\n",
      "Prediction 3 (complex): complex aesulod abher value representing the long-term mem\n",
      "Finished 65 epoches, Loss: 0.04009704682100762\n",
      "starting epoch: 65\n",
      "Prediction 1 (memory): memory memory, and a more complex algorithm to handle the\n",
      "Prediction 2 (LSTM): LSTM it hing LR/uite Leng-le wing/vandseforaloding/vani\n",
      "Prediction 3 (complex): complex alshand canding van atiwerm cem to hent TRNN al RN\n",
      "Finished 66 epoches, Loss: 0.039798963191710474\n",
      "starting epoch: 66\n",
      "Prediction 1 (memory): memory memory, and a more complex algorithm to handle the\n",
      "Prediction 2 (LSTM): LSTM ie texulrilge represexting about resuabu gesules a\n",
      "Prediction 3 (complex): complex alshand canding ganishing grom ls gesult. LSTM sta\n",
      "Finished 67 epoches, Loss: 0.03988140940915915\n",
      "starting epoch: 67\n",
      "Prediction 1 (memory): memory memory, and a more complex algorithm to handle the\n",
      "Prediction 2 (LSTM): LSTM its fee rem me exploding/vanishing gradients feter\n",
      "Prediction 3 (complex): complex ao hendlsto remorithe tong-term memory, and a more\n",
      "Finished 68 epoches, Loss: 0.04097497460640278\n",
      "starting epoch: 68\n",
      "Prediction 1 (memory): memory wemory, LSTM is a form of RNN but differs from tra\n",
      "Prediction 2 (LSTM): LSTM ity Lendsew rem to generate a nerato aling aong-te\n",
      "Prediction 3 (complex): complex result. LSTM stands for Long-Short Term Memory, LS\n",
      "Finished 69 epoches, Loss: 0.04110584736295706\n",
      "starting epoch: 69\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM it hiap etthererert Term Memory, LSTM is a form of\n",
      "Prediction 3 (complex): complex alshand explodilse pemorrecerrew recorm Mempre pat\n",
      "Finished 70 epoches, Loss: 0.04200050837778966\n",
      "starting epoch: 70\n",
      "Prediction 1 (memory): memory memory, and a more complex algorithm to handle the\n",
      "Prediction 2 (LSTM): LSTM ity Lentalong fo. RNN is b s and a perm the pasta \n",
      "Prediction 3 (complex): complex result. LSTM is a form of RNN but differs from tra\n",
      "Finished 71 epoches, Loss: 0.04245237905231232\n",
      "starting epoch: 71\n",
      "Prediction 1 (memory): memory wemory, long-term meories and can also resolve exp\n",
      "Prediction 2 (LSTM): LSTM ithe Lenditfndsnlts for recurrentra, the long/shor\n",
      "Prediction 3 (complex): complex alshand exploding/vanishing gradientste feouresufr\n",
      "Finished 72 epoches, Loss: 0.0414365327011725\n",
      "starting epoch: 72\n",
      "Prediction 1 (memory): memory memory, and a more complex algolusta hent no renm \n",
      "Prediction 2 (LSTM): LSTM ity Lentenewg-ts abilits fong-term meories and can\n",
      "Prediction 3 (complex): complex al handle memory, and a more complex algorithm to \n",
      "Finished 73 epoches, Loss: 0.04025730179646454\n",
      "starting epoch: 73\n",
      "Prediction 1 (memory): memory wemplex algorithm to handle the long/short-term me\n",
      "Prediction 2 (LSTM): LSTM iti fet cetple the long/short-term memories. LSTM \n",
      "Prediction 3 (complex): complex ao alis a fort-term memory, and a more complex alg\n",
      "Finished 74 epoches, Loss: 0.03975136510784879\n",
      "starting epoch: 74\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM it hest-tererom the ronishies and can also resolve\n",
      "Prediction 3 (complex): complex result. LSTM stands for Long-Short Term Memory, LS\n",
      "Finished 75 epoches, Loss: 0.03907170391858455\n",
      "starting epoch: 75\n",
      "Prediction 1 (memory): memory meories and can also resolve exploding/vanishing g\n",
      "Prediction 2 (LSTM): LSTM ity tandstw LSTM lralinguthe long-term memory, and\n",
      "Prediction 3 (complex): complex gesulor LSTM is a form of RNN but differs from tra\n",
      "Finished 76 epoches, Loss: 0.038307132222686384\n",
      "starting epoch: 76\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM is a pem. Lending/vanishing gradients feterompLcx \n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term memories. \n",
      "Finished 77 epoches, Loss: 0.03752479363566213\n",
      "starting epoch: 77\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM itity tenm lo generave to gendiase ts gew atselt. \n",
      "Prediction 3 (complex): complex result. LSTM stands for Long-Short Term Memory, LS\n",
      "Finished 78 epoches, Loss: 0.03679387906435206\n",
      "starting epoch: 78\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM it he thewtrert ngals from traditional RNN by havi\n",
      "Prediction 3 (complex): complex result. LSTM stands for Long-Short Term Memory, LS\n",
      "Finished 79 epoches, Loss: 0.03629906349096667\n",
      "starting epoch: 79\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM ity ten ani fe long-term memory, and a more com le\n",
      "Prediction 3 (complex): complex res and can also resolve exploding/vanishing gradi\n",
      "Finished 80 epoches, Loss: 0.035952448042180424\n",
      "starting epoch: 80\n",
      "Prediction 1 (memory): memory memory, and a  ong-Short Term Memory, LSTM is a fo\n",
      "Prediction 2 (LSTM): LSTM it hing LLndithe cong to generate a new result. LS\n",
      "Prediction 3 (complex): complex result. LSTM stands for Long-Short Term Memory, LS\n",
      "Finished 81 epoches, Loss: 0.03583150606228922\n",
      "starting epoch: 81\n",
      "Prediction 1 (memory): memory momute femeroreet forithe aong-term memory, and a \n",
      "Prediction 2 (LSTM): LSTM it hestandlm te orishm tenm meories and can also r\n",
      "Prediction 3 (complex): complex restandlus fre aes and can al network, the spe res\n",
      "Finished 82 epoches, Loss: 0.035741831814758485\n",
      "starting epoch: 82\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM it. Lenditst LSTM is mote ofmeremory, LSTM is a fo\n",
      "Prediction 3 (complex): complex result. LSTM stands for Long-Short Term Memory, LS\n",
      "Finished 83 epoches, Loss: 0.03560563223899481\n",
      "starting epoch: 83\n",
      "Prediction 1 (memory): memory momperm to oriendst no beng/vemutererecorienm toni\n",
      "Prediction 2 (LSTM): LSTM it htythewtepl ding/vanishing gradients fewandsul \n",
      "Prediction 3 (complex): complex femorat tfera oeut fentands for Long-Short Term Me\n",
      "Finished 84 epoches, Loss: 0.035513504414408294\n",
      "starting epoch: 84\n",
      "Prediction 1 (memory): memory memory, LSTM is a form of RNN but differs from tra\n",
      "Prediction 2 (LSTM): LSTM it hestandl for Lol RNN by having another value re\n",
      "Prediction 3 (complex): complex res and can also resolve exploding/vanishing gradi\n",
      "Finished 85 epoches, Loss: 0.03500153171598314\n",
      "starting epoch: 85\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM lex Let pemwre craling long-te m memories. LSTM is\n",
      "Prediction 3 (complex): complex al orithm to handle the long/short-term memories. \n",
      "Finished 86 epoches, Loss: 0.0351560849375321\n",
      "starting epoch: 86\n",
      "Prediction 1 (memory): memory wemory, and a more complex algorithm to handle the\n",
      "Prediction 2 (LSTM): LSTM iti fet re tent pem ty handle panterm pal alit ty \n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term memories. \n",
      "Finished 87 epoches, Loss: 0.03327500025997727\n",
      "starting epoch: 87\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM it hextteclge ren also teresult. LSTM neN al hesue\n",
      "Prediction 3 (complex): complex alsol RNN al onlto isha fecaving tho a fers ang a \n",
      "Finished 88 epoches, Loss: 0.03705893633459672\n",
      "starting epoch: 88\n",
      "Prediction 1 (memory): memory memory, and a more complex algorithm to handle the\n",
      "Prediction 2 (LSTM): LSTM it hestand LSTM l alst wepcork, te orm te handls a\n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term memories. \n",
      "Finished 89 epoches, Loss: 1.6892394101424524\n",
      "starting epoch: 89\n",
      "Prediction 1 (memory): memory the tong-term meories and can also resolve explodi\n",
      "Prediction 2 (LSTM): LSTM Lhind hettemplilg-tsra resulte uec at handling lon\n",
      "Prediction 3 (complex): complex algorithm to handle the long/short-term memories. \n",
      "Finished 90 epoches, Loss: 5.266189213109803\n",
      "starting epoch: 90\n",
      "Prediction 1 (memory): memory wert pe wert meority tenuthm torm renerate a new r\n",
      "Prediction 2 (LSTM): LSTM ity Landity hentstet-th at bal aLut rytority uenu \n",
      "Prediction 3 (complex): complex results from tres aewuthe toris its abutitiona lon\n",
      "Finished 91 epoches, Loss: 0.06843470370996192\n",
      "starting epoch: 91\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handlind s. vandly, LS\n",
      "Prediction 2 (LSTM): LSTM it hextwom tonure anous ferliong-teroreecerm ce ca\n",
      "Prediction 3 (complex): complex algorithm to handle tho les ality the long-term me\n",
      "Finished 92 epoches, Loss: 2.710576051286514\n",
      "starting epoch: 92\n",
      "Prediction 1 (memory): memory memory, and can also resolve explodins ly of RNN b\n",
      "Prediction 2 (LSTM): LSTM it herteemore pandben andbe the long/short-Term me\n",
      "Prediction 3 (complex): complex algorithm of RNN bs lenus for recurrent nourandsen\n",
      "Finished 93 epoches, Loss: 0.12731599632555127\n",
      "starting epoch: 93\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM LL/yrity melot long-term meories and can also reso\n",
      "Prediction 3 (complex): complex result. LSTM is a form of RNN but differs from tra\n",
      "Finished 94 epoches, Loss: 1.3621635779291077\n",
      "starting epoch: 94\n",
      "Prediction 1 (memory): memory candsenting abo RNNNNN Te handlent no gone abud ff\n",
      "Prediction 2 (LSTM): LSTM its fett LSTM nentalong-term meories and can also \n",
      "Prediction 3 (complex): complex aenerate ang ating aands form of RNN be handle the\n",
      "Finished 95 epoches, Loss: 0.6060193928134048\n",
      "starting epoch: 95\n",
      "Prediction 1 (memory): memory momptrthint aesulte fresel . tter value abuererate\n",
      "Prediction 2 (LSTM): LSTM it hant rempson thm to handle the long/short-term \n",
      "Prediction 3 (complex): complex result. LSTM is a ionathew reng/remory, and a more\n",
      "Finished 96 epoches, Loss: 2.0896568222995238\n",
      "starting epoch: 96\n",
      "Prediction 1 (memory): memory memory, LSTM is better at handling long-term meori\n",
      "Prediction 2 (LSTM): LSTM it hexteeplralit Ta hent newureselor LSTM. RNN sta\n",
      "Prediction 3 (complex): complex result. LSTM stands for recurrent neural network, \n",
      "Finished 97 epoches, Loss: 0.20484432074135975\n",
      "starting epoch: 97\n",
      "Prediction 1 (memory): memory orithe roriene Long-Short Term Memory, LSTM is a f\n",
      "Prediction 2 (LSTM): LSTM it hextorerterm ten and can also resolve exploding\n",
      "Prediction 3 (complex): complex result. LSTM stands for Long-Short Term Memory, LS\n",
      "Finished 98 epoches, Loss: 0.08722082524547155\n",
      "starting epoch: 98\n",
      "Prediction 1 (memory): memory memories. LSTM is better at handling long-term meo\n",
      "Prediction 2 (LSTM): LSTM it hextorkew remorm, ali a Long-Short bo hestandse\n",
      "Prediction 3 (complex): complex result. LSTM standsiforaving about RNN and LSTM. R\n",
      "Finished 99 epoches, Loss: 0.03743429934564509\n",
      "starting epoch: 99\n",
      "Prediction 1 (memory): memory memory, LSTM is a form of RNN but differs from tra\n",
      "Prediction 2 (LSTM): LSTM ity Leptermetrte ani hent neural n thand abuRNN is\n",
      "Prediction 3 (complex): complex result. LSTM stands for Long-Short Term Memory, LS\n",
      "Finished 100 epoches, Loss: 0.03155909387377324\n",
      "Prediction 1 (memory): memory memory, and a more complex algorithm to handle the\n",
      "Prediction 2 (LSTM): LSTM it hextorketl .es al betthe long-term meories and \n",
      "Prediction 3 (complex): complex result. LSTM stands for Long-Short Term Memory, LS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"input.txt\", seq_length)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "rnn.train(data_reader, \\\n",
    "         print_string = \"\"\"\n",
    "print(f\"starting epoch: {epoches}\")\n",
    "predicted = self.predict(data_reader, 'memory ', 50)\n",
    "print(f'Prediction 1 (memory): {predicted}')\n",
    "            \n",
    "predicted = self.predict(data_reader, 'LSTM ', 50)\n",
    "print(f'Prediction 2 (LSTM): {predicted}')\n",
    "            \n",
    "predicted = self.predict(data_reader, 'complex ', 50)\n",
    "print(f'Prediction 3 (complex): {predicted}') \"\"\")\n",
    "\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'memory ', 50)\n",
    "print(f\"Prediction 1 (memory): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'LSTM ', 50)\n",
    "print(f\"Prediction 2 (LSTM): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'complex ', 50)\n",
    "print(f\"Prediction 3 (complex): {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b558ab-c271-4c98-ba87-675dde6faf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Special: LSTM is betttretorm memories. LSTM is better at handling long\n",
      "Prediction Special: Today, we winf as les and femory, LSTM is a form of RNN but di\n"
     ]
    }
   ],
   "source": [
    "predicted = rnn.predict(data_reader, 'LSTM is bet', 50)\n",
    "print(f\"Prediction Special: {predicted}\")\n",
    "predicted = rnn.predict(data_reader, 'Today, we wi', 50)\n",
    "print(f\"Prediction Special: {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e3ea0-3985-414b-bea1-ae3e58a01e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e3ef8b-dfe7-4587-99c0-5ece9941be5f",
   "metadata": {},
   "source": [
    "Train model with even easier input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546a6645-c2f0-4cd7-9933-65a4422ca904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 's', 'i', 'a', 'u', 'w', 'T', 'o', ' ', 'f', 'y', 'c', 't', 'n', 'h', ',', '.', 'l', 'p', 'v', 'm', 'e', 'r', 'g']\n",
      "Finished 1 epoches, Loss: 85.68324409551748\n",
      "Finished 2 epoches, Loss: 83.45195208973722\n",
      "Finished 3 epoches, Loss: 77.18536981675688\n",
      "Finished 4 epoches, Loss: 86.80730814939801\n",
      "Finished 5 epoches, Loss: 67.92017099665605\n",
      "Finished 6 epoches, Loss: 64.25119187597937\n",
      "Finished 7 epoches, Loss: 59.8476005697024\n",
      "Finished 8 epoches, Loss: 49.96565369944913\n",
      "Finished 9 epoches, Loss: 46.723429474771756\n",
      "Finished 10 epoches, Loss: 38.43270715726767\n",
      "Finished 11 epoches, Loss: 32.70403745485998\n",
      "Finished 12 epoches, Loss: 30.2225219600895\n",
      "Finished 13 epoches, Loss: 22.574215652608267\n",
      "Finished 14 epoches, Loss: 20.329430947849424\n",
      "Finished 15 epoches, Loss: 19.20493144012723\n",
      "Finished 16 epoches, Loss: 15.937614534050809\n",
      "Finished 17 epoches, Loss: 15.93332180764688\n",
      "Finished 18 epoches, Loss: 12.269067183504685\n",
      "Finished 19 epoches, Loss: 10.926656360234047\n",
      "Finished 20 epoches, Loss: 9.461762455424463\n",
      "Finished 21 epoches, Loss: 7.485696816324346\n",
      "Finished 22 epoches, Loss: 6.067271369746725\n",
      "Finished 23 epoches, Loss: 6.037984465031359\n",
      "Finished 24 epoches, Loss: 5.296733606706004\n",
      "Finished 25 epoches, Loss: 4.17868688237675\n",
      "Finished 26 epoches, Loss: 3.6067543348995725\n",
      "Finished 27 epoches, Loss: 3.203552897477226\n",
      "Finished 28 epoches, Loss: 2.8673746648957414\n",
      "Finished 29 epoches, Loss: 2.6288943103935223\n",
      "Finished 30 epoches, Loss: 2.4244767900125233\n",
      "Finished 31 epoches, Loss: 2.2451874807742156\n",
      "Finished 32 epoches, Loss: 2.123444245428953\n",
      "Finished 33 epoches, Loss: 1.978544100363607\n",
      "Finished 34 epoches, Loss: 1.8995878112186446\n",
      "Finished 35 epoches, Loss: 1.7734000320525218\n",
      "Finished 36 epoches, Loss: 1.715764495843397\n",
      "Finished 37 epoches, Loss: 1.605941666731741\n",
      "Finished 38 epoches, Loss: 1.5583449151802953\n",
      "Finished 39 epoches, Loss: 1.4671411661433318\n",
      "Finished 40 epoches, Loss: 1.4240434496928018\n",
      "Finished 41 epoches, Loss: 1.351208273535017\n",
      "Finished 42 epoches, Loss: 1.3101995611627713\n",
      "Finished 43 epoches, Loss: 1.2524931419339655\n",
      "Finished 44 epoches, Loss: 1.2130676552862094\n",
      "Finished 45 epoches, Loss: 1.1663577765363606\n",
      "Finished 46 epoches, Loss: 1.12898885586798\n",
      "Finished 47 epoches, Loss: 1.0897542228088886\n",
      "Finished 48 epoches, Loss: 1.0550953962527765\n",
      "Finished 49 epoches, Loss: 1.0209703605066478\n",
      "Finished 50 epoches, Loss: 0.9893383944787286\n",
      "Finished 51 epoches, Loss: 0.9589789103576549\n",
      "Finished 52 epoches, Loss: 0.9303096685526064\n",
      "Finished 53 epoches, Loss: 0.9029842244479822\n",
      "Finished 54 epoches, Loss: 0.8770131440304801\n",
      "Finished 55 epoches, Loss: 0.8522661124984277\n",
      "Finished 56 epoches, Loss: 0.8286760217820814\n",
      "Finished 57 epoches, Loss: 0.8061590487429556\n",
      "Finished 58 epoches, Loss: 0.7846429746859479\n",
      "Finished 59 epoches, Loss: 0.7640583825741375\n",
      "Finished 60 epoches, Loss: 0.744341804526664\n",
      "Finished 61 epoches, Loss: 0.7254355518711653\n",
      "Finished 62 epoches, Loss: 0.7072878664053505\n",
      "Finished 63 epoches, Loss: 0.6898527878501648\n",
      "Finished 64 epoches, Loss: 0.6730896957797429\n",
      "Finished 65 epoches, Loss: 0.6569627858007528\n",
      "Finished 66 epoches, Loss: 0.6414404354049059\n",
      "Finished 67 epoches, Loss: 0.6264945670069885\n",
      "Finished 68 epoches, Loss: 0.6121000110151394\n",
      "Finished 69 epoches, Loss: 0.598233909907849\n",
      "Finished 70 epoches, Loss: 0.5848751807744235\n",
      "Finished 71 epoches, Loss: 0.5720040597519525\n",
      "Finished 72 epoches, Loss: 0.5596017401531882\n",
      "Finished 73 epoches, Loss: 0.5476501088776219\n",
      "Finished 74 epoches, Loss: 0.5361315744690703\n",
      "Finished 75 epoches, Loss: 0.5250289726442333\n",
      "Finished 76 epoches, Loss: 0.5143255301163058\n",
      "Finished 77 epoches, Loss: 0.5040048670458585\n",
      "Finished 78 epoches, Loss: 0.49405102075550683\n",
      "Finished 79 epoches, Loss: 0.48444847759843185\n",
      "Finished 80 epoches, Loss: 0.47518220431632546\n",
      "Finished 81 epoches, Loss: 0.46623767413673955\n",
      "Finished 82 epoches, Loss: 0.45760088566273127\n",
      "Finished 83 epoches, Loss: 0.44925837439425376\n",
      "Finished 84 epoches, Loss: 0.44119721763215064\n",
      "Finished 85 epoches, Loss: 0.43340503387131957\n",
      "Finished 86 epoches, Loss: 0.42586997779926006\n",
      "Finished 87 epoches, Loss: 0.41858073187655875\n",
      "Finished 88 epoches, Loss: 0.41152649528263896\n",
      "Finished 89 epoches, Loss: 0.40469697083165035\n",
      "Finished 90 epoches, Loss: 0.3980823503177286\n",
      "Finished 91 epoches, Loss: 0.3916732986448453\n",
      "Finished 92 epoches, Loss: 0.3854609370257663\n",
      "Finished 93 epoches, Loss: 0.37943682549001767\n",
      "Finished 94 epoches, Loss: 0.37359294491215916\n",
      "Finished 95 epoches, Loss: 0.3679216787525005\n",
      "Finished 96 epoches, Loss: 0.36241579468690527\n",
      "Finished 97 epoches, Loss: 0.35706842628774604\n",
      "Finished 98 epoches, Loss: 0.35187305490241616\n",
      "Finished 99 epoches, Loss: 0.3468234918589108\n",
      "Finished 100 epoches, Loss: 0.3419138611098199\n",
      "Prediction 1 (This): This can now perform well with predicting the outp vued\n",
      "Prediction 2 (simpler): simpler string, hopefully the model can now perform well w\n",
      "Prediction 3 (perform): perform wel  here with predicting the outperform well with\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"input.txt\", seq_length, \\\n",
    "                         data=\"This is a even simpler string, hopefully the model can now perform well with predicting the outputs.\")\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "rnn.train(data_reader)\n",
    "\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'This ', 50)\n",
    "print(f\"Prediction 1 (This): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'simpler ', 50)\n",
    "print(f\"Prediction 2 (simpler): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'perform ', 50)\n",
    "print(f\"Prediction 3 (perform): {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bea65-bfe7-4a69-a108-0f04f78726f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
