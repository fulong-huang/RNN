{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f45fe0-6e50-4894-b79e-589023a3dd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b56d226-ac6f-42e5-8abe-cd0d6eed48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path_en, path_de, len_en, len_de):\n",
    "        self.len_en = len_en\n",
    "        self.len_de = len_de\n",
    "        try:\n",
    "            fp_en = open(path_en, \"r\")\n",
    "            fp_de = open(path_de, \"r\")\n",
    "            data_en = fp_en.read()\n",
    "            data_de = fp_de.read()\n",
    "        except e:\n",
    "            print(f\"FAILED TO FILE\\n {e.what()}\")\n",
    "\n",
    "        # split lines\n",
    "        self.lines_en = data_en.split(\"\\n\")[:100]\n",
    "        self.lines_de = data_de.split(\"\\n\")[:100]\n",
    "        # find unique words\n",
    "        word_set = set()\n",
    "        for line in self.lines_en:\n",
    "            for word in line.split():\n",
    "                word_set.add(word)\n",
    "        words_en = ['-'] + list(word_set)\n",
    "        self.vocab_size_en = len(words_en)\n",
    "        \n",
    "        word_set = set()\n",
    "        for line in self.lines_de:\n",
    "            for word in line.split():\n",
    "                word_set.add(word)\n",
    "        words_de = ['-'] + list(word_set)\n",
    "        self.vocab_size_de = len(words_de)\n",
    "        \n",
    "        # create dictionary mapping for each word\n",
    "        self.word_to_ix_en = {w:i for (i,w) in enumerate(words_en)}\n",
    "        self.ix_to_word_en = {i:w for (i,w) in enumerate(words_en)}\n",
    "        \n",
    "        self.word_to_ix_de = {w:i for (i,w) in enumerate(words_de)}\n",
    "        self.ix_to_word_de = {i:w for (i,w) in enumerate(words_de)}\n",
    "\n",
    "        # total data\n",
    "        self.lines_total = len(self.lines_en)\n",
    "        \n",
    "        #num of unique words\n",
    "        self.vocab_size_en = len(words_en)\n",
    "        self.vocab_size_de = len(words_de)\n",
    "\n",
    "        self.pointer = -1\n",
    "        self.indices = [i for i in range(self.lines_total)]\n",
    "\n",
    "        # close file\n",
    "        fp_en.close()\n",
    "        fp_de.close()\n",
    "\n",
    "    # en -> 17 words max per line\n",
    "    # fr -> 23 words max per line\n",
    "    def next_batch(self):\n",
    "        if self.pointer >= self.lines_total:\n",
    "            self.pointer = -1\n",
    "            random.shuffle(self.indices)\n",
    "            return None, None\n",
    "        inputs = [self.word_to_ix_en[w] for w in self.lines_en[self.indices[self.pointer]].split()]\n",
    "        targets = [self.word_to_ix_de[w] for w in self.lines_de[self.indices[self.pointer]].split()]\n",
    "\n",
    "        # padding\n",
    "        # inputs += [0] * (self.len_en - len(inputs))\n",
    "        targets += [0] * (self.len_de - len(targets))\n",
    "\n",
    "        # increment and return\n",
    "        self.pointer += 1\n",
    "        return inputs, targets\n",
    "        \n",
    "    def get_first_10(self):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for i in range(10):\n",
    "            inputs.append([self.word_to_ix_en[w] for w in self.lines_en[self.indices[i]].split()])\n",
    "            tar = [self.word_to_ix_de[w] for w in self.lines_de[self.indices[i]].split()]\n",
    "            tar += [0] * (self.len_de - len(tar))\n",
    "            targets.append(tar)\n",
    "            \n",
    "        return inputs, targets\n",
    "\n",
    "    \n",
    "    def new_epoch(self):\n",
    "        return self.pointer == -1\n",
    "    def start_epoch(self):\n",
    "        self.pointer = 0\n",
    "    \n",
    "    def validate_input(self, input):\n",
    "        words = input.split()\n",
    "        for word in words:\n",
    "            if not word in self.word_to_ix_en:\n",
    "                print(f\"Word not found: {word}\")\n",
    "                return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f19f368-5036-4bf6-9261-a7357aebfe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, hidden_size, vocab_size_en, seq_length, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size_en\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size_en), np.sqrt(1./vocab_size_en), (hidden_size, vocab_size_en))\n",
    "        # self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size_de, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        # self.c = np.zeros((vocab_size_de, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        # self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        # self.mc = np.zeros_like(self.c)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # xs, hs, os, ycap = {}, {}, {}, {}\n",
    "        xs, hs = {}, {}\n",
    "        hs[-1] = np.zeros((self.hidden_size, 1))\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((self.vocab_size,1))\n",
    "            xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "            hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "            # os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "            # ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "        # return xs, hs, ycap\n",
    "        return xs, hs, hs[len(inputs) - 1]\n",
    "   \n",
    "    def backward(self, xs, hs, dhnext):\n",
    "        # backward pass: compute gradients going backwards\n",
    "        # dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        # dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        # db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dU, dW = np.zeros_like(self.U), np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "        # dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(len(hs) - 1)):\n",
    "            #through softmax\n",
    "            #dh includes gradient from two sides, next cell and current output\n",
    "            dh = dhnext # backprop into h\n",
    "            # backprop through tanh non-linearity \n",
    "            dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "            db += dhrec\n",
    "            #calculate dU and dW\n",
    "            dU += np.dot(dhrec, xs[t].T)\n",
    "            dW += np.dot(dhrec, hs[t-1].T)\n",
    "            #pass the gradient from next cell to the next iteration.\n",
    "            dhnext = np.dot(self.W.T, dhrec)\n",
    "        # clip to mitigate exploding gradients\n",
    "        for dparam in [dU, dW, db]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) \n",
    "        return dU, dW,  db\n",
    "    \n",
    "    def update_model(self, dU, dW, db):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.b],\n",
    "                                  [dU, dW, db],\n",
    "                                  [self.mU, self.mW, self.mb]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "\n",
    "    \n",
    "    def predict(self, data_reader, input):\n",
    "\n",
    "        #initialize input vector\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        words = input.split()\n",
    "        ixes = []\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        for word in words:\n",
    "            ix = data_reader.word_to_ix_en[word]\n",
    "            x[ix] = 1\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            ixes.append(ix)\n",
    "            x[ix] = 0\n",
    "        return h\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371906b3-1ba2-4292-902d-2a342ff63a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class Decoder:\n",
    "    def __init__(self, hidden_size, vocab_size_de, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.vocab_size = vocab_size_de\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./self.vocab_size), np.sqrt(1./self.vocab_size), (hidden_size, self.vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (self.vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((self.vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, hprev):\n",
    "        xs, hs, os, ycap = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        xs[0] = np.zeros((self.vocab_size, 1))\n",
    "        for t in range(self.seq_length):\n",
    "            hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "            os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "            ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "\n",
    "            ix = np.random.choice(range(self.vocab_size), p = ycap[t].ravel())\n",
    "            xs[t+1] = np.zeros((self.vocab_size, 1))\n",
    "            xs[t+1][ix] = 1\n",
    "        return xs, hs, ycap\n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        # backward pass: compute gradients going backwards\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        self.c += 1\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dy = np.copy(ps[t])\n",
    "            #through softmax\n",
    "            dy[targets[t]] -= 1 # backprop into y\n",
    "            #calculate dV, dc\n",
    "            dV += np.dot(dy, hs[t].T)\n",
    "            dc += dc\n",
    "            #dh includes gradient from two sides, next cell and current output\n",
    "            dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "            # backprop through tanh non-linearity \n",
    "            dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "            db += dhrec\n",
    "            #calculate dU and dW\n",
    "            dU += np.dot(dhrec, xs[t].T)\n",
    "            dW += np.dot(dhrec, hs[t-1].T)\n",
    "            #pass the gradient from next cell to the next iteration.\n",
    "            dhnext = np.dot(self.W.T, dhrec)\n",
    "        # clip to mitigate exploding gradients\n",
    "        for dparam in [dU, dW, dV, db, dc]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dU, dW, dV, db, dc, dhnext\n",
    "\n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"loss for a sequence\"\"\"\n",
    "        # calculate cross-entrpy loss\n",
    "        return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "    def predict(self, data_reader, h):\n",
    "        #initialize input vector\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        ixes = []\n",
    "\n",
    "        for t in range(self.seq_length):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            # p = np.exp(y)/np.sum(np.exp(y))\n",
    "            p_shift = np.exp(y - np.max(y))\n",
    "            p = p_shift/p_shift.sum(axis=0)\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = np.zeros((self.vocab_size,1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ' '.join(data_reader.ix_to_word_de[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850305fc-bf85-4009-9c4a-2bec9f3dff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "    def __init__(self, hidden_size_en, hidden_size_de, \\\n",
    "                 vocab_size_en, vocab_size_de, \\\n",
    "                 seq_length_en, seq_length_de, \\\n",
    "                 learning_rate_en, learning_rate_de):\n",
    "        # hyper parameters\n",
    "        self.hidden_size_en = hidden_size_en\n",
    "        self.hidden_size_de = hidden_size_de\n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.vocab_size_de = vocab_size_de\n",
    "        self.seq_length_en = seq_length_en\n",
    "        self.seq_length_de = seq_length_de\n",
    "        self.learning_rate_en = learning_rate_en\n",
    "        self.learning_rate_de = learning_rate_de\n",
    "        # encoder / decoder\n",
    "        self.encoder = Encoder(hidden_size = hidden_size_en, \\\n",
    "                               vocab_size_en = vocab_size_en, \\\n",
    "                               seq_length = seq_length_en, \\\n",
    "                               learning_rate = learning_rate_en)\n",
    "        self.decoder = Decoder(hidden_size = hidden_size_de, \\\n",
    "                               vocab_size_de = vocab_size_de, \\\n",
    "                               seq_length = seq_length_de, \\\n",
    "                               learning_rate = learning_rate_de)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        xs_en, hs_en, last_hs = self.encoder.forward(inputs)\n",
    "        xs_de, hs_de, ycap_de = self.decoder.forward(last_hs)\n",
    "\n",
    "        return xs_en, hs_en, xs_de, hs_de, ycap_de\n",
    "        \n",
    "        \n",
    "    def backward(self, xs_en, xs_de, hs_en, hs_de, ps_de, targets):\n",
    "        dU_de, dW_de, dV_de, db_de, dc_de, dh = self.decoder.backward(xs_de, hs_de, ps_de, targets)\n",
    "        dU_en, dW_en, db_en = self.encoder.backward(xs_en, hs_en, dh)\n",
    "        return dU_en, dW_en, db_en, dU_de, dW_de, dV_de, db_de, dc_de\n",
    "        \n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"loss for a sequence\"\"\"\n",
    "        # calculate cross-entrpy loss\n",
    "        # ps_len = len(ps)\n",
    "        # tar_len = len(targets)\n",
    "        # if ps_len > tar_len:\n",
    "        #     targets = targets + np.zeros((self.vocab_size, 1))*(ps_len - tar_len)\n",
    "        # elif ps_len < tar_len:\n",
    "        #     ps = ps + np.zeros((self.vocab_size, 1))*(tar_len - ps_len)\n",
    "            \n",
    "        return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length_de))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU_en, dW_en, db_en, dU_de, dW_de, dV_de, db_de, dc_de):\n",
    "        self.encoder.update_model(dU_en, dW_en, db_en)\n",
    "        self.decoder.update_model(dU_de, dW_de, dV_de, db_de, dc_de)\n",
    "        \n",
    "\n",
    "\n",
    "    def train(self, data_reader, threshold = 0.01, epoch=100):\n",
    "        iter_num = 1\n",
    "        \n",
    "        while epoch != 0:\n",
    "            data_reader.start_epoch()\n",
    "            inputs, targets = data_reader.next_batch()\n",
    "            while not data_reader.new_epoch():\n",
    "                xs_en, hs_en, xs_de, hs_de, ps_de = self.forward(inputs)\n",
    "                dU_en, dW_en, db_en, dU_de, dW_de, dV_de, db_de, dc_de = \\\n",
    "                            self.backward(xs_en, xs_de, hs_en, hs_de, ps_de, targets)\n",
    "                loss = self.loss(ps_de, targets)\n",
    "                self.update_model(dU_en, dW_en, db_en, dU_de, dW_de, dV_de, db_de, dc_de)\n",
    "                \n",
    "                inputs, targets = data_reader.next_batch()\n",
    "\n",
    "            if iter_num % 1000 == 0:\n",
    "                i, t = data_reader.get_first_10()\n",
    "                total_loss = 0\n",
    "                for idx in range(len(i)):\n",
    "                    _, _, _, _, y_h = self.forward(i[idx])\n",
    "                    total_loss += self.loss(y_h, t[idx])\n",
    "                    \n",
    "                print(f\"========== Epoch {iter_num} completed, average first 10 loss: {total_loss / 10} ==========\")\n",
    "                text_to_translate = \"new jersey is sometimes quiet during autumn , and it is snowy in april .\"\n",
    "                if data_reader.validate_input(text_to_translate):\n",
    "                    print(f\"ENG: {text_to_translate}\")\n",
    "                    print(self.predict(data_reader, text_to_translate))\n",
    "                text_to_translate = \"the united states is usually chilly during july , and it is usually freezing in november .\"\n",
    "                if data_reader.validate_input(text_to_translate):\n",
    "                    print(f\"ENG: {text_to_translate}\")\n",
    "                    print(self.predict(data_reader, text_to_translate))\n",
    "                print()\n",
    "\n",
    "            iter_num += 1\n",
    "            epoch -= (epoch >= 0)\n",
    "\n",
    "    def predict(self, data_reader, input):\n",
    "        h = self.encoder.predict(data_reader, input)\n",
    "        result = self.decoder.predict(data_reader, h)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "529a4220-2fb7-4106-a5dc-a5ad535058e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '-', '.', 'a', 'and', 'animal', 'animals', 'apple', 'apples', 'april', 'are', 'august', 'autumn', 'banana', 'bananas', 'bananas.', 'beautiful', 'busy', 'but', 'california', 'car', 'cat', 'chilly', 'china', 'cold', 'december', 'dislike', 'disliked', 'dislikes', 'driving', 'dry', 'during', 'elephants', 'fall', 'favorite', 'favorite.', 'feared', 'february', 'france', 'freezing', 'fruit', 'going', 'grape', 'grapefruit', 'grapes', 'he', 'her', 'his', 'hot', 'i', 'in', 'india', 'is', 'it', 'january', 'jersey', 'july', 'june', 'least', 'lemon', 'lemons', 'like', 'liked', 'likes', 'lime', 'limes', 'little', 'loved', 'mango', 'mangoes', 'mangoes.', 'march', 'may', 'mild', 'most', 'my', 'never', 'new', 'next', 'nice', 'november', 'october', 'old', 'orange', 'oranges', 'our', 'paris', 'peaches', 'pear', 'pears', 'plan', 'pleasant', 'quiet', 'rainy', 'red', 'relaxing', 'rusty', 'saw', 'september', 'shark', 'she', 'snowy', 'sometimes', 'spring', 'states', 'strawberries', 'strawberry', 'summer', 'that', 'the', 'their', 'they', 'to', 'truck', 'united', 'usually', 'visit', 'warm', 'was', 'we', 'were', 'wet', 'winter', 'wonderful', 'yellow', 'your']\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "input_file_name_en = \"small_vocab_en.txt\"\n",
    "input_file_name_de = \"small_vocab_fr.txt\"\n",
    "\n",
    "seq_length_en = 24\n",
    "seq_length_de = 24\n",
    "\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(input_file_name_en, input_file_name_de, seq_length_en, seq_length_de)\n",
    "\n",
    "valid_words = [k for k in data_reader.word_to_ix_en.keys()]\n",
    "valid_words.sort()\n",
    "print(valid_words)\n",
    "print(len(valid_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96e3ea0-3985-414b-bea1-ae3e58a01e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size_en = 300\n",
    "hidden_size_de = 300\n",
    "learning_rate_en = 0.009\n",
    "learning_rate_de = 0.009\n",
    "\n",
    "\n",
    "rnn = Translator(hidden_size_en, hidden_size_de, \\\n",
    "                 data_reader.vocab_size_en, data_reader.vocab_size_de, \\\n",
    "                 seq_length_en, seq_length_de, \\\n",
    "                 learning_rate_en = learning_rate_en, learning_rate_de = learning_rate_de)\n",
    "              \n",
    "# rnn.train(data_reader)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "998c6ab5-d4a1-429b-97b2-fe565003fad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hidden_size_en = 300\n",
    "# hidden_size_de = 300\n",
    "# learning_rate_en = 0.009\n",
    "# learning_rate_de = 0.009\n",
    "\n",
    "# # learning_rates = [0.02, 0.01, 0.009]\n",
    "# # learning_rates = [0.05, 0.03, 0.01, 0.008, 0.005]\n",
    "# learning_rates = [ 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# for lr in learning_rates:\n",
    "#     print(\"#########################################################\")\n",
    "#     print(f\"########################## {lr} ############################\")\n",
    "#     print(\"#########################################################\")\n",
    "#     data_reader = DataReader(input_file_name_en, input_file_name_de, seq_length_en, seq_length_de)\n",
    "#     rnn = Translator(hidden_size_en, hidden_size_de, \\\n",
    "#                      data_reader.vocab_size_en, data_reader.vocab_size_de, \\\n",
    "#                      seq_length_en, seq_length_de, \\\n",
    "#                      learning_rate_en = lr, learning_rate_de = lr)\n",
    "                     \n",
    "#     rnn.train(data_reader, epoch = 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "934d1077-6fa1-4b27-8f53-8f3ff5ae6a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Epoch 1000 completed, average first 10 loss: 13.00811910749232 ==========\n",
      "ENG: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est gÃ©nÃ©ralement froid pendant mois de et il et il le doux automne . . . - - - - - -\n",
      "ENG: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "les Ã©tats-unis est jamais poire en janvier , et il est gÃ©nÃ©ralement habituellement en avril . - - - - - - - -\n",
      "\n",
      "========== Epoch 2000 completed, average first 10 loss: 4.146046640127813 ==========\n",
      "ENG: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est froid chaud pendant mars automne , et il est neigeux l' avril . - - - - - - - -\n",
      "ENG: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "les Ã©tats-unis est parfois occupÃ© en juillet , et il est habituellement en novembre . - - - - - - - - -\n",
      "\n",
      "========== Epoch 3000 completed, average first 10 loss: 1.1837611187632273 ==========\n",
      "ENG: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en automne . - - - - - - - -\n",
      "ENG: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "les Ã©tats-unis est parfois froid en juillet , et il gÃ¨le habituellement en novembre . - - - - - - - - -\n",
      "\n",
      "========== Epoch 4000 completed, average first 10 loss: 0.44382139551786476 ==========\n",
      "ENG: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril . - - - - - - - -\n",
      "ENG: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "les Ã©tats-unis est gÃ©nÃ©ralement froid en mars , et il gÃ¨le habituellement en novembre . - - - - - - - - -\n",
      "\n",
      "========== Epoch 5000 completed, average first 10 loss: 0.2772416180552878 ==========\n",
      "ENG: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril . - - - - - - - -\n",
      "ENG: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "les Ã©tats-unis est gÃ©nÃ©ralement froid en juillet , et il gÃ¨le habituellement en novembre . - - - - - - - - -\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# rnn.train(data_reader, 5000)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# rnn.encoder.learning_rate *= 100\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# rnn.encoder.learning_rate = 0.005\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# rnn.decoder.learning_rate = 0.005\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_reader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 67\u001b[0m, in \u001b[0;36mTranslator.train\u001b[1;34m(self, data_reader, threshold, epoch)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_reader\u001b[38;5;241m.\u001b[39mnew_epoch():\n\u001b[0;32m     65\u001b[0m     xs_en, hs_en, xs_de, hs_de, ps_de \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[0;32m     66\u001b[0m     dU_en, dW_en, db_en, dU_de, dW_de, dV_de, db_de, dc_de \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 67\u001b[0m                 \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs_en\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs_de\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs_en\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs_de\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mps_de\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(ps_de, targets)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_model(dU_en, dW_en, db_en, dU_de, dW_de, dV_de, db_de, dc_de)\n",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m, in \u001b[0;36mTranslator.backward\u001b[1;34m(self, xs_en, xs_de, hs_en, hs_de, ps_de, targets)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs_en, xs_de, hs_en, hs_de, ps_de, targets):\n\u001b[1;32m---> 34\u001b[0m     dU_de, dW_de, dV_de, db_de, dc_de, dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs_de\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs_de\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mps_de\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     dU_en, dW_en, db_en \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mbackward(xs_en, hs_en, dh)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dU_en, dW_en, db_en, dU_de, dW_de, dV_de, db_de, dc_de\n",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m, in \u001b[0;36mDecoder.backward\u001b[1;34m(self, xs, hs, ps, targets)\u001b[0m\n\u001b[0;32m     54\u001b[0m dc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dc\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#dh includes gradient from two sides, next cell and current output\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m dhnext \u001b[38;5;66;03m# backprop into h\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# backprop through tanh non-linearity \u001b[39;00m\n\u001b[0;32m     58\u001b[0m dhrec \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m hs[t] \u001b[38;5;241m*\u001b[39m hs[t]) \u001b[38;5;241m*\u001b[39m dh  \u001b[38;5;66;03m#dhrec is the term used in many equations\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# rnn.train(data_reader, 5000)\n",
    "\n",
    "# rnn.encoder.learning_rate *= 100\n",
    "\n",
    "# def train(self, data_reader, threshold = 0.01, len=-1, converge_limit=2000):\n",
    "\n",
    "# rnn.encoder.learning_rate = 0.005\n",
    "# rnn.decoder.learning_rate = 0.005\n",
    "rnn.train(data_reader, epoch = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05bd226-bc74-4955-8ab7-1689eb6eb985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63b82eba-abd3-4836-9818-c1d462ac9745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril . - - - - - - - -\n",
      "les Ã©tats-unis est gÃ©nÃ©ralement froid en juillet , et il gÃ¨le habituellement en novembre . - - - - - - - - -\n"
     ]
    }
   ],
   "source": [
    "text_to_translate = \"new jersey is sometimes quiet during autumn , and it is snowy in april .\"\n",
    "if data_reader.validate_input(text_to_translate):\n",
    "    print(rnn.predict(data_reader, text_to_translate))\n",
    "text_to_translate = \"the united states is usually chilly during july , and it is usually freezing in november .\"\n",
    "if data_reader.validate_input(text_to_translate):\n",
    "    print(rnn.predict(data_reader, text_to_translate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cfac4bb-0016-4386-8388-2997be9fdb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "son fruit est moins aimÃ© le citron , mais mon moins aimÃ© est le raisin . - - - - - - - -\n"
     ]
    }
   ],
   "source": [
    "text_to_translate = \"our least liked fruit is the lemon , but my least liked is the grape .\"\n",
    "if data_reader.validate_input(text_to_translate):\n",
    "    print(rnn.predict(data_reader, text_to_translate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a4fc0b7-fa22-4a59-8cbe-fa7c5f176c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la est est gÃ©nÃ©ralement pluvieux en fÃ©vrier , mais il est jamais relaxant en fÃ©vrier . - - - - - - - -\n",
      "la france est gÃ©nÃ©ralement pluvieux en fÃ©vrier , mais il est jamais relaxant en juillet . - - - - - - - -\n",
      "la france est gÃ©nÃ©ralement pendant en fÃ©vrier , mais il est jamais relaxant en juillet . - - - - - - - -\n",
      "l' france est gÃ©nÃ©ralement calme en novembre , mais il est jamais relaxant en juillet . - - - - - - - -\n",
      "la france est gÃ©nÃ©ralement agrÃ©able en novembre , mais il est jamais relaxant en juillet . - - - - - - - -\n"
     ]
    }
   ],
   "source": [
    "text_to_translate = \"california is usually hot during december , and it is never dry in autumn . \"\n",
    "if data_reader.validate_input(text_to_translate):\n",
    "    print(rnn.predict(data_reader, text_to_translate))\n",
    "\n",
    "text_to_translate = \"california is usually hot during december , and it is never dry in autumn . \"\n",
    "if data_reader.validate_input(text_to_translate):\n",
    "    print(rnn.predict(data_reader, text_to_translate))\n",
    "\n",
    "text_to_translate = \"california is usually hot during december , and it is never dry in autumn . \"\n",
    "if data_reader.validate_input(text_to_translate):\n",
    "    print(rnn.predict(data_reader, text_to_translate))\n",
    "\n",
    "text_to_translate = \"california is usually hot during december , and it is never dry in autumn . \"\n",
    "if data_reader.validate_input(text_to_translate):\n",
    "    print(rnn.predict(data_reader, text_to_translate))\n",
    "\n",
    "text_to_translate = \"california is usually hot during december , and it is never dry in autumn . \"\n",
    "if data_reader.validate_input(text_to_translate):\n",
    "    print(rnn.predict(data_reader, text_to_translate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "061279f3-656f-41a2-9b35-ff4a2d00e15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '-', '.', 'a', 'and', 'animal', 'animals', 'apple', 'apples', 'april', 'are', 'august', 'autumn', 'banana', 'bananas', 'bananas.', 'beautiful', 'busy', 'but', 'california', 'car', 'cat', 'chilly', 'china', 'cold', 'december', 'dislike', 'disliked', 'dislikes', 'driving', 'dry', 'during', 'elephants', 'fall', 'favorite', 'favorite.', 'feared', 'february', 'france', 'freezing', 'fruit', 'going', 'grape', 'grapefruit', 'grapes', 'he', 'her', 'his', 'hot', 'i', 'in', 'india', 'is', 'it', 'january', 'jersey', 'july', 'june', 'least', 'lemon', 'lemons', 'like', 'liked', 'likes', 'lime', 'limes', 'little', 'loved', 'mango', 'mangoes', 'mangoes.', 'march', 'may', 'mild', 'most', 'my', 'never', 'new', 'next', 'nice', 'november', 'october', 'old', 'orange', 'oranges', 'our', 'paris', 'peaches', 'pear', 'pears', 'plan', 'pleasant', 'quiet', 'rainy', 'red', 'relaxing', 'rusty', 'saw', 'september', 'shark', 'she', 'snowy', 'sometimes', 'spring', 'states', 'strawberries', 'strawberry', 'summer', 'that', 'the', 'their', 'they', 'to', 'truck', 'united', 'usually', 'visit', 'warm', 'was', 'we', 'were', 'wet', 'winter', 'wonderful', 'yellow', 'your']\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "input_file_name_en = \"small_vocab_en.txt\"\n",
    "input_file_name_de = \"small_vocab_fr.txt\"\n",
    "\n",
    "seq_length_en = 24\n",
    "seq_length_de = 24\n",
    "\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(input_file_name_en, input_file_name_de, seq_length_en, seq_length_de)\n",
    "\n",
    "valid_words = [k for k in data_reader.word_to_ix_en.keys()]\n",
    "valid_words.sort()\n",
    "print(valid_words)\n",
    "print(len(valid_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb10eb45-d32d-4c9e-b6f7-7163f1299ae9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58708eb-7384-4922-baea-fac2b3d777a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d6a9c-abbf-43c0-89f2-70239393f6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d473615-ec7a-4d25-9b55-e914d4ef960c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
