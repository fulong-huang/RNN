{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f45fe0-6e50-4894-b79e-589023a3dd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b56d226-ac6f-42e5-8abe-cd0d6eed48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path_en, path_fr, len_en, len_fr):\n",
    "        self.epoch = 1\n",
    "        self.len_en = len_en\n",
    "        self.len_fr = len_fr\n",
    "        try:\n",
    "            fp_en = open(path_en, \"r\")\n",
    "            fp_fr = open(path_fr, \"r\")\n",
    "            data_en = fp_en.read()\n",
    "            data_fr = fp_fr.read()\n",
    "        except e:\n",
    "            print(f\"FAILED TO FILE\\n {e.what()}\")\n",
    "\n",
    "        # split lines\n",
    "        self.lines_en = data_en.split(\"\\n\")[:100]\n",
    "        self.lines_fr = data_fr.split(\"\\n\")[:100]\n",
    "        \n",
    "        # find unique words\n",
    "        word_set = set()\n",
    "        for line in self.lines_en:\n",
    "            for word in line.split():\n",
    "                word_set.add(word)\n",
    "        words_en = ['-'] + list(word_set)\n",
    "        self.vocab_size_en = len(words_en)\n",
    "        \n",
    "        word_set = set()\n",
    "        for line in self.lines_fr:\n",
    "            for word in line.split():\n",
    "                word_set.add(word)\n",
    "        words_fr = ['-'] + list(word_set)\n",
    "        self.vocab_size_fr = len(words_fr)\n",
    "        \n",
    "        # create dictionary mapping for each word\n",
    "        self.word_to_ix_en = {w:i for (i,w) in enumerate(words_en)}\n",
    "        self.ix_to_word_en = {i:w for (i,w) in enumerate(words_en)}\n",
    "        \n",
    "        self.word_to_ix_fr = {w:i for (i,w) in enumerate(words_fr)}\n",
    "        self.ix_to_word_fr = {i:w for (i,w) in enumerate(words_fr)}\n",
    "\n",
    "        # total data\n",
    "        self.lines_total = len(self.lines_en)\n",
    "        \n",
    "        #num of unique words\n",
    "        self.vocab_size_en = len(words_en)\n",
    "        self.vocab_size_fr = len(words_fr)\n",
    "\n",
    "        self.pointer = 0\n",
    "        self.indices = [i for i in range(self.lines_total)]\n",
    "\n",
    "        # close file\n",
    "        fp_en.close()\n",
    "        fp_fr.close()\n",
    "\n",
    "    # en -> 17 words max per line\n",
    "    # fr -> 23 words max per line\n",
    "    def next_batch(self):\n",
    "        if self.pointer >= self.lines_total:\n",
    "            self.pointer = 0\n",
    "            random.shuffle(self.indices)\n",
    "            self.epoch += 1\n",
    "        inputs = [self.word_to_ix_en[w] for w in self.lines_en[self.indices[self.pointer]].split()]\n",
    "        targets = [self.word_to_ix_fr[w] for w in self.lines_fr[self.indices[self.pointer]].split()]\n",
    "\n",
    "        # padding\n",
    "        inputs += [0] * (self.len_en - len(inputs))\n",
    "        targets += [0] * (self.len_fr - len(targets))\n",
    "\n",
    "        # increment and return\n",
    "        self.pointer += 1\n",
    "        return inputs, targets\n",
    "\n",
    "    def validate_input(self, input):\n",
    "        words = input.split()\n",
    "        for word in words:\n",
    "            if not word in self.word_to_ix_en:\n",
    "                print(f\"Word not found: {word}\")\n",
    "                return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f19f368-5036-4bf6-9261-a7357aebfe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, hidden_size, vocab_size_en, seq_length, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size_en\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size_en), np.sqrt(1./vocab_size_en), (hidden_size, vocab_size_en))\n",
    "        # self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size_fr, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        # self.c = np.zeros((vocab_size_fr, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        # self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        # self.mc = np.zeros_like(self.c)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # xs, hs, os, ycap = {}, {}, {}, {}\n",
    "        xs, hs = {}, {}\n",
    "        hs[-1] = np.zeros((self.hidden_size, 1))\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((self.vocab_size,1))\n",
    "            xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "            hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "            # os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "            # ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "        # return xs, hs, ycap\n",
    "        return xs, hs[len(inputs) - 1]\n",
    "   \n",
    "    def backward(self, xs, hs, dhnext):\n",
    "        # backward pass: compute gradients going backwards\n",
    "        # dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        # dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        # db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dU, dW = np.zeros_like(self.U), np.zeros_like(self.W)\n",
    "        dU, dW = np.zeros_like(self.U), np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "        # dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            #through softmax\n",
    "            #dh includes gradient from two sides, next cell and current output\n",
    "            dh = dhnext # backprop into h\n",
    "            # backprop through tanh non-linearity \n",
    "            dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "            db += dhrec\n",
    "            #calculate dU and dW\n",
    "            dU += np.dot(dhrec, xs[t].T)\n",
    "            dW += np.dot(dhrec, hs[t-1].T)\n",
    "            #pass the gradient from next cell to the next iteration.\n",
    "            dhnext = np.dot(self.W.T, dhrec)\n",
    "        # clip to mitigate exploding gradients\n",
    "        for dparam in [dU, dW, db]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) \n",
    "        return dU, dW,  db\n",
    "    \n",
    "    def update_model(self, dU, dW, db):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.b],\n",
    "                                  [dU, dW, db],\n",
    "                                  [self.mU, self.mW, self.mb]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "\n",
    "    \n",
    "    def predict(self, data_reader, input):\n",
    "\n",
    "        #initialize input vector\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        words = input.split()\n",
    "        ixes = []\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        for word in words:\n",
    "            ix = data_reader.word_to_ix_en[word]\n",
    "            x[ix] = 1\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            ixes.append(ix)\n",
    "            x[ix] = 0\n",
    "        return h\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "371906b3-1ba2-4292-902d-2a342ff63a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class Decoder:\n",
    "    def __init__(self, hidden_size, vocab_size_fr, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.vocab_size = vocab_size_fr\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./self.vocab_size), np.sqrt(1./self.vocab_size), (hidden_size, self.vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (self.vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((self.vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "        xs, hs, os, ycap = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((self.vocab_size,1))\n",
    "            xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "            hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "            os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "            ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "        return xs, hs, ycap\n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        # backward pass: compute gradients going backwards\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        self.c += 1\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dy = np.copy(ps[t])\n",
    "            #through softmax\n",
    "            dy[targets[t]] -= 1 # backprop into y\n",
    "            #calculate dV, dc\n",
    "            dV += np.dot(dy, hs[t].T)\n",
    "            dc += dc\n",
    "            #dh includes gradient from two sides, next cell and current output\n",
    "            dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "            # backprop through tanh non-linearity \n",
    "            dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "            db += dhrec\n",
    "            #calculate dU and dW\n",
    "            dU += np.dot(dhrec, xs[t].T)\n",
    "            dW += np.dot(dhrec, hs[t-1].T)\n",
    "            #pass the gradient from next cell to the next iteration.\n",
    "            dhnext = np.dot(self.W.T, dhrec)\n",
    "        # clip to mitigate exploding gradients\n",
    "        for dparam in [dU, dW, dV, db, dc]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dU, dW, dV, db, dc, dhnext\n",
    "\n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"loss for a sequence\"\"\"\n",
    "        # calculate cross-entrpy loss\n",
    "        return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "    def predict(self, data_reader, h):\n",
    "        #initialize input vector\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        ixes = []\n",
    "\n",
    "        for t in range(self.seq_length):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            # p = np.exp(y)/np.sum(np.exp(y))\n",
    "            p_shift = np.exp(y - np.max(y))\n",
    "            p = p_shift/p_shift.sum(axis=0)\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = np.zeros((self.vocab_size,1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ' '.join(data_reader.ix_to_word_fr[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850305fc-bf85-4009-9c4a-2bec9f3dff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator:\n",
    "    def __init__(self, hidden_size_en, hidden_size_fr, \\\n",
    "                 vocab_size_en, vocab_size_fr, \\\n",
    "                 seq_length_en, seq_length_fr, \\\n",
    "                 learning_rate_en, learning_rate_fr):\n",
    "        # hyper parameters\n",
    "        self.hidden_size_en = hidden_size_en\n",
    "        self.hidden_size_fr = hidden_size_fr\n",
    "        self.vocab_size_en = vocab_size_en\n",
    "        self.vocab_size_fr = vocab_size_fr\n",
    "        self.seq_length_en = seq_length_en\n",
    "        self.seq_length_fr = seq_length_fr\n",
    "        self.learning_rate_en = learning_rate_en\n",
    "        self.learning_rate_fr = learning_rate_fr\n",
    "        # encoder / decoder\n",
    "        self.encoder = Encoder(hidden_size = hidden_size_en, \\\n",
    "                               vocab_size_en = vocab_size_en, \\\n",
    "                               seq_length = seq_length_en, \\\n",
    "                               learning_rate = learning_rate_en)\n",
    "        self.decoder = Decoder(hidden_size = hidden_size_fr, \\\n",
    "                               vocab_size_fr = vocab_size_fr, \\\n",
    "                               seq_length = seq_length_fr, \\\n",
    "                               learning_rate = learning_rate_fr)\n",
    "        self.smooth_loss_data = []\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        xs_en, hs_en = self.encoder.forward(inputs)\n",
    "        xs_fr, hs_fr, ycap_fr = self.decoder.forward(inputs, hs_en)\n",
    "\n",
    "        return xs_en, hs_en, xs_fr, hs_fr, ycap_fr\n",
    "        \n",
    "        \n",
    "    def backward(self, xs_en, xs_fr, hs_en, hs_fr, ps_fr, targets):\n",
    "        dU_fr, dW_fr, dV_fr, db_fr, dc_fr, dh = self.decoder.backward(xs_fr, hs_fr, ps_fr, targets)\n",
    "        dU_en, dW_en, db_en = self.encoder.backward(xs_en, hs_en, dh)\n",
    "        return dU_en, dW_en, db_en, dU_fr, dW_fr, dV_fr, db_fr, dc_fr\n",
    "        \n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "            \"\"\"loss for a sequence\"\"\"\n",
    "            # calculate cross-entrpy loss\n",
    "            return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length_fr))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU_en, dW_en, db_en, dU_fr, dW_fr, dV_fr, db_fr, dc_fr):\n",
    "        self.encoder.update_model(dU_en, dW_en, db_en)\n",
    "        self.decoder.update_model(dU_fr, dW_fr, dV_fr, db_fr, dc_fr)\n",
    "        \n",
    "\n",
    "\n",
    "    def train(self, data_reader, threshold = 0.01, len=-1, converge_limit=2000):\n",
    "        iter_num = 0\n",
    "        smooth_loss = -np.log(1.0/data_reader.vocab_size_fr)*self.seq_length_fr\n",
    "\n",
    "        ###########################################################\n",
    "        converge_count = 0\n",
    "        min_loss = 100000\n",
    "        ###########################################################\n",
    "        self.smooth_loss_data = []\n",
    "        while (smooth_loss > threshold):\n",
    "            if len > 0 and len <= iter_num:\n",
    "                break\n",
    "            if smooth_loss < min_loss:\n",
    "                min_loss = smooth_loss\n",
    "                converge_count = 0\n",
    "            else:\n",
    "                if converge_count >= converge_limit:\n",
    "                    print(f\"Model seems to converge. Min loss: {min_loss}, Curr loss: {smooth_loss}\")\n",
    "                    break \n",
    "                converge_count += 1\n",
    "            \n",
    "            inputs, targets = data_reader.next_batch()\n",
    "            xs_en, hs_en, xs_fr, hs_fr, ps_fr = self.forward(inputs)\n",
    "            dU_en, dW_en, db_en, dU_fr, dW_fr, dV_fr, db_fr, dc_fr = \\\n",
    "                        self.backward(xs_en, xs_fr, hs_en, hs_fr, ps_fr, targets)\n",
    "            loss = self.loss(ps_fr, targets)\n",
    "            self.update_model(dU_en, dW_en, db_en, dU_fr, dW_fr, dV_fr, db_fr, dc_fr)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            \n",
    "            if iter_num % 2000 == 0:\n",
    "                print( \"\\n\\niter :%d, loss:%f, min loss:%f\"%(iter_num, smooth_loss, min_loss))\n",
    "                self.smooth_loss_data.append(smooth_loss)\n",
    "\n",
    "            iter_num += 1\n",
    "        return self.smooth_loss_data\n",
    "\n",
    "    def predict(self, data_reader, input):\n",
    "        h = self.encoder.predict(data_reader, input)\n",
    "        result = self.decoder.predict(data_reader, h)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529a4220-2fb7-4106-a5dc-a5ad535058e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_file_name_en = \"small_vocab_en.txt\"\n",
    "input_file_name_fr = \"small_vocab_fr.txt\"\n",
    "\n",
    "seq_length_en = 24\n",
    "seq_length_fr = 24\n",
    "\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(input_file_name_en, input_file_name_fr, seq_length_en, seq_length_fr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96e3ea0-3985-414b-bea1-ae3e58a01e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hidden_size_en = 100\n",
    "# hidden_size_fr = 100\n",
    "# learning_rate_en = 1e-1\n",
    "# learning_rate_fr = 1e-1\n",
    "\n",
    "\n",
    "# rnn = Translator(hidden_size_en, hidden_size_fr, \\\n",
    "#                  data_reader.vocab_size_en, data_reader.vocab_size_fr, \\\n",
    "#                  seq_length_en, seq_length_fr, \\\n",
    "#                  learning_rate_en = learning_rate_en, learning_rate_fr = learning_rate_fr)\n",
    "              \n",
    "# rnn.train(data_reader)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "998c6ab5-d4a1-429b-97b2-fe565003fad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_size_en = 150\n",
    "hidden_size_fr = 150\n",
    "learning_rate_en = 0.1\n",
    "learning_rate_fr = 0.1\n",
    "\n",
    "\n",
    "rnn = Translator(hidden_size_en, hidden_size_fr, \\\n",
    "                 data_reader.vocab_size_en, data_reader.vocab_size_fr, \\\n",
    "                 seq_length_en, seq_length_fr, \\\n",
    "                 learning_rate_en = learning_rate_en, learning_rate_fr = learning_rate_fr)\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "934d1077-6fa1-4b27-8f53-8f3ff5ae6a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iter :0, loss:121.350034, min loss:121.349899\n",
      "\n",
      "\n",
      "iter :2000, loss:48.094378, min loss:48.069890\n",
      "\n",
      "\n",
      "iter :4000, loss:23.368951, min loss:23.368834\n",
      "\n",
      "\n",
      "iter :6000, loss:11.126603, min loss:11.131668\n",
      "\n",
      "\n",
      "iter :8000, loss:5.290554, min loss:5.295178\n",
      "\n",
      "\n",
      "iter :10000, loss:3.112351, min loss:3.107917\n",
      "\n",
      "\n",
      "iter :12000, loss:2.263723, min loss:2.264629\n",
      "\n",
      "\n",
      "iter :14000, loss:1.891454, min loss:1.882803\n",
      "\n",
      "\n",
      "iter :16000, loss:1.705248, min loss:1.692067\n",
      "\n",
      "\n",
      "iter :18000, loss:1.591489, min loss:1.582900\n",
      "\n",
      "\n",
      "iter :20000, loss:1.517235, min loss:1.512448\n",
      "\n",
      "\n",
      "iter :22000, loss:1.464091, min loss:1.449140\n",
      "\n",
      "\n",
      "iter :24000, loss:1.420790, min loss:1.401441\n",
      "\n",
      "\n",
      "iter :26000, loss:1.394344, min loss:1.374435\n",
      "\n",
      "\n",
      "iter :28000, loss:1.365034, min loss:1.344080\n",
      "\n",
      "\n",
      "iter :30000, loss:1.342553, min loss:1.330034\n",
      "\n",
      "\n",
      "iter :32000, loss:1.321578, min loss:1.314960\n",
      "\n",
      "\n",
      "iter :34000, loss:1.309348, min loss:1.279704\n",
      "Model seems to converge. Min loss: 1.279703564982453, Curr loss: 1.301549939212216\n",
      "\n",
      "\n",
      "iter :0, loss:121.231990, min loss:121.349899\n",
      "\n",
      "\n",
      "iter :2000, loss:17.421275, min loss:17.438559\n",
      "\n",
      "\n",
      "iter :4000, loss:3.380224, min loss:3.383518\n",
      "\n",
      "\n",
      "iter :6000, loss:1.470442, min loss:1.462369\n",
      "\n",
      "\n",
      "iter :8000, loss:1.212083, min loss:1.203622\n",
      "\n",
      "\n",
      "iter :10000, loss:1.170575, min loss:1.157537\n",
      "\n",
      "\n",
      "iter :12000, loss:1.162744, min loss:1.142928\n",
      "\n",
      "\n",
      "iter :14000, loss:1.172242, min loss:1.142928\n",
      "\n",
      "\n",
      "iter :16000, loss:1.157510, min loss:1.142928\n",
      "\n",
      "\n",
      "iter :18000, loss:1.157217, min loss:1.142928\n",
      "\n",
      "\n",
      "iter :20000, loss:1.167040, min loss:1.142928\n",
      "\n",
      "\n",
      "iter :22000, loss:1.157535, min loss:1.142928\n",
      "\n",
      "\n",
      "iter :24000, loss:1.169349, min loss:1.142928\n",
      "\n",
      "\n",
      "iter :26000, loss:1.175129, min loss:1.142928\n",
      "\n",
      "\n",
      "iter :28000, loss:1.162125, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :30000, loss:1.160888, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :32000, loss:1.154086, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :34000, loss:1.156431, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :36000, loss:1.162015, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :38000, loss:1.155809, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :40000, loss:1.148127, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :42000, loss:1.164385, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :44000, loss:1.166302, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :46000, loss:1.155915, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :48000, loss:1.155932, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :50000, loss:1.162687, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :52000, loss:1.148994, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :54000, loss:1.159473, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :56000, loss:1.155706, min loss:1.138280\n",
      "\n",
      "\n",
      "iter :58000, loss:1.169110, min loss:1.137934\n",
      "\n",
      "\n",
      "iter :60000, loss:1.147735, min loss:1.136606\n",
      "\n",
      "\n",
      "iter :62000, loss:1.169340, min loss:1.132779\n",
      "\n",
      "\n",
      "iter :64000, loss:1.158672, min loss:1.132779\n",
      "\n",
      "\n",
      "iter :66000, loss:1.166054, min loss:1.132779\n",
      "\n",
      "\n",
      "iter :68000, loss:1.161922, min loss:1.132779\n",
      "\n",
      "\n",
      "iter :70000, loss:1.154340, min loss:1.132779\n",
      "\n",
      "\n",
      "iter :72000, loss:1.147336, min loss:1.132779\n",
      "\n",
      "\n",
      "iter :74000, loss:1.160379, min loss:1.132779\n",
      "\n",
      "\n",
      "iter :76000, loss:1.157187, min loss:1.132779\n",
      "\n",
      "\n",
      "iter :78000, loss:1.166526, min loss:1.132779\n",
      "\n",
      "\n",
      "iter :80000, loss:1.134640, min loss:1.131290\n",
      "\n",
      "\n",
      "iter :82000, loss:1.149534, min loss:1.131290\n",
      "\n",
      "\n",
      "iter :84000, loss:1.152690, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :86000, loss:1.167376, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :88000, loss:1.160876, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :90000, loss:1.157601, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :92000, loss:1.161025, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :94000, loss:1.162229, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :96000, loss:1.148250, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :98000, loss:1.149044, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :100000, loss:1.157304, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :102000, loss:1.156656, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :104000, loss:1.159026, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :106000, loss:1.170361, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :108000, loss:1.152961, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :110000, loss:1.150347, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :112000, loss:1.162339, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :114000, loss:1.153774, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :116000, loss:1.146581, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :118000, loss:1.158884, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :120000, loss:1.150847, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :122000, loss:1.152851, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :124000, loss:1.159942, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :126000, loss:1.152456, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :128000, loss:1.164895, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :130000, loss:1.150925, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :132000, loss:1.159614, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :134000, loss:1.147368, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :136000, loss:1.168528, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :138000, loss:1.151832, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :140000, loss:1.159887, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :142000, loss:1.153577, min loss:1.127581\n",
      "\n",
      "\n",
      "iter :144000, loss:1.159350, min loss:1.127581\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m rnn\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m      7\u001b[0m rnn\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_reader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverge_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 74\u001b[0m, in \u001b[0;36mTranslator.train\u001b[1;34m(self, data_reader, threshold, len, converge_limit)\u001b[0m\n\u001b[0;32m     71\u001b[0m     converge_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     73\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m data_reader\u001b[38;5;241m.\u001b[39mnext_batch()\n\u001b[1;32m---> 74\u001b[0m xs_en, hs_en, xs_fr, hs_fr, ps_fr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m dU_en, dW_en, db_en, dU_fr, dW_fr, dV_fr, db_fr, dc_fr \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(xs_en, xs_fr, hs_en, hs_fr, ps_fr, targets)\n\u001b[0;32m     77\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(ps_fr, targets)\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mTranslator.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     28\u001b[0m     xs_en, hs_en \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mforward(inputs)\n\u001b[1;32m---> 29\u001b[0m     xs_fr, hs_fr, ycap_fr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs_en\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xs_en, hs_en, xs_fr, hs_fr, ycap_fr\n",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, inputs, hprev)\u001b[0m\n\u001b[0;32m     33\u001b[0m xs[t] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     34\u001b[0m xs[t][inputs[t]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# one hot encoding , 1-of-k\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m hs[t] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU,xs[t]) \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb) \u001b[38;5;66;03m# hidden state\u001b[39;00m\n\u001b[0;32m     36\u001b[0m os[t] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV,hs[t]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc \u001b[38;5;66;03m# unnormalised log probs for next char\u001b[39;00m\n\u001b[0;32m     37\u001b[0m ycap[t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(os[t]) \u001b[38;5;66;03m# probs for next char\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# rnn.train(data_reader, 5000)\n",
    "\n",
    "# rnn.encoder.learning_rate *= 100\n",
    "\n",
    "rnn.train(data_reader)\n",
    "rnn.encoder.learning_rate = 0.001\n",
    "rnn.decoder.learning_rate = 0.001\n",
    "rnn.train(data_reader, converge_limit=200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05bd226-bc74-4955-8ab7-1689eb6eb985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b82eba-abd3-4836-9818-c1d462ac9745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la est Ã©tait parfois chaud le des , citrons votre california en - octobre - - - - - - - - - -\n"
     ]
    }
   ],
   "source": [
    "text_to_translate = \"the orange is her favorite fruit , but the banana is your favorite .\"\n",
    "if data_reader.validate_input(text_to_translate):\n",
    "    print(rnn.predict(data_reader, text_to_translate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cfac4bb-0016-4386-8388-2997be9fdb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la est les un janvier au citron fruits mais il est parfois frisquet en novembre . - - - - - - - -\n"
     ]
    }
   ],
   "source": [
    "text_to_translate = \"our least liked fruit is the lemon , but my least liked is the grape .\"\n",
    "if data_reader.validate_input(text_to_translate):\n",
    "    print(rnn.predict(data_reader, text_to_translate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fc0b7-fa22-4a59-8cbe-fa7c5f176c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "061279f3-656f-41a2-9b35-ff4a2d00e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = [222, 188, 184, 174, 65, 194, 2, 112, 136, 125, 171, 174, 65, 196, 120, 8, 198, 0, 0, 0, 0, 0, 0, 0]\n",
    "# outputs = [110, 343, 67, 183, 129, 217, 209, 219, 351, 348, 56, 278, 217, 11, 159, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# for i in inputs:\n",
    "#     print(data_reader.ix_to_word_en[i], end=' ')\n",
    "# print()\n",
    "# for i in outputs:\n",
    "#     print(data_reader.ix_to_word_fr[i], end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb10eb45-d32d-4c9e-b6f7-7163f1299ae9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '-', '.', 'a', 'and', 'animal', 'animals', 'apple', 'apples', 'april', 'are', 'august', 'autumn', 'banana', 'bananas', 'bananas.', 'beautiful', 'busy', 'but', 'california', 'car', 'cat', 'chilly', 'china', 'cold', 'december', 'dislike', 'disliked', 'dislikes', 'driving', 'dry', 'during', 'elephants', 'fall', 'favorite', 'favorite.', 'feared', 'february', 'france', 'freezing', 'fruit', 'going', 'grape', 'grapefruit', 'grapes', 'he', 'her', 'his', 'hot', 'i', 'in', 'india', 'is', 'it', 'january', 'jersey', 'july', 'june', 'least', 'lemon', 'lemons', 'like', 'liked', 'likes', 'lime', 'limes', 'little', 'loved', 'mango', 'mangoes', 'mangoes.', 'march', 'may', 'mild', 'most', 'my', 'never', 'new', 'next', 'nice', 'november', 'october', 'old', 'orange', 'oranges', 'our', 'paris', 'peaches', 'pear', 'pears', 'plan', 'pleasant', 'quiet', 'rainy', 'red', 'relaxing', 'rusty', 'saw', 'september', 'shark', 'she', 'snowy', 'sometimes', 'spring', 'states', 'strawberries', 'strawberry', 'summer', 'that', 'the', 'their', 'they', 'to', 'truck', 'united', 'usually', 'visit', 'warm', 'was', 'we', 'were', 'wet', 'winter', 'wonderful', 'yellow', 'your']\n"
     ]
    }
   ],
   "source": [
    "valid_words = [k for k in data_reader.word_to_ix_en.keys()]\n",
    "valid_words.sort()\n",
    "print(valid_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58708eb-7384-4922-baea-fac2b3d777a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
