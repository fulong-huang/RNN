{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f45fe0-6e50-4894-b79e-589023a3dd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b56d226-ac6f-42e5-8abe-cd0d6eed48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length, data=\"\"):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        self.data = \"Today we will be talking about RNN and LSTM. RNN stands for recurrent neural network, \" + \\\n",
    "                    \"the special property of RNN is its ability to use the results from the past to generate a new result. \" + \\\n",
    "                    \"LSTM stands for Long-Short Term Memory, \" + \\\n",
    "                    \"LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, \" + \\\n",
    "                    \"and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradients.\"\n",
    "        if len(data) != 0:\n",
    "            self.data = data\n",
    "        #self.fp = open(path, \"r\")\n",
    "        #self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371906b3-1ba2-4292-902d-2a342ff63a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "            xs, hs, os, ycap = {}, {}, {}, {}\n",
    "            hs[-1] = np.copy(hprev)\n",
    "            for t in range(len(inputs)):\n",
    "                xs[t] = np.zeros((self.vocab_size,1))\n",
    "                xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "                hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "                os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "                ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "            return xs, hs, ycap\n",
    "        \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "            # backward pass: compute gradients going backwards\n",
    "            dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "            db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "            dhnext = np.zeros_like(hs[0])\n",
    "            for t in reversed(range(self.seq_length)):\n",
    "                dy = np.copy(ps[t])\n",
    "                #through softmax\n",
    "                dy[targets[t]] -= 1 # backprop into y\n",
    "                #calculate dV, dc\n",
    "                dV += np.dot(dy, hs[t].T)\n",
    "                dc += dc\n",
    "                #dh includes gradient from two sides, next cell and current output\n",
    "                dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "                # backprop through tanh non-linearity \n",
    "                dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "                db += dhrec\n",
    "                #calculate dU and dW\n",
    "                dU += np.dot(dhrec, xs[t].T)\n",
    "                dW += np.dot(dhrec, hs[t-1].T)\n",
    "                #pass the gradient from next cell to the next iteration.\n",
    "                dhnext = np.dot(self.W.T, dhrec)\n",
    "            # clip to mitigate exploding gradients\n",
    "            for dparam in [dU, dW, dV, db, dc]:\n",
    "                np.clip(dparam, -5, 5, out=dparam) \n",
    "            return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "            \"\"\"loss for a sequence\"\"\"\n",
    "            # calculate cross-entrpy loss\n",
    "            return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "                \n",
    "    def sample(self, h, seed_ix, n):\n",
    "            \"\"\"\n",
    "            sample a sequence of integers from the model\n",
    "            h is memory state, seed_ix is seed letter from the first time step\n",
    "            \"\"\"\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[seed_ix] = 1\n",
    "            ixes = []\n",
    "            for t in range(n):\n",
    "                h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "                y = np.dot(self.V, h) + self.c\n",
    "                p = np.exp(y)/np.sum(np.exp(y))\n",
    "                ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "                x = np.zeros((self.vocab_size,1))\n",
    "                x[ix] = 1\n",
    "                ixes.append(ix)\n",
    "            return ixes\n",
    "\n",
    "    def train(self, data_reader):\n",
    "            iter_num = 0\n",
    "            # threshold = 0.05 ####################### Thresholds #######################\n",
    "            threshold = 0.001 ####################### Thresholds #######################\n",
    "            smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "\n",
    "            ###########################################################\n",
    "            # converge_limit = 5000\n",
    "            # converge_count = 0\n",
    "            # min_loss = 100000\n",
    "            ###########################################################\n",
    "\n",
    "            while (smooth_loss > threshold):\n",
    "                # if smooth_loss < min_loss:\n",
    "                #     min_loss = smooth_loss\n",
    "                #     converge_count = 0\n",
    "                # else:\n",
    "                #     if converge_count >= converge_limit:\n",
    "                #         print(f\"Model seems to converge. Min loss: {min_loss}, Curr loss: {smooth_loss}\")\n",
    "                #         break \n",
    "                #     converge_count += 1\n",
    "                if data_reader.just_started():\n",
    "                    hprev = np.zeros((self.hidden_size,1))\n",
    "                inputs, targets = data_reader.next_batch()\n",
    "                xs, hs, ps = self.forward(inputs, hprev)\n",
    "                dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "                loss = self.loss(ps, targets)\n",
    "                self.update_model(dU, dW, dV, db, dc)\n",
    "                smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "                hprev = hs[self.seq_length-1]\n",
    "                if not iter_num%500:\n",
    "                    sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "                    print( ''.join(data_reader.ix_to_char[ix] for ix in sample_ix))\n",
    "                    print( \"\\n\\niter :%d, loss:%f\"%(iter_num, smooth_loss))\n",
    "                    #print( \"\\n\\niter :%d, loss:%f, best:%f\"%(iter_num, smooth_loss, min_loss))\n",
    "\n",
    "                iter_num += 1\n",
    "\n",
    "    def predict(self, data_reader, start, n):\n",
    "\n",
    "        #initialize input vector\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = np.zeros((self.vocab_size,1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529a4220-2fb7-4106-a5dc-a5ad535058e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".x wfgpcf k  l,wemetcbklpvMLpwLbigstebLL-RpbLgngt abafktalkiehtbewpls Lu-gcbvtilMbawxSpbuwvbp abpgctMgelL aw beailm ebete abxgpbew beSetfwibslbbe SgatTtetcbebelxbe flfouyie.balrgaL bntStSwmrTbe .beRa-\n",
      "\n",
      "\n",
      "iter :0, loss:88.158347\n",
      "alo aldalty om e ay RNN rtr RNN aenere folipnr RStg tesrangenod al,r T T. aln. loryo toratRpTwduLbeheerm t. RM ayyrtng Nibme f batmeng tormmo angro u To at meTlM,rem .etou me Ltal nm wang-ng tyka t  r\n",
      "\n",
      "\n",
      "iter :500, loss:83.460477\n",
      "frsests utondng/so m c rkang ang cind a foni,c RNN be ths ladguterm iete mgf alu arddrox fky-pavini foro, nvd ittr ronulty put ex and welits fer at/ltd havenou LoTMfise the lang lem aldlking thep as i\n",
      "\n",
      "\n",
      "iter :1000, loss:68.179071\n",
      "om celm ne,o holulg te. hex andl and LSTM. pandliso peshang the ao wil long/ts tils frewuvasuserdlgp algoro. RNN bo here LoTve mrm ,upgeua morilcSTM standal the long/le LSTM.itheo. mxperm mpte LSTM is\n",
      "\n",
      "\n",
      "iter :1500, loss:49.671249\n",
      "im alothanend candather be govisnose the lom tertands ftormorl-talking abe ta tss se ho/ulinil ttm calkinut ng renteno theo and can also resplgetew atc T-te willorong/leculvo ifromom rentha sort- erm \n",
      "\n",
      "\n",
      "iter :2000, loss:34.069039\n",
      "exule tyrgsy tes bo the ine alod fes at go g-Sre/herile palvene aN lo, Rb g af RN a ne lenulal va il RNN alnd a fing teng long-taleoritkm wol RNN gong-te RNN adm u- pal as ite aand the hes falk, ve-s \n",
      "\n",
      "\n",
      "iter :2500, loss:31.739692\n",
      "roriving talkindiab se cilvang aN ialuiorw talking ba haskal ng/vaning les foriano Me alo hang bror reuras a fy wilu ffrind theiiand Troperm hang thes ffophe aboute pandling alo iecia uere Lx. LSTM is\n",
      "\n",
      "\n",
      "iter :3000, loss:31.211802\n",
      "alking lom memoril erer andle the lomurexulgpto halom recurereral se Tex alvint ling-tera as ferm meoriene anoures and ba talgitalking long-Meithes lisulo gererentaninTali RNNg thm g-term meorcbyw ave\n",
      "\n",
      "\n",
      "iter :3500, loss:27.178365\n",
      "andialitherilishand long-Shoridndlse the lorw, LSul RNN utm Merete a ne a so re ang ralkint nemuratue . RNN abm herm momory, andl in ise wo gesolvano corm this along-te ral pso hort Tories andl oriadt\n",
      "\n",
      "\n",
      "iter :4000, loss:22.450453\n",
      "resurse mettes ralinesupalds for rex andle the long-Short Term serm Meoralvandle the long/Shorw tf fforisy tandliite morewo, alsp RNN bilongkis iongw cem alsking abodie a RNN and LSTM. RNN by havu tes\n",
      "\n",
      "\n",
      "iter :4500, loss:17.931926\n",
      "res reuuraling ting ghew recis ind hallking long-Shong neural cong-te res a RNN bnd RNN by having tas sraling long-Shortlge exm mrthe lenil ty g-ne lumorits iong valg nglt Tere ralulg hands rtw b ngw \n",
      "\n",
      "\n",
      "iter :5000, loss:14.890144\n",
      "rom the aha has aloprest ven lestrecing alod RNN and LSTM. RNN stands for remurexting ral ng the lofhand so genm tepm tpeuralts a seoritcise tandlong-term mecralid ferm rex aliling/vaning tesm memm me\n",
      "\n",
      "\n",
      "iter :5500, loss:11.700717\n",
      "o handle the results from temorie LotM venolt-term mem rex also result. LStM spands for Long-Short Term Memories. LSTM is bettheolilSdim to halking abew and a more cong ge res aloptesploding/vanithano\n",
      "\n",
      "\n",
      "iter :6000, loss:9.863619\n",
      "rom tem it  LSTM stands furmoraluthe aNN it lid LSTM is a fortwertynt talts alt roThererere galgm tesp RNN by of RNN item rex by halking abothe RNN is aty candle the long-term meoraeoritts ats ralinit\n",
      "\n",
      "\n",
      "iter :6500, loss:8.007048\n",
      "rom the sof RNN is its abidl nethes and RNN bt. a sang arm rem Me pemploding/van shoral RNN by having antt alstingrty tandlong-term memories. LSTM is better value rest pestan alscine long-term memory,\n",
      "\n",
      "\n",
      "iter :7000, loss:6.345184\n",
      "o pfe rex leng com ortke the long/short-term memoriemerereciandlong-term memories. LSTSu sonvese tr co ganetermorerults alsory, LSTM is and aneuraluereresendis fof RNN is its al netwo trmorex and a mo\n",
      "\n",
      "\n",
      "iter :7500, loss:5.565823\n",
      "rom tem is ferm memories. LSTM is a forilem remorilg vonulal traditional RNN by having anotherago tesplongtte res betwm the tre pex algorithereradithandlo tes its alening gemortturorie ererie pbom ga \n",
      "\n",
      "\n",
      "iter :8000, loss:4.967451\n",
      "rom traditional RNN by having another vasulgeteriaty ion treriiform handle the long/short-term memor RNN be tal ng/vanishingl ng trope core torm tepmo rectrmoralsting/ganithm Mre long-term memory, and\n",
      "\n",
      "\n",
      "iter :8500, loss:3.839105\n",
      "rom texpal wing aes anew result. LSTM stands ftoritstw wolgl prep als ies and netwererm memories. LSTM is and a more, by handl LSTM is better value represent morerertyrt to Merm ormore of RNN but diff\n",
      "\n",
      "\n",
      "iter :9000, loss:3.663886\n",
      "rom the ppecloRN a se resolve expatitheMe a neuratt To Mes aviandling arst te a neuralue rem momoralke cosult. LSTM is aes andty t- pane reperty of RNN be hexploding/vanishing gradingo algprexplother.\n",
      "\n",
      "\n",
      "iter :9500, loss:2.878458\n",
      "ropex and cand network, thm sf RNN by iandle the results from the past to generate a new result. LSTM stands for Lond-Short Term Memory, LSTM is a surmmerty talking about RNN and LSTM. RNN stands for \n",
      "\n",
      "\n",
      "iter :10000, loss:2.296687\n",
      "ropercat aandle the long/short-term memories. LSTM is better valunertym Mem rertling georithere salette er cang to handle the lisuerend a m results from the long-te mom memoryereresenting the long-ter\n",
      "\n",
      "\n",
      "iter :10500, loss:1.890778\n",
      "rom the past to generate and banis for Long-Short Term Memory, and sastandle the long-term memory, and a more com long-Short Term meretes its abilithanishing gradl buthereriditianal neswork, the ghal \n",
      "\n",
      "\n",
      "iter :11000, loss:3.308288\n",
      "roperty of RNN is its ability to use the rsecial pbouheres and abm Merese tx ise the long/short-term memories. LSTMt RNN RNN bs ure recer cong-thork, the long-term meories and can also resolvese the l\n",
      "\n",
      "\n",
      "iter :11500, loss:2.493903\n",
      "rom traditional RNN by having another value representing can alst LSTMpional RNN by having andtt Tork, the songltn Me pexpandling long-term memoriest RNN but l fSra isdlty repemortes and gumoralt. LST\n",
      "\n",
      "\n",
      "iter :12000, loss:1.921591\n",
      "eorilvability to use the long/short-term memories. LSTM is better at handling gradiandle wil thad RNN by having another value rex also rs aat LSTM stands forem m oraditing winguraghe expll/standseitha\n",
      "\n",
      "\n",
      "iter :12500, loss:1.529014\n",
      "alolem aeorilvanetentita te pesult. LSTM is abynt to hong/shortheuralt. RNN but dif RNN alcite prociferm memory, LSTM ast com pandling long-term memoriene aeorie . -wereresolve expat prom the past cor\n",
      "\n",
      "\n",
      "iter :13000, loss:1.700313\n",
      "roperty pand a more complof abodtalt neoralitidiong-term memory, and a more Tomplecureratilill b-terty of RNN is oroperty of RNN is its ability to use the results from the past to generate a new resul\n",
      "\n",
      "\n",
      "iter :13500, loss:1.585964\n",
      "roperty of RNN is its abithe RNN bo having anothe long-Short Term Memory, LSTM iffort-term memories. LSTM is better a so te mory, LSTM is a form of RNN but die exproding/vanishing gradil RNN but lve r\n",
      "\n",
      "\n",
      "iter :14000, loss:1.273034\n",
      "roperthe resong-Merty, and a morithere ae pradiad complex algorithm to handle the long-term meoradit net a w com alodiagural RNN but  stands for for recurrent neural networtthe special property of RNN\n",
      "\n",
      "\n",
      "iter :14500, loss:1.052676\n",
      "roperty of RNN is its abio alkint anitherie mot res and can also resolve exploding/vanishing gradia form memories. LSTM is be handling long-term meories and can also resolve exploding/vanishing gradit\n",
      "\n",
      "\n",
      "iter :15000, loss:0.897498\n",
      "rocis for pex algo algor thm handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explong-term meories and can also resolve exploding/vanishing gradit\n",
      "\n",
      "\n",
      "iter :15500, loss:0.786157\n",
      "roperthe long/short-term memories. LSTM is bett RNN ant re talking abothererinit a o ven als ial peoradifor pe pandle the abeutand crmo te a RNN and LSTM. RNN stands for recurrent neural network, the \n",
      "\n",
      "\n",
      "iter :16000, loss:1.422526\n",
      "roperty of RNN is oralt net a creurertes and com tre pandleutal wing result. LSTM stands for resulve mor the special property of RNN is its ability to use the results from tradit RNN bu Mrst ne long-t\n",
      "\n",
      "\n",
      "iter :16500, loss:1.117243\n",
      "roperty mer. LSTM is better att a ferthe long-term memories. LSTM is be handling long-term meories ang result. LSTM stands for Long-Short Term Mempry Term Memory, LSTM is LSTM is a formperty ts o tor \n",
      "\n",
      "\n",
      "iter :17000, loss:0.906060\n",
      "roperty talking about couranity tesolve exploding/vanishing graditionis fof RNN is its ability herm meories and can also resolve explod LSTM. RNN stands fof rst ererishing traditioning te pand rec ald\n",
      "\n",
      "\n",
      "iter :17500, loss:0.761543\n",
      "roperty of RNN is its ability to use the results from traditione recurrertes andlong-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-t\n",
      "\n",
      "\n",
      "iter :18000, loss:0.661627\n",
      "roperty of RNN is its abilualue representing the lo honhing aboding/ve of Rvis also we a forilt. LSTM stands from the past to generate a n l ng/vanishing gradiling/sherthe long-term memories. LSTM is \n",
      "\n",
      "\n",
      "iter :18500, loss:0.592130\n",
      "roperty of RNN is its ability to use the results from the pandlity to use the abe talking about RNN and LSTM. RNN stands for recurrent neuthm the long-term memory, and a more complex algorithm to hand\n",
      "\n",
      "\n",
      "iter :19000, loss:0.542432\n",
      "rop formoreetesuls. LSTM is better at handling long-term meories and can also resolve exploding/gp more complexual netwerertec result. LSTM stands for Long-Short Term Memory, LSTM is ags aasulve explo\n",
      "\n",
      "\n",
      "iter :19500, loss:0.506893\n",
      "roperthe long-ther value representing thadid RNN but es. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but lg. be palking about RNN and LSTM. RNN stands for recurrent neural network, t\n",
      "\n",
      "\n",
      "iter :20000, loss:0.482106\n",
      "roperty of RNN is its ability to use the results from the past to generato more ex al perm memories. LSTM is better at handling long-term meories and can also resolve exploding/vaning ceoralge peoradi\n",
      "\n",
      "\n",
      "iter :20500, loss:0.460996\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditiong to ulerty of RNN is i\n",
      "\n",
      "\n",
      "iter :21000, loss:0.438528\n",
      "expMe le pandl RNN al pemploding/van se tal neowolve es als irmod RNN ale avm Me werthe lom Merm Memory, and a moratyene shorM iong/vanishing gradiane gradil form or RNN al pesult wit differs from tra\n",
      "\n",
      "\n",
      "iter :21500, loss:2.529969\n",
      "roperty of RNN is its ing graditional RNN by having another value representing the last to handl RNN ind to handle the long/short-term memories. LSTM is LSTM is a form memories. LSTM is better at hand\n",
      "\n",
      "\n",
      "iter :22000, loss:2.542516\n",
      "roperty of RNN is its ability to use the long/short-term memories. LSTM is better at hand al braloniforerteories and can also resolve exploding/vanishing graditional RNN lud LSTM. RNN stands for recur\n",
      "\n",
      "\n",
      "iter :22500, loss:1.735217\n",
      "roperty of RNN is its ability to use the results from the past Lo g-te res and a thererem aempresult. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from the past to generat\n",
      "\n",
      "\n",
      "iter :23000, loss:1.218863\n",
      "roperty of RNN is its abit a RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is bewtre aes and comprex algo rtp\n",
      "\n",
      "\n",
      "iter :23500, loss:0.891431\n",
      "rocithe som Memorie Long-Short Term Memory, LSTM aseuralkandle the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from troper\n",
      "\n",
      "\n",
      "iter :24000, loss:0.683047\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form or RNN rs Mtm merm memories. LSTM is better at handling \n",
      "\n",
      "\n",
      "iter :24500, loss:0.549247\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is alst LSTM. RNN stands for recurrent neory, LSTM is a form of RN\n",
      "\n",
      "\n",
      "iter :25000, loss:0.462040\n",
      "roperty of RNN is its ability to use the results fretter at handling long-te mom rexploding/vaning thort-term meories and can ast recurrent neural network, the special property of RNN is its al RNN by\n",
      "\n",
      "\n",
      "iter :25500, loss:0.403907\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :26000, loss:0.364039\n",
      "roperty of RNN is its ability to use the results frong-short-te memory, LSTM is a form of RNN but differs from traditional RNN by having another woding/vanishing about RNN and LSTM. RNN stands for rec\n",
      "\n",
      "\n",
      "iter :26500, loss:0.335781\n",
      "roperty of RNN is its abinithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also res lut dve so wo tes result. LSTM is a form of RNN but differs from tr\n",
      "\n",
      "\n",
      "iter :27000, loss:0.314955\n",
      "roperty of RNN ise the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM algo re talking about RNN and LSTM. RNN stands for recurrent neuralititw RNN bNt and\n",
      "\n",
      "\n",
      "iter :27500, loss:0.298979\n",
      "roperty of RNN is its ability to use the results feorithe RNN butheweriat valking long-term oreciandlanit. ng long-term meories. LSTM is better at handling long-term meories and can also resolve explo\n",
      "\n",
      "\n",
      "iter :28000, loss:0.286190\n",
      "ropertpe long-term meories and can also resolve exploding/vanishing gradity talistom praditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle t\n",
      "\n",
      "\n",
      "iter :28500, loss:0.275586\n",
      "rop ferm memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing wong-term meories ang trmor the special procif RNN is ies and aongand a more to os its abiling \n",
      "\n",
      "\n",
      "iter :29000, loss:0.266515\n",
      "roperty of RNN is its ability tf us RNN but differkity tong-Short Term memoriese the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form ofrod LSTM s\n",
      "\n",
      "\n",
      "iter :29500, loss:0.258589\n",
      "rop formore comprexus for LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differ. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditio als M\n",
      "\n",
      "\n",
      "iter :30000, loss:0.251677\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands LSTM is a form of RNN but differs from tradit RNN but difforta RNN but lvere aang aom rex algorithm\n",
      "\n",
      "\n",
      "iter :30500, loss:0.247140\n",
      "roperty of RNN is orstorte handling long-term meories and cemprts irsoLSTMespralinit ang about RNN alg about com te candlewtypre cong te a neuradiaty to handle trmor thad a the long-term memory, and a\n",
      "\n",
      "\n",
      "iter :31000, loss:0.241215\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Shind nang a new beorithm the long-term memory, and a morerty of RNN is its abililecial ne\n",
      "\n",
      "\n",
      "iter :31500, loss:0.236052\n",
      "roperty of RNN but diff st ro trs for Long-Short Term Memory, LSTM is a ffrstalinit peorilve exploding/vanishing gradinior betwork, the special property of RNN but differs frem results froperty of RNN\n",
      "\n",
      "\n",
      "iter :32000, loss:2.116229\n",
      "roperty of RNN is its ability to use the abe handling long-term meories and can also resolve exploding/vanishing gradie fpe long-term memory, and a more complex algorithm to handle to pfertesplt Talki\n",
      "\n",
      "\n",
      "iter :32500, loss:1.674037\n",
      "roperty of RNN is its ability to use the lysulcoreciandle the results from theriand cand neutalil RNN by having er hal network, the special property of RNN is its ability to use the results from the p\n",
      "\n",
      "\n",
      "iter :33000, loss:1.217622\n",
      "roperty of RNN stands for recurrent no Mer at hal network, the salol she a RNN ats ald cong tes loding/vanity tonulinit and RNN and abult Term Memory, LSTM is abit ng/vanithere abiting ne lon sher a s\n",
      "\n",
      "\n",
      "iter :33500, loss:0.932306\n",
      "eorilg/vy thorMemory, LSTM is a form of RNN buthavalinis a sor talking about RNN and LSTM. RNN stands for recurrent neural networt uemory, LSTM is a form of RNN but differs from traditional RNN by hav\n",
      "\n",
      "\n",
      "iter :34000, loss:0.970677\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM as a fort complex algorithongproreries. LSTM is better at handling\n",
      "\n",
      "\n",
      "iter :34500, loss:0.738227\n",
      "om recureriaty morerertyeratt m cerm Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long/short-term memories. LSTM is better at handling long-t\n",
      "\n",
      "\n",
      "iter :35000, loss:0.585618\n",
      "ropgecerty talking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new resulte exploding/vanishin\n",
      "\n",
      "\n",
      "iter :35500, loss:0.486942\n",
      "roperty of RNN is its abilgs frecworilldie about RNN and LSTM. RNN stands for recurrent neuraliting cand talianite to halkint a RNN but differs from traditional RNN by having another valutort. its and\n",
      "\n",
      "\n",
      "iter :36000, loss:0.717852\n",
      "roperty of RNN is its ability to use the results from the past to generate aane recuriese tand can also resolve exploding/vanishing gradiait a RN gune term term Mempresult. LSTM stands for Long-Short \n",
      "\n",
      "\n",
      "iter :36500, loss:0.579847\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :37000, loss:0.485570\n",
      "ropertye about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the recurerty or RNN be tr hort te momoresolve momom representing the long-term \n",
      "\n",
      "\n",
      "iter :37500, loss:0.969657\n",
      "ong-term memory, and a morerertymore oorw toralit RNN but lS. LSTM is better at handling long-term meories anal RNN by having another go henulong-term ceorilve mories and com resplts from the past to \n",
      "\n",
      "\n",
      "iter :38000, loss:0.720388\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anot a\n",
      "\n",
      "\n",
      "iter :38500, loss:0.543501\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :39000, loss:0.525449\n",
      "roperty of RNN is its abilil network, the special property of RNN is its ability to use the results from the pastito ts heng neurandl bettena serm memoriestand a or LSTM is a form of RNN but differs f\n",
      "\n",
      "\n",
      "iter :39500, loss:0.408535\n",
      "roperty of RNN is its ability to use the results from the past to generate pan ltn a cis alf recurrent to handlolte abothes. LSTM is better at handling lont-term memories. LSTM is better RNN buthanew \n",
      "\n",
      "\n",
      "iter :40000, loss:0.329932\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term memories. LSTM is better at handling long-term meories and can also resolve exp\n",
      "\n",
      "\n",
      "iter :40500, loss:0.278156\n",
      "reuratylte eraditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories \n",
      "\n",
      "\n",
      "iter :41000, loss:0.243823\n",
      "roperte a ng nand neuterm memoriest RNN but to uex and can also resolve exploding/vanishing graditiod a thad RNN by ianishing gradiandls its alsting the long-te mom rexpM S-te dast to genew rom term t\n",
      "\n",
      "\n",
      "iter :41500, loss:0.220609\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :42000, loss:0.204479\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM su canal netwer value representing the long-term memory, and a of resulve exploding ge palking about RNN a\n",
      "\n",
      "\n",
      "iter :42500, loss:0.192877\n",
      "roperty of RNN is its ability to use the rerm Meories and aandle the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is bet a form of RNN but differs from t\n",
      "\n",
      "\n",
      "iter :43000, loss:0.184198\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :43500, loss:0.177423\n",
      "roperty of RNN is its ability to use texpaty hoTpd the result. LSTM is a form of RNN but differs from traditional RNN by having another valuererere ex ands frodie traditional sure ceorils. LSus andle \n",
      "\n",
      "\n",
      "iter :44000, loss:0.171912\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :44500, loss:0.167253\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :45000, loss:0.163183\n",
      "rom the ponulSRNN ss ngt cand a ne wol fhm Mesolve exploding/vanishing graditiore to handlo tes ats orty of RNN is its ability to use the results from the past to generate a new result. LSTM stands fo\n",
      "\n",
      "\n",
      "iter :45500, loss:0.159530\n",
      "recial tal ale long-term meories and a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term mem\n",
      "\n",
      "\n",
      "iter :46000, loss:0.156185\n",
      "om reoriaty te a nelong-ther value rem resolve exploding/vanishing gradinif ont term Meories. LSTM is better at handling long-term meories and cand network, the special property of RNN is its ability \n",
      "\n",
      "\n",
      "iter :46500, loss:0.153072\n",
      "roperresolve exploding/vanishing gradity to use the results from the past to generate a new result. LSTM is a form of RNN but differs from traditional RNN by having another value representingwty hecua\n",
      "\n",
      "\n",
      "iter :47000, loss:0.150145\n",
      "roperty of RNN is its ability to use the results from the past to generato t-Se com the rast to generate a new recure com re cal networt, avindse com pesple wong-term meories and can also resolve reor\n",
      "\n",
      "\n",
      "iter :47500, loss:0.147369\n",
      "roperty of RNN is its ability to use theravesult. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from tr RNN and LSTM. RNN stands for recurrent neural network, the special p\n",
      "\n",
      "\n",
      "iter :48000, loss:0.144725\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :48500, loss:0.142195\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :49000, loss:0.139769\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :49500, loss:0.137438\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :50000, loss:0.135195\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :50500, loss:0.133036\n",
      "ropertpe long-term meories and a com the past to generate a new result. LSTM stands for Long-Short Term Memore, ave a ngcine long-term meories and crmorithm to handle the results from the past to gene\n",
      "\n",
      "\n",
      "iter :51000, loss:0.130954\n",
      "roperty of RNN is its abilut LST RNN and LSTM is from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :51500, loss:0.128945\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :52000, loss:0.127004\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :52500, loss:0.125129\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :53000, loss:0.123314\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :53500, loss:0.121559\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :54000, loss:0.119856\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :54500, loss:0.118207\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :55000, loss:0.116604\n",
      "roperty of RNN is its abnling the long-term meories and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and cropromprexplong-term meories \n",
      "\n",
      "\n",
      "iter :55500, loss:0.115051\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :56000, loss:0.113539\n",
      "roperty oavist neure cane to havS-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishin\n",
      "\n",
      "\n",
      "iter :56500, loss:0.112072\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :57000, loss:0.110642\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-te ralking about RNN and LSTM. RNN stands for recurrent neural network, the special proper\n",
      "\n",
      "\n",
      "iter :57500, loss:0.109254\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :58000, loss:0.107899\n",
      "roperty of RNN inetty of RNN is its ability to use the results from the past to generate a ne aane to hexplod LSTM. RNN stands for rewulve rom hand al bratyecriol and can and Torg complong-Shorts fori\n",
      "\n",
      "\n",
      "iter :58500, loss:0.106583\n",
      "roperty of RNN is its abilganity te a ne aane recurrent neural perm memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradie RNN is its alsting/vanishing \n",
      "\n",
      "\n",
      "iter :59000, loss:0.105297\n",
      "roperty of RNN is its ability to use the rexploding congl net. ropertpe abiliting a or praditior LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradiere sof and \n",
      "\n",
      "\n",
      "iter :59500, loss:0.104047\n",
      "ropertpe abilithers a forM hexpalong-term momories at hang ant ling the longltnishing gradity ho use the results from the past to genere er loning trmor thad RNN a netw no henuling. LSTM is better at \n",
      "\n",
      "\n",
      "iter :60000, loss:0.102825\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :60500, loss:0.101637\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but  stalds for Long-Short Term Memory, LSTM is a\n",
      "\n",
      "\n",
      "iter :61000, loss:0.100474\n",
      "roperty of RNN is its ability to use the results from the past to generate a ne a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having a\n",
      "\n",
      "\n",
      "iter :61500, loss:0.099343\n",
      "roperty of RNN is its ability to use the results from the past to generate a new resulte exploding/vanishing gradianodand term meor cop aonits algorithm to handle the long/short-term memories. LSTM is\n",
      "\n",
      "\n",
      "iter :62000, loss:0.098235\n",
      "roperte a nes and can llndle the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also rs alstind cong the re\n",
      "\n",
      "\n",
      "iter :62500, loss:0.097157\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :63000, loss:0.096099\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :63500, loss:0.095071\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands forerte resuls. LSTM is bet about RNN and LSTM. RNN stands for rectalingoreccresolve exploding/vani\n",
      "\n",
      "\n",
      "iter :64000, loss:0.094061\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :64500, loss:0.093079\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long/short-term meories and can also resolve exploding/vanishing gradianodand can also resolve \n",
      "\n",
      "\n",
      "iter :65000, loss:0.092113\n",
      "roperty of RNN is its ability to use the results fro, LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term \n",
      "\n",
      "\n",
      "iter :65500, loss:0.091173\n",
      "ropgete a fsolte exploding/vanishing graditio alg crmore complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :66000, loss:0.090248\n",
      "ve long-term meories and can also resolve exploding/vanishing graditiog the special properthe long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM ffio trmprex a\n",
      "\n",
      "\n",
      "iter :66500, loss:0.089346\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :67000, loss:0.088447\n",
      "roperty of RNN is its ability to use the results frottene a ng leutaliting anot andlolt-term meories and can also resolve exploding/vanishing graditifhe pand neural network, the special propercherthe \n",
      "\n",
      "\n",
      "iter :67500, loss:0.087686\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anoura\n",
      "\n",
      "\n",
      "iter :68000, loss:0.086913\n",
      "ropithe speciandl LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradinioN by hexpll bb w resulge eradiand souritong-term meories and a ne gesplve Lo is ity hand\n",
      "\n",
      "\n",
      "iter :68500, loss:0.086136\n",
      "roperty of RNN is its ability to use the result. LSTM is a form of RNN but diRNN but differs from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but d\n",
      "\n",
      "\n",
      "iter :69000, loss:0.085365\n",
      "ruluerer to homplifor RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-\n",
      "\n",
      "\n",
      "iter :69500, loss:0.084592\n",
      "roperty of RNN is its ability to use the results from the Mandle the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories\n",
      "\n",
      "\n",
      "iter :70000, loss:0.083830\n",
      "ropertyerate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but dif RNN but ere the long-term memory, and a more complex algorithm to handle the long/short-term memories. \n",
      "\n",
      "\n",
      "iter :70500, loss:0.083068\n",
      "rop ferm memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditifhe pandling long-term meories and can also resolve exploding/vanishing gradiano herm er\n",
      "\n",
      "\n",
      "iter :71000, loss:0.082318\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :71500, loss:0.081570\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSRN a net a a Tou frs fropifof RNN is its ability to use the results from the past to generate a new result. L\n",
      "\n",
      "\n",
      "iter :72000, loss:0.080838\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :72500, loss:0.080108\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :73000, loss:0.079396\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :73500, loss:0.078687\n",
      "roperte a nes and can llndle the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vani\n",
      "\n",
      "\n",
      "iter :74000, loss:0.077996\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs froritandads bet andl LSTM is a form \n",
      "\n",
      "\n",
      "iter :74500, loss:0.077311\n",
      "ropitSp RNN stands for recurrent neural network, the specialk bocradinio algorithm to handlolte exploding/vanishing gradianodherkis fris orm memomorit hang aboutant tan also resolve exploding/vanishin\n",
      "\n",
      "\n",
      "iter :75000, loss:0.076642\n",
      "rom the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a m\n",
      "\n",
      "\n",
      "iter :75500, loss:0.075980\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is forierer tos fof rst tom represolve exploding/vanishing graditi\n",
      "\n",
      "\n",
      "iter :76000, loss:0.075334\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :76500, loss:0.074694\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :77000, loss:0.074071\n",
      "roperte a NN fferthe so uemprerty hadinit a form of RNN is ity to handl nong-ne long-term meories and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-t\n",
      "\n",
      "\n",
      "iter :77500, loss:0.073453\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :78000, loss:0.072851\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form toru a nes and can also resolve exploding/vanishing grad\n",
      "\n",
      "\n",
      "iter :78500, loss:0.072254\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Methe long-term memory, and a more complex algorithm to handle the long/short-t\n",
      "\n",
      "\n",
      "iter :79000, loss:0.071673\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :79500, loss:0.071096\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :80000, loss:0.070535\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of prad a fort com aboding/vanishing gradinif a talgor t\n",
      "\n",
      "\n",
      "iter :80500, loss:0.069977\n",
      "roperte a ge calking gradilif reciandling le wert cong neuther. LSTM is bettal network, the sanelt iandling long-term meories and can also resolve exploding/vanishing graditio al /alkilt neshandls nes\n",
      "\n",
      "\n",
      "iter :81000, loss:0.069433\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :81500, loss:0.068893\n",
      "roperty of RNN is its ability to use the repe ae phe tand thands for recurrent neural network, the special properte a nette pane reprend alug and abut lSTits indty tes alsting/vanishing graditio al pe\n",
      "\n",
      "\n",
      "iter :82000, loss:0.068367\n",
      "ropresertecrity handl LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditiod RNN binit palkal network, the salking about RNN and LSTM. RNN stands LSTM is a for\n",
      "\n",
      "\n",
      "iter :82500, loss:0.067844\n",
      "roperty of RNN is its ability to use the results from the pasolveter als ats algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodi\n",
      "\n",
      "\n",
      "iter :83000, loss:0.067333\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :83500, loss:0.066825\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :84000, loss:0.066329\n",
      "roperty of RNN is its ability to use the results from the past to generate a ne halulangene a opeprodil f RNN is its ability tx alg trmorererty to use the results from the past to generate a new resul\n",
      "\n",
      "\n",
      "iter :84500, loss:0.065834\n",
      "ropithe shert com talking about RNN and LSTM. RNN stands for recurrent neural network, the specierm memories. LSTM is better at handling long-term meories and a more complex algorithm to use the resul\n",
      "\n",
      "\n",
      "iter :85000, loss:0.065350\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by hing gra sher\n",
      "\n",
      "\n",
      "iter :85500, loss:0.064867\n",
      "roperty of RNN is its ability to usolve exploding/vanishing wong-ne a forg, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :86000, loss:0.064394\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :86500, loss:0.063922\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :87000, loss:0.063459\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :87500, loss:0.062996\n",
      "ropitte a noding/vanishing gradidifSradilstito gene a mor to herm teories. LSTM is better at handl LSTM is bethesw mem repre aenure result. LSTM stands for Long-Short Term Memory, LSTM is a form of RN\n",
      "\n",
      "\n",
      "iter :88000, loss:0.062541\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :88500, loss:0.062087\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :89000, loss:0.061640\n",
      "roperty of RNN is its ability to use the results from the past to generate a the lex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve\n",
      "\n",
      "\n",
      "iter :89500, loss:0.061197\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :90000, loss:0.060763\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, Longs RNN but differs from traditional RNN by having another value repr\n",
      "\n",
      "\n",
      "iter :90500, loss:0.060334\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs fop os also resolve exploding/vanishi\n",
      "\n",
      "\n",
      "iter :91000, loss:0.059907\n",
      "roperty of RNN is its ability to use the results from the past to gene ave about RNN and LSTM. RNN stands for recurrent nepresult. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but dif\n",
      "\n",
      "\n",
      "iter :91500, loss:0.059483\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Merm merm memories. LSTM is better at handling long-term meories and can also r\n",
      "\n",
      "\n",
      "iter :92000, loss:0.059060\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :92500, loss:0.058644\n",
      "roperte a ng net andl ngl nethe sp copher. LSTM is aes and almpresult. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representi\n",
      "\n",
      "\n",
      "iter :93000, loss:0.058235\n",
      "roperty of RNN is its ability to use the results from the past to gs ind LSTM is bet a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by hav\n",
      "\n",
      "\n",
      "iter :93500, loss:0.057838\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :94000, loss:0.057443\n",
      "roperty of RNN bu i so tor nette talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but dilk, the so tal networ wilg is ity the lom texploding/vanishing gradier n\n",
      "\n",
      "\n",
      "iter :94500, loss:3.026680\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN butalt. LSTM stands for Long-Short Term Memory, L\n",
      "\n",
      "\n",
      "iter :95000, loss:2.405224\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Shands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN \n",
      "\n",
      "\n",
      "iter :95500, loss:1.576654\n",
      "roperty of RNN is its ability to use the results from the past to generate a new resu te pand a mererete a ag ate aang com rast to generate a new rexploding/vanishing gradianales RNN stands for recurr\n",
      "\n",
      "\n",
      "iter :96000, loss:1.313158\n",
      "ropeese to handle the long-term memories. LSTM is better at handling long-term meories and camorecurrent ne a new result. LSTM stands forerte a ngw resu te resulu. RNN stands for recurrent neural netw\n",
      "\n",
      "\n",
      "iter :96500, loss:0.986690\n",
      "roperresolts from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term m\n",
      "\n",
      "\n",
      "iter :97000, loss:0.865427\n",
      "roperty of RNN is its ability to use the resulof RNN is its ability to use the results from the pastito geneia a nett no M sulol the special property of RNN is its ability to use the results from the \n",
      "\n",
      "\n",
      "iter :97500, loss:0.583466\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :98000, loss:0.405298\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :98500, loss:0.293429\n",
      "ropgete avalinitSplolk, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs \n",
      "\n",
      "\n",
      "iter :99000, loss:0.222446\n",
      "ropithe speciandle the results from the past to generate a new result. LSTM stands forerte pand a merere a ne ral network, the specialkint long-term meories and can also resolve exploding/vanishing gr\n",
      "\n",
      "\n",
      "iter :99500, loss:0.177025\n",
      "roperty of RNN is its ability to use the results from the pasm re handling long-term meories and can also resolve exploding/vanishing gradiano tong van als iem mempmories. LSTM is better at handling l\n",
      "\n",
      "\n",
      "iter :100000, loss:0.147463\n",
      "ropgecertesolve exploding/vanishing gradiano renerrepe wo usaent nouw nothere mom repre anouhertyere expal She long-term memory, and a more complex algorithm to handle the long/short-term memories. LS\n",
      "\n",
      "\n",
      "iter :100500, loss:0.127933\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :101000, loss:0.114752\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :101500, loss:0.105614\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :102000, loss:0.099058\n",
      "rope comuresult. LSTM stands for Long-Short Term Memory, LSTM is a form ofSRN abit n ngtty te rong-term meories and can also resolve explo hand can also resolve exploding/vanishing graditiog the long-\n",
      "\n",
      "\n",
      "iter :102500, loss:0.094170\n",
      "roperte a ng wing nem alndles a f RNN by having another value representing the long-term memory, and a more complex algorithm to handlolt Teser yecraving abowil pve erm memories. LSTM is better at han\n",
      "\n",
      "\n",
      "iter :103000, loss:0.090379\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :103500, loss:0.087326\n",
      "roperty of RNN is its ability to use the results from the wong-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can al\n",
      "\n",
      "\n",
      "iter :104000, loss:0.084778\n",
      "roperte alvind foru te morererty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditi\n",
      "\n",
      "\n",
      "iter :104500, loss:0.082588\n",
      "ropgese the abom the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term me\n",
      "\n",
      "\n",
      "iter :105000, loss:0.080656\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :105500, loss:0.078922\n",
      "roperte a neshing graditandlints and renural networt. LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to \n",
      "\n",
      "\n",
      "iter :106000, loss:0.077339\n",
      "ropithe speciandl frecia T-Su radidifferiit a spforrese the results from the past tol -Shorts from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but \n",
      "\n",
      "\n",
      "iter :106500, loss:0.075882\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :107000, loss:0.074528\n",
      "roperte a ne handl ngtterm meor yority ge more complex algorithm to handle the lenulinvad a ng the long-term meories and can also resolve exploding/vanishing graditiodaliting ano to handle the long/sh\n",
      "\n",
      "\n",
      "iter :107500, loss:0.073264\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by talking about\n",
      "\n",
      "\n",
      "iter :108000, loss:0.072077\n",
      "roperte a ne gecral network, the special prouravishing gradity ha ibectrmore complom ther at long-term meories and can also resolve exploding/vanishing gradiano tong the special property of RNN is its\n",
      "\n",
      "\n",
      "iter :108500, loss:0.070960\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :109000, loss:0.069903\n",
      "roperte a ne gecral network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but dif\n",
      "\n",
      "\n",
      "iter :109500, loss:0.068902\n",
      "roperte alva ferm memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditiodal network, the special long-ne aane recurrent neural network, the special pr\n",
      "\n",
      "\n",
      "iter :110000, loss:0.067949\n",
      "ropertp of RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM i\n",
      "\n",
      "\n",
      "iter :110500, loss:0.067041\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :111000, loss:0.066172\n",
      "ropgese the long/short-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradity t\n",
      "\n",
      "\n",
      "iter :111500, loss:0.065339\n",
      "roperte a nes and can also resolve exploding/vanishing graditio g neuraliliffert mom repe ave so werthing gradity to use the results from the past to generate a new result. LSTM stands for Long-Short \n",
      "\n",
      "\n",
      "iter :112000, loss:0.064539\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :112500, loss:0.063769\n",
      "roperte a nes and corM istong-term memories. LSTM is better a ne werttr RN gung to rene aeoritte mom rexual networ thads for LSTM stands from the past to generate a new result. LSTM stands for Long-Sh\n",
      "\n",
      "\n",
      "iter :113000, loss:0.063026\n",
      "roperte a nes and corg to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value\n",
      "\n",
      "\n",
      "iter :113500, loss:0.062310\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Meories. LSTM is better at handling long-term meories and can also resolve expl\n",
      "\n",
      "\n",
      "iter :114000, loss:0.061618\n",
      "roperty of RNN is its ab NN algoratte talkint long-term meories and can also resolve exploding/vanishing gradidifSradils ies ability to use the results from the past to generate a new result. LSTM sta\n",
      "\n",
      "\n",
      "iter :114500, loss:0.060951\n",
      "roperte a ne pand cim tesults from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representin\n",
      "\n",
      "\n",
      "iter :115000, loss:0.060307\n",
      "ropgese the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodhing a more complex algorithm to handle the long/short-term memories. LSTM is better at ha\n",
      "\n",
      "\n",
      "iter :115500, loss:0.059685\n",
      "roperte a ng tal aenuane aem repterty of RNN is its ability to use the results from the pasults from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a forrse teso te\n",
      "\n",
      "\n",
      "iter :116000, loss:0.059083\n",
      "roperte a ne handl forthe long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing g\n",
      "\n",
      "\n",
      "iter :116500, loss:0.058499\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :117000, loss:0.057934\n",
      "ropitte mom recurrene a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditiodalsting wong-term \n",
      "\n",
      "\n",
      "iter :117500, loss:0.057385\n",
      "roperty of RNN is its abut LSd ng/vanishing graditithe special property of RNN is its ability to use the results from the past to generate a new resu te past al lerty, its alst LSTM. RNN stands for re\n",
      "\n",
      "\n",
      "iter :118000, loss:0.056852\n",
      "roperte arit-teravaling getterm momoralve the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value repr\n",
      "\n",
      "\n",
      "iter :118500, loss:0.056334\n",
      "ropgese the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditio algorithm to handle the long/short-term memories. LSTM is better at\n",
      "\n",
      "\n",
      "iter :119000, loss:0.055830\n",
      "roperty of RNN is its ab hing net ne presuls. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradiaif a neterial property of RNN is its ability to use the result\n",
      "\n",
      "\n",
      "iter :119500, loss:0.055339\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :120000, loss:0.054860\n",
      "roperte a ne hal network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differ\n",
      "\n",
      "\n",
      "iter :120500, loss:0.054394\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :121000, loss:0.053939\n",
      "roperchal RNN b trat leciandl ne wort-term meories and can also resolve exploding/vanishing gradity to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, L\n",
      "\n",
      "\n",
      "iter :121500, loss:0.053495\n",
      "roperty of RNN is its ability to use the results frout nodw c cerm memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditiod LSTM. RNN stands for recurr\n",
      "\n",
      "\n",
      "iter :122000, loss:0.053061\n",
      "roperte a ng talkint aang can llndle the long-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gres and LSTM. RNN stands for recurrent neural networ\n",
      "\n",
      "\n",
      "iter :122500, loss:0.052637\n",
      "ropithe speciandle the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representi\n",
      "\n",
      "\n",
      "iter :123000, loss:0.052223\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of R\n",
      "\n",
      "\n",
      "iter :123500, loss:0.051818\n",
      "roperte a ng tal aendling long-term meories and can also resolve exploding/vanishing graditiod LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the resu\n",
      "\n",
      "\n",
      "iter :124000, loss:0.051421\n",
      "ropithe abot l fho Torm complex algo tha des at handling long-term meorits and can also resolve exploding/vanishing graditional RNN by handl its ability to use the results from the past to generate a \n",
      "\n",
      "\n",
      "iter :124500, loss:0.051032\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :125000, loss:0.050652\n",
      "roperte a nes and corit mom memories. LSTM is bectalkint lanity to use the results from the past to generate a form of RNN but differs from tragity to use the results from the past to generate a new r\n",
      "\n",
      "\n",
      "iter :125500, loss:0.050279\n",
      "Prediction 1 (memory): memory  remmoralithes and a the results from the past to \n",
      "Prediction 2 (LSTM): LSTM Splut bbo tomore complex algorithm to handle the l\n",
      "Prediction 3 (complex): complex rmemorecrm cis atn algorithm to handle the results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"input.txt\", seq_length)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "rnn.train(data_reader)\n",
    "\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'memory ', 50)\n",
    "print(f\"Prediction 1 (memory): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'LSTM ', 50)\n",
    "print(f\"Prediction 2 (LSTM): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'complex ', 50)\n",
    "print(f\"Prediction 3 (complex): {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b558ab-c271-4c98-ba87-675dde6faf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Special: LSTM is better at handling long-term meories ong verty of RNN is its ability to use the results\n"
     ]
    }
   ],
   "source": [
    "predicted = rnn.predict(data_reader, 'LSTM is better at handling long-term meories ', 50)\n",
    "print(f\"Prediction Special: {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e3ea0-3985-414b-bea1-ae3e58a01e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e3ef8b-dfe7-4587-99c0-5ece9941be5f",
   "metadata": {},
   "source": [
    "Train model with even easier input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546a6645-c2f0-4cd7-9933-65a4422ca904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tcoro  is aueg staawagilir s s, .   stos  snuhiwgchssren sfay sua ln r ilus iipdyowi lo sct . srss srnhcw yf awisimT. fdto p s  s,iT y suf,li  i vuvis iitsTg,hygwun ssgtui ppfrcscsa pvt.al,a eg vmiThi\n",
      "\n",
      "\n",
      "iter :0, loss:79.451393\n",
      "en c thescan w he wiwell witheden sop she model can now perform well witvopefu  y n wishe model ccn model win s model ca  n winwe model can now plyfo mpler string, hopefully the mvler string, nopefull\n",
      "\n",
      "\n",
      "iter :500, loss:57.950423\n",
      "n the model can now perform well cing, wingopelllly wopefully the model can now perform well  ithe mpler string, hopepu li wifell well withodmalea the eimpler string, hopefully the model madel can now\n",
      "\n",
      "\n",
      "iter :1000, loss:35.385280\n",
      "inel can now perform well withopefulll withop vle  im we m delm well withe model can now perform well withopefulll ri ll withopefug, hopefully the model can now perform well withelrf even simpler stri\n",
      "\n",
      "\n",
      "iter :1500, loss:21.559222\n",
      "tde  ieea  ton string, hopefully the model can now perfo mowell weoerform well withopeveorer hinow perform we,  the model can now perform well withelef st e model can now perform well withesev she mod\n",
      "\n",
      "\n",
      "iter :2000, loss:13.135359\n",
      "can w le  ithe model can now perform well withopefug, hopefully the model can now perform well withelmplel can now perform well withopefug, hopefully the model can now perform well withopmfd lingw hel\n",
      "\n",
      "\n",
      "iter :2500, loss:8.008959\n",
      "ing, hopefully the model can now perform well withopefulll withophodey can wotring, hopefully the model can now perform well withopefully win eimpler string, hopefully the model can now perform well w\n",
      "\n",
      "\n",
      "iter :3000, loss:4.890406\n",
      "elec tteley the model can now perform well withopefulll tiegmodel can now perform well withopefully the model can now perform well withis flll  horfodelfulel ss a evel simpler string, hopefully the mo\n",
      "\n",
      "\n",
      "iter :3500, loss:2.993072\n",
      "can now perform well withopeodel can now perform well withopef en simpler string, hopefully the model can now perform well withopefully the model can now perform well withopeodellc i mon withopelllrm \n",
      "\n",
      "\n",
      "iter :4000, loss:1.838210\n",
      "ing, hopefully the model can now perform well withopefully wingsimpler string, hopefully the model can now perform well withishopefully the model can now perform well withopefug, hopefully the model c\n",
      "\n",
      "\n",
      "iter :4500, loss:1.134770\n",
      "eler string, hopefully the model can now perform well withopefulll withesev noe can now perform well withopefully w rfowalll can now perform well withop flll can now perform well withiswfplfp er aieve\n",
      "\n",
      "\n",
      "iter :5000, loss:0.705834\n",
      "w the m well withopefwell mplly the model can now perform well withelmodel can now perform well withelef st ing, hopefully the model can now perform well withopefulll yithe model can now perform well \n",
      "\n",
      "\n",
      "iter :5500, loss:0.443864\n",
      "ing, hopefully the model can now perform well withelm wepl withopefully the model can now perform well withopefully the model can now perform well withopefdel sim del can now perform well withopeodel \n",
      "\n",
      "\n",
      "iter :6000, loss:0.283516\n",
      "eler then a even simpler string, hopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well withopeodel can now perform well witho\n",
      "\n",
      "\n",
      "iter :6500, loss:0.185045\n",
      "can now perform well withopefully the model can now perform well withelm stri holgpllll wi woperform well withoper eten a even simpler string, hopefully the model can now perform well withelm sery the\n",
      "\n",
      "\n",
      "iter :7000, loss:0.124273\n",
      "ing, hopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well withopeodel can now pe\n",
      "\n",
      "\n",
      "iter :7500, loss:0.086513\n",
      "eleriepe  the model can now perform well withevmodel can now perform well withopeodel can now perform well wiel withe model can now perform well withopeoler string, hopefully the model can now perform\n",
      "\n",
      "\n",
      "iter :8000, loss:0.062819\n",
      "can nod s pler string, hopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well withopeodelll can ieel wineorm well withopefully\n",
      "\n",
      "\n",
      "iter :8500, loss:0.047742\n",
      "ing, hopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well withopefully the model\n",
      "\n",
      "\n",
      "iter :9000, loss:0.037977\n",
      "opeow he model can non simpler string, hopefully the model can now perform well withopmodel can nolef lly thel a even simpler string, hopefully the model can now perform well withopefuer  woll wisefol\n",
      "\n",
      "\n",
      "iter :9500, loss:0.031501\n",
      "can now perform well withopeodelll can wethon eimpler string, hopefully the model can now perform well withopefully the model can now perform well withopeven simpler string, hopefully the model can no\n",
      "\n",
      "\n",
      "iter :10000, loss:0.027070\n",
      "ing, hopefully the model can now perform well withelm seri thermodelly the model can now perform well withelw sermowm well well wipe form well withopefully the model can now perform well withopeodelll\n",
      "\n",
      "\n",
      "iter :10500, loss:0.023939\n",
      "open sven a even simpler string, hopefully the model can now perform well withe model can now pe form well withopeow pe woperuerm well withop vlerm tel can now perform well withopefully the model can \n",
      "\n",
      "\n",
      "iter :11000, loss:0.021639\n",
      "can codel can now perform well withopeodelll now perform wele  ithopmod plcan now perform well withopefuefully the model can now perform well withopefully the model can now perform well withe model ca\n",
      "\n",
      "\n",
      "iter :11500, loss:0.019877\n",
      "ing, hopefully the model can now perform well withopevler string, hopefully the model can now perform well withe model can n wi wopefully the mowel can n wele  ithopeow thorm well withopefully the mod\n",
      "\n",
      "\n",
      "iter :12000, loss:0.018481\n",
      "elm well withopefully the model can now perform well withopmodel can now perform well chn can now perform well withopefully the model can now perform well withopefully the model can now perform well w\n",
      "\n",
      "\n",
      "iter :12500, loss:0.017337\n",
      "can now perform well withopeovefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well withe model can now perform well withopeod l\n",
      "\n",
      "\n",
      "iter :13000, loss:0.016366\n",
      "ing, hopefully the model can now perform well withopeodel can now perform well withopeodelll nom del can now perform well withopeodefully the model can now perform well withopefully the model can now \n",
      "\n",
      "\n",
      "iter :13500, loss:0.015529\n",
      "opmfdel can now perform well withopmodel can noweperform well withopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well withop\n",
      "\n",
      "\n",
      "iter :14000, loss:0.014792\n",
      "can wothovel  wnom ser stming, hopefully the model the model can ierf wisefplly the model can now perform well withopmodel can now perform well withopeveell model can nom dellcan wopefully the model c\n",
      "\n",
      "\n",
      "iter :14500, loss:0.014130\n",
      "ing, hopefully the model can now perform well withopefully the model can now perform well withopefully the model can now perform well withopevlerm well withopefully the model can now perform well with\n",
      "\n",
      "\n",
      "iter :15000, loss:0.013534\n",
      "elmowell witrfl pl a even simpler string, hopefully the model can now perform well withopefully the model can now perform well withopmodel can iell wiser hthi model can now perform well withopmodel ca\n",
      "\n",
      "\n",
      "iter :15500, loss:0.012990\n",
      "can now perform well withop fper stefully the model can now perform well withopeodelll nom del can now perform well withopefully the model can now perform well withopmodel can now perform well withope\n",
      "\n",
      "\n",
      "iter :16000, loss:0.012489\n",
      "ing, hopefully the model can now perform well withopefully the model can now perform well withopmfler string, hopefully the model can now perform well withopmodel can now perw wi hopefully the model c\n",
      "\n",
      "\n",
      "iter :16500, loss:0.012027\n",
      "opmodel can now perform well withopeodelll n m del can now perform well withopeodel can now perform well withopeo, hodel nven the model can now perform well withopeodel can now perform well withopeow \n",
      "\n",
      "\n",
      "iter :17000, loss:0.011600\n",
      "can woteing, hopefully the model can now perform well withopefully the model can now perform well withopeodel can now perform well withopefully the model can now perform well withopeodel can now perfo\n",
      "\n",
      "\n",
      "iter :17500, loss:0.011200\n",
      "ing, hopefully the model can now perform well withopeodells now perform well withelmasen simpler string, hopefully the model can now perform well withel fier string, hopefully the model can now perfor\n",
      "\n",
      "\n",
      "iter :18000, loss:0.010828\n",
      "ele  the model can now perform well withe model can now perform well withopefully the model can now perform well withopefeer  wowelllli he m del simpler string, hopefully the model can now perform wel\n",
      "\n",
      "\n",
      "iter :18500, loss:0.010480\n",
      "can now perform well withopeodelll now perform wele withopefully the model can now perform well withopefully the model can now perform well withopeoperm well withopefuefu mpler string, hopefully the m\n",
      "\n",
      "\n",
      "iter :19000, loss:0.010151\n",
      "hngw rale  ithe model can now perform well withopefully the model can now perform well withopmodel can n wepe withing, hepefug, mopefully the model can now perform well withopmodel can s winhopefully \n",
      "\n",
      "\n",
      "iter :19500, loss:0.009844\n",
      "e model can now perform well withopevue m well withelm del  well withe model can now perform well withopeodelly n ev n ein witho model can now perform well withopeodelll now well ,ing, hopefully the m\n",
      "\n",
      "\n",
      "iter :20000, loss:0.009554\n",
      "can noe  herm well withopefully the model can now perform well withopefulfy hocan nothing, hopefully the model can now perform well withopefdel   npe mor n el petmiwell withos a even simpler string, h\n",
      "\n",
      "\n",
      "iter :20500, loss:0.009279\n",
      "ing, hopefully the model can now perform well withopeorefully the model can now perform well withopmodel can now perform well withopeopell well withopeoperm wele  ithotefsrv model can now perform well\n",
      "\n",
      "\n",
      "iter :21000, loss:0.009021\n",
      "elm well withope ug,inow perform well withe model can now perform well withopeadelen now perform well withi even s mpler string, hopefully the model can now perform well withe model can now perform we\n",
      "\n",
      "\n",
      "iter :21500, loss:0.008776\n",
      "can hodel can now perform well withe model can now perform well withopmodel can nowele hn model can now perform well withe model can now perform well withopefully the model can now perform well withop\n",
      "\n",
      "\n",
      "iter :22000, loss:0.008542\n",
      "ing, hopefully the model can now perform well withopefulld the model can now perform well withopmodel can nowelen string, hopefully the model can now perform well withopeodelllle m sellpg, hopefully t\n",
      "\n",
      "\n",
      "iter :22500, loss:0.008321\n",
      "e model can non simpler string, hopefully the model can now perform well withopeodeli hope morelev n evev s can now perform well withopeow can iell  eler string, hopefully the model can now perform we\n",
      "\n",
      "\n",
      "iter :23000, loss:0.008111\n",
      "can now perform well withopeodel can no, hell well withisefully can niw llrform well withiwell  now perform well withopefully the model can now perform well withe model can now perform wiperform well \n",
      "\n",
      "\n",
      "iter :23500, loss:0.007910\n",
      "ing, hopefully the model can now perform well withopefdefully the model can now perform well withopevee m wele withopefuely well withopefully the model can now perform well withopefuefully the model c\n",
      "\n",
      "\n",
      "iter :24000, loss:0.007719\n",
      "e model can now perform well withe model can now perform well withopeodel can now perform well withelmodel can now perform well witho model can noweimplyrastr ser eg, hopefully the model can now perfo\n",
      "\n",
      "\n",
      "iter :24500, loss:0.007537\n",
      "can sow per orm well withopefully the model can now perform well withopeodel   npe hehe model can now perform well withelmodel can now perform well withopeodelichn model can now perform well withopmol\n",
      "\n",
      "\n",
      "iter :25000, loss:0.007362\n",
      "ing, hopefully the model can now perform well withe model can now perform well withow the model can iope model can now perform well withopefler string, hopefully the model can now perform well withope\n",
      "\n",
      "\n",
      "iter :25500, loss:0.007195\n",
      "opeowell will w tven simpler string, hopefully the model can now perform well withopefully the model can now perform well withopeodell  now perform well withopeoperi model can now perform well withopm\n",
      "\n",
      "\n",
      "iter :26000, loss:0.007036\n",
      "can fow perform well withe model can noweler ithopefuely wele withop iwell well withele withel fuell wine form wg, hopefully the model can now perform well withopefler string, hopefully the model can \n",
      "\n",
      "\n",
      "iter :26500, loss:0.006882\n",
      "ing, hopefully the model can now perform well withopeodellp can now perform wiperform well withe model can now perform well withopeodel can now perform well withopeoler string, hopefully the model can\n",
      "\n",
      "\n",
      "iter :27000, loss:0.006736\n",
      "iseopefully the model can now perform well withopefuer  welll can n well withe model can now perform well withopmfdef can now perform well withopevlerm well withelm well withopevlerm well withelm well\n",
      "\n",
      "\n",
      "iter :27500, loss:0.006595\n",
      "n now thoper tven pimpler string, hopefully the model can now perform well withopeodelly the model can now perform well withopeodell wiper sthi thon aieven simpler string, hopefully the model can now \n",
      "\n",
      "\n",
      "iter :28000, loss:0.006459\n",
      "ing, hopefully the model can now perform well withopealer simiwithopefully the model can nowoperform well withopeopefully model can canen well withe model can now perform well withopeodel cwn model ca\n",
      "\n",
      "\n",
      "iter :28500, loss:0.006329\n",
      "i eope ngee  ser  tel can now perform well withopeodellp can now perform well withope withopefully the model can now perform well withopeodelll n model can now perform well withelm ser ng, hopefully t\n",
      "\n",
      "\n",
      "iter :29000, loss:0.006205\n",
      "can now perform well withe model can nothoee lan ing, hopefully the model can now perform well withopen ehestring, hopefully the model can now perform well withopmodel can i ther stel  simpler string,\n",
      "\n",
      "\n",
      "iter :29500, loss:0.006084\n",
      "ing, hopefully the model can now perform well withopeodel  we model can now perform well withe model can now perform well withopevlerm well withe model can now perform well withopefully the model can \n",
      "\n",
      "\n",
      "iter :30000, loss:0.005968\n",
      "ele hthe model can now perform well withopmodel can n ho eow thorm well withe model can now perform well withopefulfu n wiwefully the model can now perform well withopeodefully the model can now perfo\n",
      "\n",
      "\n",
      "iter :30500, loss:0.005856\n",
      "can ,ow perform well withe model can now perform well withopeodel nwell withelm ser  welellcan now pler string, hopefully the model can now perform well withopeodelm wnpe withoper even aiea peef well \n",
      "\n",
      "\n",
      "iter :31000, loss:0.005748\n",
      "ing, hopefully the model can now perform well withelmpler then a even simpler string, hopefully the model can now perform well withe model can now perform well withopeodelln now perform well withopefu\n",
      "\n",
      "\n",
      "iter :31500, loss:0.005644\n",
      "ele hell well withe model can now perform well withopeodelly the model can now perform well withopeodelll now well withod the model can lefully the model can now perform well withopeodel c n model can\n",
      "\n",
      "\n",
      "iter :32000, loss:0.005544\n",
      "can now perform well withopeodel  helm well withopeow can now perform well withopea erm well withopeo, honefully the model can now perform well withopeodel swellowell  ithopeolerfnrm wellu even simple\n",
      "\n",
      "\n",
      "iter :32500, loss:0.005446\n",
      "ing, hopefully the model can now perform well withopeodelll now pell withos peer itring, hopefully the model can now perform well withopeodel n mon now pe rormowellrring, hopefully the model can now p\n",
      "\n",
      "\n",
      "iter :33000, loss:0.005353\n",
      "elev the model can now perform well withopeoper steing, hopefully the model can now perform well withopeodelicwn model can now perform well withopmodel can now pevan simpler string, hopefully the mode\n",
      "\n",
      "\n",
      "iter :33500, loss:0.005263\n",
      "can wotefully isev neefutrl ea  tel can now perform well withopmfler steing, hopefully the model can now perform well withelea the model can now perform well withopmodel can w thonmpielly tler niell r\n",
      "\n",
      "\n",
      "iter :34000, loss:0.005174\n",
      "ing, hopefully the model can now perform well withopevherm well withopeodel can now perform well withopeodel n ein withormodel can now perform well withopeodelly the model can now perform well withope\n",
      "\n",
      "\n",
      "iter :34500, loss:0.005089\n",
      "e model can now perform well withopevlerm well withe model can now perform well withopmodel can nowe eowithorm del  hell well withopefully the model can now perform well withopmodel can i thow perf we\n",
      "\n",
      "\n",
      "iter :35000, loss:0.005008\n",
      "can now perform well withe model can now pell withos peeven simpler string, hopefully the model can now perform well withopeodel  we model can now perform well withopmodel can now perform wopefully th\n",
      "\n",
      "\n",
      "iter :35500, loss:0.004927\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopefuefully the model can now perform well withopevlerm well withe model can now perform well withis an noeform well \n",
      "\n",
      "\n",
      "iter :36000, loss:0.004850\n",
      "ele fere mpler string, hopefully the model can now perform well withopeodelly the model can now perform well withopmodel can well withe model can now performlwn wopefully the model can now perform wel\n",
      "\n",
      "\n",
      "iter :36500, loss:0.004776\n",
      "can now perform well withopeopefully the model can now perform well withopmodel can ieven simpler string, hopefully the model can now perform well withopevlerm well withe model can now perform well wi\n",
      "\n",
      "\n",
      "iter :37000, loss:0.004702\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopeodellswe modellcan now perform well withe model can now perform well withopeodel cwn model can now perform well wi\n",
      "\n",
      "\n",
      "iter :37500, loss:0.004632\n",
      "elmpler string, hopefully the model can now perform well withe model can now perform well withopeodelm wi wele witholefu e  form well witho model can nowell wirieri mpler string, hopefully the model c\n",
      "\n",
      "\n",
      "iter :38000, loss:0.004564\n",
      "can now perform well withopeodellp can now perform wiperform well withopefully the model can now perform well withelmplel cal now perform well withele hell well withe model can now perform well withop\n",
      "\n",
      "\n",
      "iter :38500, loss:0.004497\n",
      "ing, hopefully the model can now perform well withopmodel can iell wiperuotring, hopefully the model can now perform well withopeodel chn model can now perform well witho model can w thogellc simpler \n",
      "\n",
      "\n",
      "iter :39000, loss:0.004432\n",
      "elm dell wi modellc i modelllfo molel can now perform well withopevwn wiwell withopefully the model can now perform well withopeodefully the model can now perform well withe model can now perform well\n",
      "\n",
      "\n",
      "iter :39500, loss:0.004369\n",
      "can now perform well withopeodell  now perform well withopeopefully the model can now perform well witho model can thel  hell model can now perform well withopeodellswm well well wiper ewel pler stran\n",
      "\n",
      "\n",
      "iter :40000, loss:0.004308\n",
      "ing, hopefully the model can now perform well withopeaherm well withopefully the model can now perform well withe model can now perform well withopeopefully the model can now perform well withopevlerm\n",
      "\n",
      "\n",
      "iter :40500, loss:0.004249\n",
      "opeowere mpler string, hopefully the model can now perform well withopmodel can nowele  the model can now perform well withopeodelllln wopefully the model can now perform well withopmodel can nowell w\n",
      "\n",
      "\n",
      "iter :41000, loss:0.004191\n",
      "can now perform well withopmodel can noteo model can now perform well withopeodelllle m del  iell wiser ntring,ihopefully the model can now perform well withopeodelmpwell well withele withopefully the\n",
      "\n",
      "\n",
      "iter :41500, loss:0.004134\n",
      "ing, hopefully the model can now perform well withopefullu thevenmpler string, hopefully the model can now perform well withe model can now perform well withn model can now perform well withopmfler st\n",
      "\n",
      "\n",
      "iter :42000, loss:0.004079\n",
      "ele he m well witho model can w thiwmod plcai can now perform well withophodel can nowell withopeodell  now perform well withopeodelll now perf he mpler string, hopefully the model can now perform wel\n",
      "\n",
      "\n",
      "iter :42500, loss:0.004026\n",
      "can now perform well withopmodel can nowell withopwfdel w ehellc n m del  hell well  the  pler string, hopefully the model can now perform well withelm ser  wolefully the model can now perform well wi\n",
      "\n",
      "\n",
      "iter :43000, loss:0.003974\n",
      "ing, hopefully the model can now perform well withopeiuell mpler string, hopefully the model can now perform well withopeodeli wn wopefully the model can now perform well withe model can now perform w\n",
      "\n",
      "\n",
      "iter :43500, loss:0.003923\n",
      "ele  ell wiow perfnrm weler ther a even simpler string, hopefully the model can now perform well withopeodel n model n model can now perform well withopmodel can n wi wopefully the model can now perfo\n",
      "\n",
      "\n",
      "iter :44000, loss:0.003874\n",
      "can now perform well withopeopefully the model can now perform well witho model can iell witge model can now perform well withopmodel can w thoreler sim ler ctrform well withopeodel n model can now pe\n",
      "\n",
      "\n",
      "iter :44500, loss:0.003825\n",
      "ing, hopefully the model can now perform well withopeodellowell withopmodel can now perform well withopeodel  well well withopefully the model can now perform well withopeodel  wn model can now perfor\n",
      "\n",
      "\n",
      "iter :45000, loss:0.003778\n",
      "ele  eve mpler string, hopefully the model can now perform well withopeode mowefelly the model can now perform well withopeodell wiperform well withe model can now perform well withopmodel can selll s\n",
      "\n",
      "\n",
      "iter :45500, loss:0.003732\n",
      "can now perform well withopmodel can w thow plly the modellcan wope uotnovel string, hopefully the model can now perform well withe model can now perform well witho model can ieg, hopefully the model \n",
      "\n",
      "\n",
      "iter :46000, loss:0.003687\n",
      "ing, hopefully the model can now perform well withopmodel can iera wopefully the model can now perform well withopefuefully the model can now perform well withopmodel can w we wopefully the model can \n",
      "\n",
      "\n",
      "iter :46500, loss:0.003643\n",
      "elmyler string, hopefully the model can now perform well withopeodeli wi wopefully the model can now perform well withopeodellply the wopefully the model can now perform well withopeodel swe model can\n",
      "\n",
      "\n",
      "iter :47000, loss:0.003601\n",
      "can now perform well withopeodefully the model can now perform well withopeodellplefully the model can now perform well withe model can now perform well withopeodellswe modellcan now perform well with\n",
      "\n",
      "\n",
      "iter :47500, loss:0.003558\n",
      "ing, hopefully the model can now perform well withopeodel ply mpdel simpler string, hopefully the model can now perform well withopmodel can ieg, hopefully the model can now perform well withopmodef ,\n",
      "\n",
      "\n",
      "iter :48000, loss:0.003518\n",
      "elm well withope he e mpler string, hopefully the model can now perform well withopefdell  n model can now perform well witheleo ell wi wope witelll  impl can now perform well witho model can wethong,\n",
      "\n",
      "\n",
      "iter :48500, loss:0.003478\n",
      "can now perform well withe model can now perform well withopmodel can wew  wel nimen well withe model can now perform well withopeodel chn model can now perform well withopeodelin now perform well wit\n",
      "\n",
      "\n",
      "iter :49000, loss:0.003439\n",
      "ing, hopefully the model can now perform well withopmodel can nowell witholmodel can now perform well withopeodell wiow perform well withiaell  now perform well withopmodel can weteing, hopefully the \n",
      "\n",
      "\n",
      "iter :49500, loss:0.003400\n",
      "elef ev can now perform well withopeodel swepe withopefully the model can now perform well witho model can noweler ithopeowell well withopefully the model can now perform well withopmodel can wele mod\n",
      "\n",
      "\n",
      "iter :50000, loss:0.003363\n",
      "can nod serform well withopeopefully the model can now perform well withe model can now perform well witho model can wele withop the molefully the model can now perform well withe model can now perfor\n",
      "\n",
      "\n",
      "iter :50500, loss:0.003326\n",
      "ing, hopefully the model can now perform well withopeodel simplefully the model can now perform well withopeodel  he m del  iere mpler string, hopefully the model can now perform well withopmodel can \n",
      "\n",
      "\n",
      "iter :51000, loss:0.003291\n",
      "elm stwing, hopefully the model can now perform well witholmodel can now perform well withelmodel can now perform well withopeodells eow lerform well withopefully the model can now perform well withop\n",
      "\n",
      "\n",
      "iter :51500, loss:0.003256\n",
      "can now perform well withi model can n we eow the model can now perform well withe model can now perform well withopeodell wi wopefully the model can now perform well withopeodelm well well withele wi\n",
      "\n",
      "\n",
      "iter :52000, loss:0.003221\n",
      "ing, hopefully the model can now perform well withopeodel  we model can now perform well withopeodel cwn model can now perform well withopeodelll ull  ttmn non peerowell witho eode model can now perfo\n",
      "\n",
      "\n",
      "iter :52500, loss:0.003188\n",
      "e model can now perform well withe model can now perform well withopevlerm well withe model can now perform well withopeodelly the model can now perform well withopevlerm well withe model can now perf\n",
      "\n",
      "\n",
      "iter :53000, loss:0.003155\n",
      "can now perform well withe model can now perform well withopmodef thi mowell wiplrfull moley the model can now perform well withopeodellswi wele well wiperform well withopevler  welefully the model ca\n",
      "\n",
      "\n",
      "iter :53500, loss:0.003122\n",
      "ing, hopefully the model can now perform well withopmodel can nowell withopefuefully the model can now perform well withopeodel s ele  the model can now perform well withopmodel can wele withop nde  t\n",
      "\n",
      "\n",
      "iter :54000, loss:0.003091\n",
      "eleowell withelev nven simpler string, hopefully the model can now perform well withopeodel  we model can now perform well withopevler  wele withowefully the model can now perform well withopmodel can\n",
      "\n",
      "\n",
      "iter :54500, loss:0.003060\n",
      "can now perform well withi model can n he model can now perform well withopeodelm well well well withe model can now perform well withopeodells mow perform well withopmodel can node model can now perf\n",
      "\n",
      "\n",
      "iter :55000, loss:0.003029\n",
      "ing, hopefully the model can now perform well withelmodel can now perform well withopeodelllwi wopefully the model can now perform well withelmodel can now perform well withopeodelllwi wopefully the m\n",
      "\n",
      "\n",
      "iter :55500, loss:0.002999\n",
      "elm well withopefuer  eel withe model can now perform well withopmoler string, hopefully the model can now perform well withopmodel can nowele withopefully the model can now perform well withopmodef e\n",
      "\n",
      "\n",
      "iter :56000, loss:0.002970\n",
      "can model can wele  nthi mow t even simpler string, hopefully the model can now perform well withelmolel can now perform well withopevlerm well withe model can now perform well withopmodel can nowell \n",
      "\n",
      "\n",
      "iter :56500, loss:0.002941\n",
      "ing, hopefully the model can now perform well withopeveerm well withe model can now perform well withopeodellswi we m well withopefully the model can now perform well witho model can wele model can no\n",
      "\n",
      "\n",
      "iter :57000, loss:0.002913\n",
      "opmodel can now perform well withopeodefully the model can now perform well withopeodel cwn model can now perform well withopeodeli wn wopefully the model can now perform well withopevlerm well withe \n",
      "\n",
      "\n",
      "iter :57500, loss:0.002886\n",
      "can now perform well withopeodellp can now lerform well withe model can now perform well withopefully the model can now perform well withopefulfu n wipefully model can now perform well withopmodel can\n",
      "\n",
      "\n",
      "iter :58000, loss:0.002859\n",
      "ing, hopefully the model can now perform well withopmodel can nowell withopeodells now ferform well withopmodel can now perform well withopeodel swe model can now perform well withopmodel can nowele  \n",
      "\n",
      "\n",
      "iter :58500, loss:0.002832\n",
      "e model can now perform well withopeodele  now perform well withe model can now perform well withopeodelin now perform well withelmodel can now perform well withopmodel can nowell withel a even simple\n",
      "\n",
      "\n",
      "iter :59000, loss:0.002806\n",
      "can now perform well withopeodell wi wope withfosop the  hw  mow wina hm well withe model can now perform well witho model can nowell withopmodel can now perform well withelmpler cal n e  even simpler\n",
      "\n",
      "\n",
      "iter :59500, loss:0.002780\n",
      "ing, hopefully the model can now perform well withopmodel can none model can now perform well withopmodel can iewo modellcan now perform well withe model can now perform well withopeodelly the model c\n",
      "\n",
      "\n",
      "iter :60000, loss:0.002755\n",
      "elm well withopen sthing, hopefully the model can now perform well withopeodelly the model can now perform well withopevlerm well withe model can now perform well withopevlerm well withe model can now\n",
      "\n",
      "\n",
      "iter :60500, loss:0.002730\n",
      "can nor cell  ithe model can now perform well withopmodel can iell withe  fdel can now perform well withe model can now perform well withopmtherm well well withopeowefully the model can now perform we\n",
      "\n",
      "\n",
      "iter :61000, loss:0.002706\n",
      "ing, hopefully the model can now perform well withopeodelllly m well wiperform well withopefully the model can now perform well withopmodel can iell wiperf ta wele withopefully the model can now perfo\n",
      "\n",
      "\n",
      "iter :61500, loss:0.002682\n",
      "elm well withe model can now perform well withelmpler string, hopefully the model can now perform well withopeodeli wi model can now perform well withelmpler string, hopefully the model can now perfor\n",
      "\n",
      "\n",
      "iter :62000, loss:0.002659\n",
      "can now perform well withi moden the model can now perform well withopeodel  wn wopefully the model can now perform well witholmodel can now perform well withopefulfula wiser tteimpler string, hopeful\n",
      "\n",
      "\n",
      "iter :62500, loss:0.002635\n",
      "ing, hopefully the model can now perform well withopmodel can nowell withopefler string, hopefully the model can now perform well withelmpler string, hopefully the model can now perform well withopmod\n",
      "\n",
      "\n",
      "iter :63000, loss:0.002612\n",
      "e model can now perform well withopeodel  wngwoperuewm well well withel aseven simpler string, hopefully the model can now perform well withopeodelin now perform well withe model can now perform well \n",
      "\n",
      "\n",
      "iter :63500, loss:0.002590\n",
      "can now perform well withopenrerm well withi evll y even simpler string, hopefully the model can now perform well withelmpler ng, hopefully the model can now perform well withe model can now perform w\n",
      "\n",
      "\n",
      "iter :64000, loss:0.002568\n",
      "ing, hopefully the model can now perform well withopeodela wnpeope model can now perform well withopmodel can well withe model can now perform well withe model can now perform well withopeodel  we mod\n",
      "\n",
      "\n",
      "iter :64500, loss:0.002546\n",
      "e model can now perform well withe model can now perform well withopmodel can wele withel an a even simpler string, hopefully the model can now perform well withe model can now perform well withelmple\n",
      "\n",
      "\n",
      "iter :65000, loss:0.002525\n",
      "can now perform well withe model can now perform well withelmodel can now perform well witho model can well withe model can iell wis form wepe mpler string, hopefully the model can now perform well wi\n",
      "\n",
      "\n",
      "iter :65500, loss:0.002504\n",
      "ing, hopefully the model can now perform well withopmodel non llthing, hopefully the model can now perform well withopeodelin now perform well withelmodel can now perform well withopmodel can ieg, hop\n",
      "\n",
      "\n",
      "iter :66000, loss:0.002484\n",
      "elm ser string, hopefully the model can now perform well withopeodell wi wopefully the model can now perform well withopeoper stefully the model can now perform well withe model can now perform well w\n",
      "\n",
      "\n",
      "iter :66500, loss:0.002464\n",
      "can now perform well withopevlerustring, hopefully the model can now perform well withopmodel can wele  the model can now perform well witho model can iell wisw pef, hopefully the model can now perfor\n",
      "\n",
      "\n",
      "iter :67000, loss:0.002443\n",
      "ing, hopefully the model can now perform well withelmodel can now perform well withopeodellpen wing wiow pler string, hopefully the model can now perform well withopmodel can nowell withopeodel cwi ho\n",
      "\n",
      "\n",
      "iter :67500, loss:0.002424\n",
      "e model can now perform well withe model can now perform well withelmodel can now perform well withopevee m wele  eteo eow periope wgperw tellc i hoperform wellfven simpler string, hopefully the model\n",
      "\n",
      "\n",
      "iter :68000, loss:0.002405\n",
      "can now perform well withe model can now perform well withopeodelmpwell withell well withopeohefully the model can now perform well withopeoler string, hopefully the model can now perform well withope\n",
      "\n",
      "\n",
      "iter :68500, loss:0.002385\n",
      "ing, hopefully the model can now perform well withopeodel  wi model can now perform well withopmodel can n well withi model can nowelly mow perithas aieven nom iepe wope model can now perform well wit\n",
      "\n",
      "\n",
      "iter :69000, loss:0.002366\n",
      "e model can now perform well withopeodel  wn wopefully the model can now perform well withopmodel can nele mgwell wnpe withopefully the model can now perform well withopevlerm well withe model can now\n",
      "\n",
      "\n",
      "iter :69500, loss:0.002348\n",
      "can now perform well withe model can now perform well withopmodel can well witho model n model can now perform well withopmodel can n wiper ntmodel can now perform well withopmodel can weteing, hopefu\n",
      "\n",
      "\n",
      "iter :70000, loss:0.002330\n",
      "ing, hopefully the model can now perform well withopeodel swnpeofully the model can now perform well withelmodel can now perform well withopmodel can iell wiowepefully the model can now perform well w\n",
      "\n",
      "\n",
      "iter :70500, loss:0.002312\n",
      "e model can now perform well withopmodel can nowell withopeoperm well withing, hopefully the model can now perform well withopeonefully the model can now perform well witho model can wethono, hopefuel\n",
      "\n",
      "\n",
      "iter :71000, loss:0.002294\n",
      "can model can ieg, hopefully the model can now perform well withe model can now perform well withopeodelllle model can now perform well withe model can now perform well withopeiuell mpler string, hope\n",
      "\n",
      "\n",
      "iter :71500, loss:0.002277\n",
      "ing, hopefully the model can now perform well witho model can well withe model can now perform well witho modef nonan well simpler cal n w ler stmodel can now perform well withopmodel can witho model \n",
      "\n",
      "\n",
      "iter :72000, loss:0.002259\n",
      "elm ser string, hopefully the model can now perform well withopmodel can wele  the model can now perform well witho model can iell wislen the mohefelllw hn w wepllw the model can now perform well with\n",
      "\n",
      "\n",
      "iter :72500, loss:0.002243\n",
      "can now perform well withopeodel n mowellc now perler string, hopefully the model can now perform well withopmodel can n we mow pl aisteing, hopefully the model can now perform well withopmodel can ie\n",
      "\n",
      "\n",
      "iter :73000, loss:0.002226\n",
      "ing, hopefully the model can now perform well witho model can well witho model can nowele wi model can now perform well withopefully the model can now perform well withopeodel plll pier string, hopefu\n",
      "\n",
      "\n",
      "iter :73500, loss:0.002209\n",
      "ele hell well withinell  now perform well withe model can now perform well witho model can nole uow formodel can now perform well withopevee m wele  eve  piering, hopefully the model can now perform w\n",
      "\n",
      "\n",
      "iter :74000, loss:0.002193\n",
      "can now perform well withe model can now perform well withopevee m well witho model can now perform well withopmodel can iera wopefully the model can now perform well withe model can now perform well \n",
      "\n",
      "\n",
      "iter :74500, loss:0.002177\n",
      "ing, hopefully the model can now perform well withopmodel can nowell withopefueru teing, hopefully the model can now perform well withelmow ll ring, hopefully the model can now perform well withe mode\n",
      "\n",
      "\n",
      "iter :75000, loss:0.002161\n",
      "eley the model can now perform well withopeodellswm well well wiow pera wi wiper  well withi modefully the model can now perform well withopmodel can nowell withopmodel can now perform well withopmode\n",
      "\n",
      "\n",
      "iter :75500, loss:0.002146\n",
      "can now perform well withe model can now perform well witho model can n we eow pl withod a even simpler string, hopefully the model can now perform well withopeodelllle model can now perform well with\n",
      "\n",
      "\n",
      "iter :76000, loss:0.002130\n",
      "ing, hopefully the model can now perform well withopmodel can nowell withopeodellswa model can noweler sthing, hopefully the model can now perform well withopmodel can weteong, hopefully the model can\n",
      "\n",
      "\n",
      "iter :76500, loss:0.002115\n",
      "e model can now perform well witho model can nowele  itho model simperm well withe model can now perform well withopeodel swepefuell witho model can nowell withopefuell wiperform well withe model can \n",
      "\n",
      "\n",
      "iter :77000, loss:0.002100\n",
      "can now perform well witho model can wele model cworm well witho model can weno iopefully the model can now perform well withopeodelicwi model can now perform well withopeodell wi wopefully the model \n",
      "\n",
      "\n",
      "iter :77500, loss:0.002085\n",
      "ing, hopefully the model can now perform well withopmodel can ieveompler string, hopefully the model can now perform well withopevlerm well withi model can sero modellcan wopelnowin non withophodel ev\n",
      "\n",
      "\n",
      "iter :78000, loss:0.002071\n",
      "elm well withelm well withopeopefully the model can now perform well withopeodelin now perform well witho model can wele  the model can now perform well withe model can now perform well withopmodel ca\n",
      "\n",
      "\n",
      "iter :78500, loss:0.002057\n",
      "can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withopmodel can new perform well withe model can now perform well withe model ca\n",
      "\n",
      "\n",
      "iter :79000, loss:0.002042\n",
      "ing, hopefully the model can now perform well withi model can wethine, hopefully the model can now perform well withopeodel  wnpenfelly the model can now perform well withopmodef lll can lel can now p\n",
      "\n",
      "\n",
      "iter :79500, loss:0.002028\n",
      "e model can now perform well withe model can now perform well withopmodel can wele  the model can now perform well withopmodel can i wipeowithopefully the model can now perform well withopmodel can ne\n",
      "\n",
      "\n",
      "iter :80000, loss:0.002015\n",
      "can now perform well withopeodel nwepeowithelm well withopeodel n eim well withelm well withelly the model can now perform well withopmodel can iell wiperf la wopefully the model can now perform well \n",
      "\n",
      "\n",
      "iter :80500, loss:0.002001\n",
      "ing, hopefully the model can now perform well withe model can now perform well withe model can now perform well withopmodef can nowellc n model can now perform well withopeodel cwn model can now perfo\n",
      "\n",
      "\n",
      "iter :81000, loss:0.001987\n",
      "even simpler string, hopefully the model can now perform well withopmodel can w pe model can now perform well withopeodelin now perform well withe model can now perform well withopeodel can now perfor\n",
      "\n",
      "\n",
      "iter :81500, loss:0.001974\n",
      "can now perform well withopeodelully the model can now perform well witho model can well withe model can now perform well witho model can well withe model can now perform well withe model can now perf\n",
      "\n",
      "\n",
      "iter :82000, loss:0.001961\n",
      "ing, hopefully the model can now perform well withopeodelmpwopefully the model can now perform well withopmodel can iehl model can now perform well withopeolefully the model can now perform well withe\n",
      "\n",
      "\n",
      "iter :82500, loss:0.001948\n",
      "e model can now perform well witho model can wele withop owefully the model can now perform well withopmodel can w le witho model can now perform well withopeodell wi wopefully the model can now perfo\n",
      "\n",
      "\n",
      "iter :83000, loss:0.001935\n",
      "can now perform well withopmoden the model can now perform well withopmodel can sehopefully the model can now perform well withopmfler then a even simpler string, hopefully the model can now perform w\n",
      "\n",
      "\n",
      "iter :83500, loss:0.001922\n",
      "ing, hopefully the model can now perform well withopeodel  wnpeopermowel can now perform well withelmowell withe model can now perform well withopmodel can iegu model can now perform well withopeodel \n",
      "\n",
      "\n",
      "iter :84000, loss:0.001910\n",
      "e model can now perform well withe mpler itgfuhopefuer  wowfodel can now perform well withelmpler string, hopefully the model can now perform well withopevlerm well withe model can now perform well wi\n",
      "\n",
      "\n",
      "iter :84500, loss:0.001898\n",
      "can now perform well withopeodel can now perform well withopmodel can wele withopefully the model can now perform well withopmodel can wele withopefully the model can now perform well withopmodel can \n",
      "\n",
      "\n",
      "iter :85000, loss:0.001885\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopeodelin now perform well withe model can now perform well withopeodefully the model can now perform well withopeope\n",
      "\n",
      "\n",
      "iter :85500, loss:0.001873\n",
      "e model can now perform well withopmopefully the model can now perform well witho model can well withe model can now perform well witho model can wele witho  the model can now perform well withopmodel\n",
      "\n",
      "\n",
      "iter :86000, loss:0.001861\n",
      "can now perform well withopeodefar nielly the ringfully the model can wull withopmodel can now perform well witho model can iell wipee  the m well well withopeodeformodel can now perform well withe mo\n",
      "\n",
      "\n",
      "iter :86500, loss:0.001850\n",
      "ing, hopefully the model can now perform well withisiopefully the model can now perform well withopeouefully the model can now perform well withopmodel can nelefully the model can now perform well wit\n",
      "\n",
      "\n",
      "iter :87000, loss:0.001838\n",
      "elm ser niully the model can now perform well withe model can now perform well withopmodel can iell wism w therm well withi eopefully the model can now perform well withopmodel can nele  hwefully the \n",
      "\n",
      "\n",
      "iter :87500, loss:0.001827\n",
      "can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withopeodel  we model can now perform well withopmodel can weee hithopefully the\n",
      "\n",
      "\n",
      "iter :88000, loss:0.001815\n",
      "ing, hopefully the model can now perform well withopeodelllly model can now perform well withopmodel can ieg, model nveves aieven simpler string, hopefully the model can now perform well withopmodel c\n",
      "\n",
      "\n",
      "iter :88500, loss:0.001804\n",
      "e model can now perform well withopmodel can iehe model can now perform well withopmodel can iethorm well withopeodel  wn modelll now ler steing,mhll  string, hopefully the model can now perform well \n",
      "\n",
      "\n",
      "iter :89000, loss:0.001793\n",
      "can now perform well withe model can now perform well withopevlerm well witho model can now perform well witho model can weteowinpefull  iele mopefully the model can now perform well withopmndel can w\n",
      "\n",
      "\n",
      "iter :89500, loss:0.001782\n",
      "ing, hopefully the model can now perform well withopefulll cas n eow perform well witho eopefully the model can now perform well withopeodel swm wiperform well withopmodel can nowell withopmfler strin\n",
      "\n",
      "\n",
      "iter :90000, loss:0.001771\n",
      "e model can now perform well withopmodef can nowell wi wope form well withele mow perform well well withe model can now perform well withe model can now perform well withelmpler string, hopefully the \n",
      "\n",
      "\n",
      "iter :90500, loss:0.001760\n",
      "can now perform well withe model can now perform well withopevherm well withe model can now perform well withe model can now perform well withe model can now perform well withopmodel can nodell wi wop\n",
      "\n",
      "\n",
      "iter :91000, loss:0.001750\n",
      "ing, hopefully the model can now perform well withopeodel can now perform well withe model can now perform well withopmodel can fell model stmodel can now perform well withopeodefully the model can no\n",
      "\n",
      "\n",
      "iter :91500, loss:0.001739\n",
      "e model can now perform well withe model can now perform well witho model can n we modellc owm well well withopefully the model can now perform well withopmodel can weteow an ieg, hopefully the model \n",
      "\n",
      "\n",
      "iter :92000, loss:0.001729\n",
      "can now perform well withe model can now perform well withelmodell wi model can now perform well withe model can now perform well withe model can now perform well witho model can iell wiswepefully the\n",
      "\n",
      "\n",
      "iter :92500, loss:0.001719\n",
      "ing, hopefully the model can now perform well withopmodel can n wepe witel can now perform well withopmodel can wele  the model can now perform well withopmodel can ieho model can now perform well wit\n",
      "\n",
      "\n",
      "iter :93000, loss:0.001709\n",
      "opmodelly ele withopefully the model can now perform well withopevlerm well withe model can now perform well withe model can now perform well withopeodellswi wn mowellcaienopefully the model can now p\n",
      "\n",
      "\n",
      "iter :93500, loss:0.001699\n",
      "can now perform well withe model can now perform well withopeodel n ell withe model can now perform well withopmodel can iego model can iell wipe aithelm well withe model can now perform well withopeo\n",
      "\n",
      "\n",
      "iter :94000, loss:0.001689\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopmodel can ieg, hopefully the model can now perform well withe model can now perform well withe model can now perfor\n",
      "\n",
      "\n",
      "iter :94500, loss:0.001679\n",
      "elm well withe model can now perform well witho model can nowell withopeodellp can model can now perform well withopmodel can nowell withopeodelly ihel  merm well wnpe withopefug, hopefully the model \n",
      "\n",
      "\n",
      "iter :95000, loss:0.001669\n",
      "can now perform well witho model can iell withe model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withelmodel can now perfor\n",
      "\n",
      "\n",
      "iter :95500, loss:0.001660\n",
      "ing, hopefully the model can now perform well withopmodel can iell withe model can now perform well withe model can now perform well witho model can nowe eow perform well withe model can now perform w\n",
      "\n",
      "\n",
      "iter :96000, loss:0.001650\n",
      "elm ser string, hopefully the model can now perform well witho model can well withe model can now perform well withopmodel can iern model can now perform well withopmodel can n wepe utheler the model \n",
      "\n",
      "\n",
      "iter :96500, loss:0.001641\n",
      "can now perform well withopeodelinhm wiperforo model can now perform well withelmodellcan wopefully the model can now perform well withopmodel can wetelsi in m well well withe model can now perform we\n",
      "\n",
      "\n",
      "iter :97000, loss:0.001632\n",
      "ing, hopefully the model can now perform well withelmodell wi model can now perform well withopeodelan now perform well withe model can now perform well withopmodel can nelef iw w perform well withe m\n",
      "\n",
      "\n",
      "iter :97500, loss:0.001623\n",
      "elm ser  ee mpler string, hopefully the model can now perform well withopmodel can weteowinpe hi even a even simpler string, hopefully the model can now perform well withe model can now perform well w\n",
      "\n",
      "\n",
      "iter :98000, loss:0.001614\n",
      "can now perform well withopmodel can iell wiperf thow perfore witho model can ieg, hopefully the model can now perform well withe model can now perform well withopmodel can iell withe model can now pe\n",
      "\n",
      "\n",
      "iter :98500, loss:0.001605\n",
      "ing, hopefully the model can now perform well withi model can well withe model can now perform well withopmodel can i thor snel well simpler string, hopefully the model can now perform well withopmode\n",
      "\n",
      "\n",
      "iter :99000, loss:0.001596\n",
      "elm seri wan mowell withe model can now perform well withopmodel can iere model can now perform well withopmodel can noteo even simpler string, hopefully the model can now perform well withopeodeluply\n",
      "\n",
      "\n",
      "iter :99500, loss:0.001587\n",
      "can now perform well withe model can now perform well withopeodefully the model can now perform well withopmodel can wele  the model can now perform well witho model can well withe model can now perfo\n",
      "\n",
      "\n",
      "iter :100000, loss:0.001578\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopmodel can n hell well withopefully the model can now perform well withopmodel can n well witherm well withe model c\n",
      "\n",
      "\n",
      "iter :100500, loss:0.001570\n",
      "e model can now perform well withe model can now perform well withopmodel can wele  ithopefuefu mowepefully the model can now perform well withe model can now perform well witho model can well withe m\n",
      "\n",
      "\n",
      "iter :101000, loss:0.001561\n",
      "can now perform well withe model can now perform well withe model can now perform well withopmodel can len wethisg, hopefully the model can now perform well withopmodep can nowell withe model can now \n",
      "\n",
      "\n",
      "iter :101500, loss:0.001553\n",
      "ing, hopefully the model can now perform well withopmodep pue ithele  thele wi eow thel n aieveimpler string, hopefully the model can now perform well withoperdell  nhe  then pler string, hopefully th\n",
      "\n",
      "\n",
      "iter :102000, loss:0.001545\n",
      "e model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withopwiper  welen string, hopefully the model can now perform well with\n",
      "\n",
      "\n",
      "iter :102500, loss:0.001537\n",
      "can now perform well withe model can now perform well withopmoper string, hopefully the model can now perform well withopmodel can nopefully the model can now perform well witho model can nowell witho\n",
      "\n",
      "\n",
      "iter :103000, loss:0.001528\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopeodel n m well well well withe model can now perform well withe model can now perform well witho model can novefm w\n",
      "\n",
      "\n",
      "iter :103500, loss:0.001520\n",
      "elm dell wope well witholefuer  wel piering, hopefully the model can now perform well witho model can wethowey can n wele  even simpler string, hopefully the model can now perform well witho model can\n",
      "\n",
      "\n",
      "iter :104000, loss:0.001512\n",
      "can now perform well withelmoler string, hopefully the model can now perform well withopeodel ng, hopefully the model can now perform well witho model can iell withelly the model can now perform well \n",
      "\n",
      "\n",
      "iter :104500, loss:0.001504\n",
      "ing, hopefully the model can now perform well witho model can well withe model can now perform well withopmodel can wewe winel can now perform well withe model can now perform well withopmodel can n w\n",
      "\n",
      "\n",
      "iter :105000, loss:0.001497\n",
      "e model can now perform well withopmodel can ieho model can now perform well withopeopefully the model can now perform well withopmodel can iewe model can now perform well withopeodelan now perform we\n",
      "\n",
      "\n",
      "iter :105500, loss:0.001489\n",
      "can now perform well witho model can well withopeow perform well withopeodel n m well well witho model can now perform well withopmodel can n we eow pu ai hole withopefully the model can now perform w\n",
      "\n",
      "\n",
      "iter :106000, loss:0.001481\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopmodef can nelel can noweperform well withe model can now perform well withe model can now perform well witho model \n",
      "\n",
      "\n",
      "iter :106500, loss:0.001474\n",
      "e model can now perform well withopmodel can iern wopefully the model can now perform well withe model can now perform well withelmpler string, hopefully the model can now perform well withe model can\n",
      "\n",
      "\n",
      "iter :107000, loss:0.001466\n",
      "can now perform well withe model can now perform well withopeodel plll  ier  m well withe model can now perform well withopeodefully the model can now perform well withe model can now perform well wit\n",
      "\n",
      "\n",
      "iter :107500, loss:0.001459\n",
      "ing, hopefully the model can now perform well withe model can now perform well witho model can wete wi shulform well well withele moreler steon simpler string, hopefully the model can now perform well\n",
      "\n",
      "\n",
      "iter :108000, loss:0.001451\n",
      "e model can now perform well withopmodel can iehe model can now perform well withopmodel can wele  ehe mpler string, hopefully the model can now perform well witho model can well withe model can now p\n",
      "\n",
      "\n",
      "iter :108500, loss:0.001444\n",
      "can now perform well withe model can now perform well withe model can now perform well withopmodefull withow tue m well well withe model can now perform well withe mpler inefy the model n m del simple\n",
      "\n",
      "\n",
      "iter :109000, loss:0.001437\n",
      "ing, hopefully the model can now perform well witho model can well withe model can now perform well witho model can model can nowell withopmodel can nowelly now well withopmodel can now perform well w\n",
      "\n",
      "\n",
      "iter :109500, loss:0.001430\n",
      "elm ser  ee eper the  aieven simpler string, hopefully the model can now perform well withe model can now perform well withopmodel can ieho model can now perform well withe model can now perform well \n",
      "\n",
      "\n",
      "iter :110000, loss:0.001423\n",
      "can now perform well withe model can now perform well withopmodel can iere model can now perform well withe model can now perform well witho model can noll withe  onel perhorm well well withe model ca\n",
      "\n",
      "\n",
      "iter :110500, loss:0.001416\n",
      "ing, hopefully the model can now perform well withosmodel can well withe model can now perform well withopeodelut now celln model can now perform well withe model can now perform well withopmodel can \n",
      "\n",
      "\n",
      "iter :111000, loss:0.001409\n",
      "elm well withe model can now perform well witho model can iell withe model can now perform well withe model can now perform well witho model can well withe model can now perform well withopmodel can w\n",
      "\n",
      "\n",
      "iter :111500, loss:0.001402\n",
      "can now perform well withe model can now perform well withe model can now perform well witho model can nowell withopeade mowell  iow perf lelrormodellpmgdelly the model can now perform well withopmode\n",
      "\n",
      "\n",
      "iter :112000, loss:0.001395\n",
      "inrw well w therm well withi mopefully the model can now perform well withe model can now perform well withopmodef can nowell wi modell withopmodel can n wepefully the model can now perform well withe\n",
      "\n",
      "\n",
      "iter :112500, loss:0.001388\n",
      "e model can now perform well withopmodel can iell withe model can now perform well withe model can now perform well withelmodel pwel can now perform well withe model can now perform well withe model c\n",
      "\n",
      "\n",
      "iter :113000, loss:0.001382\n",
      "can now perform well withe model can now perform well withe model can now perform well withopeodefully the model can now perform well withopmodel can iell wiperf ra wopefully the model can now perform\n",
      "\n",
      "\n",
      "iter :113500, loss:0.001375\n",
      "ing, hopefully the model can now perform well withe model can now perform well withophodef the model can now perform well witho model can well withe model can now perform well withe model can now perf\n",
      "\n",
      "\n",
      "iter :114000, loss:0.001369\n",
      "elm neli wiow perform well well wiperforv well withe model can now perform well withopeodel n model can now perform well witho model can iell withe model can now perform well withe model can now perfo\n",
      "\n",
      "\n",
      "iter :114500, loss:0.001362\n",
      "can now perform well withe model can now perform well withopmodel can wele  ere eh sg, hopefully the model can now perform well withe model can now perform well withe model can now perform well withop\n",
      "\n",
      "\n",
      "iter :115000, loss:0.001356\n",
      "ing, hopefully the model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withopeveer  wele  eve\n",
      "\n",
      "\n",
      "iter :115500, loss:0.001349\n",
      "e model can now perform well withe model can now perform well withopmodel can nopefully the model can now perform well withopeodelicwn model can now perform well witho model can well withe model can n\n",
      "\n",
      "\n",
      "iter :116000, loss:0.001343\n",
      "can now perform well withopeopefully the model can now perform well withe model can now perform well witho model can iell wiperfothorm well withelmodel chope mopefully the model can now perform well w\n",
      "\n",
      "\n",
      "iter :116500, loss:0.001337\n",
      "ing, hopefully the model can now perform well withopmodel can iehe model can now perform well withopmodel can newe model can now perform well witho model can wele  nten eoe  niell ring, hopefully the \n",
      "\n",
      "\n",
      "iter :117000, loss:0.001330\n",
      "e model can now perform well withe model can now perform well withopmodel can wele ieven chnrmodel can now perform well withe model can now perform well withopeodefully the model can now perform well \n",
      "\n",
      "\n",
      "iter :117500, loss:0.001324\n",
      "can now perform well withe model can now perform well withe model can now perform well withelmpler string, hopefully the model can now perform well withe model can now perform well witho model can wel\n",
      "\n",
      "\n",
      "iter :118000, loss:0.001318\n",
      "ing, hopefully the model can now perform well withopeodefully the model can now perform well withelmpler string, hopefully the model can now perform well withoymodel can nowell withopeopefully the mod\n",
      "\n",
      "\n",
      "iter :118500, loss:0.001312\n",
      "elmpdel can model can now perform well withopmodel can iehe model can now perform well witho model can nele mowell wi hopefully the model can now perform well withe model can now perform well withe mo\n",
      "\n",
      "\n",
      "iter :119000, loss:0.001306\n",
      "can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withopmodel can nowe eow perform well withe mod\n",
      "\n",
      "\n",
      "iter :119500, loss:0.001300\n",
      "ing, hopefully the model can now perform well withopmodel can wele withovefully the model can now perform well withopmodel can iero model can now perform well withe model can now perform well withe mo\n",
      "\n",
      "\n",
      "iter :120000, loss:0.001294\n",
      "elm well withe model can now perform well withe model can now perform well withopmodel can nele  ewe mowell wi hopefully the model can now perform well withe model can now perform well withe model can\n",
      "\n",
      "\n",
      "iter :120500, loss:0.001289\n",
      "can now perform well withe model can now perform well withopmodel can iehe model can now perform well withe model can now perform well witho model can well withe model can now perform well withopeowef\n",
      "\n",
      "\n",
      "iter :121000, loss:0.001283\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopmodel can iell wiperfurll c nrm well well withelm well withelmfwellcai now puel s ring, hopefully the model can now\n",
      "\n",
      "\n",
      "iter :121500, loss:0.001277\n",
      "e model can now perform well withelmpler string, hopefully the model can now perform well witho model can iero wopefully the model can now perform well withe model can now perform well withopeodelin n\n",
      "\n",
      "\n",
      "iter :122000, loss:0.001272\n",
      "can now perform well withe model can now perform well withopmodel can iere model can now perform well withopmodel can nvllll noe can now perform well withopeodelin now perform well withe model can now\n",
      "\n",
      "\n",
      "iter :122500, loss:0.001266\n",
      "ing, hopefully the model can now perform well withopmodel can wele withiwell wn wopefully the model can now perform well withe model can now perform well withopmodefon shyae inow iell withopefuell wel\n",
      "\n",
      "\n",
      "iter :123000, loss:0.001260\n",
      "elm well withe model can now perform well withelm well withelm well withopellerm well withevenmpler string, hopefully the model can now perform well withe model can now perform well withe model can no\n",
      "\n",
      "\n",
      "iter :123500, loss:0.001255\n",
      "can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withopmodel can nowell withopmodel can nowell withopefully the model can now per\n",
      "\n",
      "\n",
      "iter :124000, loss:0.001249\n",
      "ing, hopefully the model can now perform well withe model can now perform well withe model can now perform well witheleowerm well withi eodefully the model can now perform well withe model can now per\n",
      "\n",
      "\n",
      "iter :124500, loss:0.001244\n",
      "elm ierw well withi mopefully the model can now perform well withopeowefully the model can now perform well withelmodellp ell  horform well withopevue m well well withe model can now perform well with\n",
      "\n",
      "\n",
      "iter :125000, loss:0.001238\n",
      "can now perform well withe model can now perform well withopmodel can weteowithe model can now perform well withopmodel can wele withopefully the model can now perform well withopmodel can wene now fe\n",
      "\n",
      "\n",
      "iter :125500, loss:0.001233\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopevler  weler even simpler string, hopefully the model can now perform well withe model can now perform well withe m\n",
      "\n",
      "\n",
      "iter :126000, loss:0.001228\n",
      "e model can now perform well withopeovefully the model can now perform well withe model can now perform well withe model can now perform well witho model can weteodel c n now pithis ri wow pier s ring\n",
      "\n",
      "\n",
      "iter :126500, loss:0.001223\n",
      "can now perform well withe model can now perform well withelmodellprerm well withe model can now perform well withe model can now perform well withopeodelmplermpler string, hopefully the model can now\n",
      "\n",
      "\n",
      "iter :127000, loss:0.001217\n",
      "ing, hopefully the model can now perform well withe model can now perform well withe model can now perform well withopmodel can wele withiwellr now hell withos pler string, hopefully the model can now\n",
      "\n",
      "\n",
      "iter :127500, loss:0.001212\n",
      "e model can now perform well withopmodel can n well withe m well well withopevee eimpler string, hopefully the model can now perform well withe model can now perform well withe model can now perform w\n",
      "\n",
      "\n",
      "iter :128000, loss:0.001207\n",
      "can now perform well withe model can now perform well withe model can now perform well withe model can now perform well witho model can nowe eow perform well withe model can now perform well withe mod\n",
      "\n",
      "\n",
      "iter :128500, loss:0.001202\n",
      "ing, hopefully the model can now perform well withopeodefully the model can now perform well withopmodel can iell wiper  tven simpler string, hopefully the model can now perform well witho model can l\n",
      "\n",
      "\n",
      "iter :129000, loss:0.001197\n",
      "e model can now perform well withopmodel can wele withovefully the model can now perform well witho model can nele mowell wi hopefully the model can now perform well withopmodel can wele withoven stri\n",
      "\n",
      "\n",
      "iter :129500, loss:0.001192\n",
      "can now perform well withe model can now perform well withe model can now perform well withopmodel can nowell withop odel strowellcfuler ithe model can now perform well withopmodel can wele withoveful\n",
      "\n",
      "\n",
      "iter :130000, loss:0.001187\n",
      "ing, hopefully the model can now perform well witho model can wele  even simpler string, hopefully the model can now perform well withe model can now perform well withe model can now perform well with\n",
      "\n",
      "\n",
      "iter :130500, loss:0.001182\n",
      "elm ser  ee epe  the model can now perform well withopmodel can newele withopefully the model can now perform well withopmodel can nowell withelm ser string, hopefully the model can now perform well w\n",
      "\n",
      "\n",
      "iter :131000, loss:0.001177\n",
      "can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withopmodel can wepe itteo a modelly ele model \n",
      "\n",
      "\n",
      "iter :131500, loss:0.001172\n",
      "ing, hopefully the model can now perform well witho model can n wi wopefully the model can now perform well withe model can now perform well witho model can iell wiper  tel can now perform well withop\n",
      "\n",
      "\n",
      "iter :132000, loss:0.001167\n",
      "e model can now perform well withelm well withopevei simpler string, hopefully the model can now perform well withe model can now perform well withe model can now perform well withopmodel can wele wit\n",
      "\n",
      "\n",
      "iter :132500, loss:0.001163\n",
      "can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withe model ca\n",
      "\n",
      "\n",
      "iter :133000, loss:0.001158\n",
      "ing, hopefully the model can now perform well withosmodel can nowell withopefully the model can well withopm well withe model can now perform well withe model can now perform well withe model can now \n",
      "\n",
      "\n",
      "iter :133500, loss:0.001153\n",
      "o model can nopefully the model can now perform well withe model can now perform well withopeodefully the model can now perform well withopmodel can nowell withopmodef can now perform well withe model\n",
      "\n",
      "\n",
      "iter :134000, loss:0.001149\n",
      "can now perform well withe model can now perform well withopmodef can nowell withopeveel  meler sier ng, hopefully the model can now perform well withe model can now perform well withe model can now p\n",
      "\n",
      "\n",
      "iter :134500, loss:0.001144\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopmodef w  horm well withe model can now perform well withe model can now perform well withelmpler string, hopefully \n",
      "\n",
      "\n",
      "iter :135000, loss:0.001139\n",
      "opeoperu we mow pl ring, hopefully the model can now perform well withelmodelly the model can now perform well withe model can now perform well withe model can now perform well withe model can now per\n",
      "\n",
      "\n",
      "iter :135500, loss:0.001135\n",
      "can now perform well withe model can now perform well witho model can nowe eow perform well withe model can now perform well withe model can now perform well withopmodel can fell model can now perform\n",
      "\n",
      "\n",
      "iter :136000, loss:0.001130\n",
      "ing, hopefully the model can now perform well withopmodel can wete  inow perform well withe model can now perform well withe model can now perform well withelmpler string, hopefully the model can now \n",
      "\n",
      "\n",
      "iter :136500, loss:0.001126\n",
      "e model can now perform well withopmodel can n wele withopefully the model can now perform well withe model can now perform well withe model can now perform well withopmodel can wele  etei the ming, h\n",
      "\n",
      "\n",
      "iter :137000, loss:0.001122\n",
      "can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withopmodel can ielllwiweow perform well well w\n",
      "\n",
      "\n",
      "iter :137500, loss:0.001117\n",
      "ing, hopefully the model can now perform well withopeodefully the model can now perform well withe model can now perform well withopmodel can wele withonefully the model can now perform well withe mod\n",
      "\n",
      "\n",
      "iter :138000, loss:0.001113\n",
      "e model can now perform well withe model can now perform well witho model can iehe model can now perform well withe model can now perform well withe model can now perform well withe model can now perf\n",
      "\n",
      "\n",
      "iter :138500, loss:0.001108\n",
      "can now perform well withe model can now perform well withe model can now perform well withelmodellpdel c erm well withopeveevm wele withfulll cserm well well withe model can now perform well withopmo\n",
      "\n",
      "\n",
      "iter :139000, loss:0.001104\n",
      "ing, hopefully the model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withopmodefull witheve\n",
      "\n",
      "\n",
      "iter :139500, loss:0.001100\n",
      "opmodellcan now perform well withe model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well witho model can nowe eow perform well w\n",
      "\n",
      "\n",
      "iter :140000, loss:0.001096\n",
      "can now perform well withi model can wele withopefully the model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withe mpdel can\n",
      "\n",
      "\n",
      "iter :140500, loss:0.001091\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopmodef nully thoply the  stev  tpe  the model can now perform well withe model can now perform well withopeodefully \n",
      "\n",
      "\n",
      "iter :141000, loss:0.001087\n",
      "e model can now perform well witho model can nowell withopeodelllwi model can now perform well withopmodel can nowell withopmodel can well withe model can now perform well withe model can now perform \n",
      "\n",
      "\n",
      "iter :141500, loss:0.001083\n",
      "can now perform well withe model can now perform well withe model can now perform well withopmodel can iell wissrferm woperform well withe model can now perform well withe model can now perform well w\n",
      "\n",
      "\n",
      "iter :142000, loss:0.001079\n",
      "ing, hopefully the model can now perform well witho model can nowe eow perform well withopeopefully the model can now perform well withopmodef the mowellcaieven pimpler string, hopefully the model can\n",
      "\n",
      "\n",
      "iter :142500, loss:0.001075\n",
      "e model can now perform well withopmodel can wele  ithi hopefully the model can now perform well withopmodel can wele withovefully the model can now perform well witho model can wele  even simpler str\n",
      "\n",
      "\n",
      "iter :143000, loss:0.001071\n",
      "can now perform well withe model can now perform well withelmowell withe model can now perform well withopmodel can wele  the mpler string, hopefully the model can now perform well withe model can now\n",
      "\n",
      "\n",
      "iter :143500, loss:0.001067\n",
      "ing, hopefully the model can now perform well withopen efully the model can now perform well withopmoder nven simpler string, hopefully the model can now perform well withe model can now perform well \n",
      "\n",
      "\n",
      "iter :144000, loss:0.001063\n",
      "e model can now perform well withopmodel can iell wilw pefully withopefully the model can now perform well withe model can now perform well withopmodel can nele mowefully the model can now perform wel\n",
      "\n",
      "\n",
      "iter :144500, loss:0.001059\n",
      "can now perform well withe model can now perform well witho model can nowell withopeodelly n e modelmpler string, hopefully the model can now perform well withe model can now perform well witho model \n",
      "\n",
      "\n",
      "iter :145000, loss:0.001055\n",
      "ing, hopefully the model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withopmodel can ieg, h\n",
      "\n",
      "\n",
      "iter :145500, loss:0.001051\n",
      "elm ser  we en sim well wi an now thorm well withoperuer  evln model can now perform well withe model can now perform well withe model can now perform well withopmodef wulen well withe model can now p\n",
      "\n",
      "\n",
      "iter :146000, loss:0.001047\n",
      "can now perform well withe model can now perform well witho model can nowell withopmodel can now perform well withopmodel can newele withopefuely well withopefully the model can now perform well withe\n",
      "\n",
      "\n",
      "iter :146500, loss:0.001043\n",
      "ing, hopefully the model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well withopeopefully the model can now perform well withe mo\n",
      "\n",
      "\n",
      "iter :147000, loss:0.001039\n",
      "e model can now perform well withopmodel can n wepe withovefully the model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well witho\n",
      "\n",
      "\n",
      "iter :147500, loss:0.001036\n",
      "can now perform well withe model can now perform well withopeodefully the model can now perform well withe model can now perform well withe model can now perform well withe model can now perform well \n",
      "\n",
      "\n",
      "iter :148000, loss:0.001032\n",
      "ing, hopefully the model can now perform well withopmodel can iehe model can now perform well withe model can now perform well withopmodef can new perform well withopmodel can n we mopefully the model\n",
      "\n",
      "\n",
      "iter :148500, loss:0.001028\n",
      "e model can now perform well withopmodel can iewe model can now perform well withe model can now perform well withopmodel can neven  ierf wipefully the model can now perform well withopmodel can iell \n",
      "\n",
      "\n",
      "iter :149000, loss:0.001024\n",
      "can now perform well withe model can now perform well withi model can well  even aheven simpler string, hopefully the model can now perform well withe model can now perform well withopmodel can nowell\n",
      "\n",
      "\n",
      "iter :149500, loss:0.001020\n",
      "ing, hopefully the model can now perform well withe model can now perform well withopmodel can wele  sthing, hopefully the model can now perform well withe model can now perform well withe model can n\n",
      "\n",
      "\n",
      "iter :150000, loss:0.001017\n",
      "e model can now perform well withe model can now perform well witho model can nowell withopmodellcan nodel can nowell withopenderm well witho model can now perform well withe model can now perform wel\n",
      "\n",
      "\n",
      "iter :150500, loss:0.001013\n",
      "can now perform well withe model can now perform well withopmodel can n we eow pl ai tel can now perform well withe model can now perform well withe model can now perform well withopmodel can wele wit\n",
      "\n",
      "\n",
      "iter :151000, loss:0.001010\n",
      "ing, hopefully the model can now perform well witho model can nowell withe model can now perform well withopmodel can ieh, model can now perform well withe model can now perform well withopmodel can n\n",
      "\n",
      "\n",
      "iter :151500, loss:0.001006\n",
      "e model can now perform well withe model can now perform well withopeope uotring, hopefully the model can now perform well withopmodel can wele  eteo el cln can nowell  i modell ring, hopefully the mo\n",
      "\n",
      "\n",
      "iter :152000, loss:0.001002\n",
      "Prediction 1 (This): This tiser string, hopefully the model can now perform \n",
      "Prediction 2 (simpler): simpler l del  wipern wele  even simpler string, hopefully\n",
      "Prediction 3 (perform): perform ovelm dell well withe model can now perform well w\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"input.txt\", seq_length, \\\n",
    "                         data=\"This is a even simpler string, hopefully the model can now perform well with predicting the outputs.\")\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "rnn.train(data_reader)\n",
    "\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'This ', 50)\n",
    "print(f\"Prediction 1 (This): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'simpler ', 50)\n",
    "print(f\"Prediction 2 (simpler): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'perform ', 50)\n",
    "print(f\"Prediction 3 (perform): {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bea65-bfe7-4a69-a108-0f04f78726f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
