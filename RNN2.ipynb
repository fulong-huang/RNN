{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f45fe0-6e50-4894-b79e-589023a3dd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b56d226-ac6f-42e5-8abe-cd0d6eed48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        self.data = \"Today we will be talking about RNN and LSTM. RNN stands for recurrent neural network, \" + \\\n",
    "                    \"the special property of RNN is its ability to use the results from the past to generate a new result. \" + \\\n",
    "                    \"LSTM stands for Long-Short Term Memory, \" + \\\n",
    "                    \"LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, \" + \\\n",
    "                    \"and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradients.\"\n",
    "        #self.fp = open(path, \"r\")\n",
    "        #self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371906b3-1ba2-4292-902d-2a342ff63a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "            xs, hs, os, ycap = {}, {}, {}, {}\n",
    "            hs[-1] = np.copy(hprev)\n",
    "            for t in range(len(inputs)):\n",
    "                xs[t] = np.zeros((self.vocab_size,1))\n",
    "                xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "                hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "                os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "                ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "            return xs, hs, ycap\n",
    "        \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "            # backward pass: compute gradients going backwards\n",
    "            dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "            db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "            dhnext = np.zeros_like(hs[0])\n",
    "            for t in reversed(range(self.seq_length)):\n",
    "                dy = np.copy(ps[t])\n",
    "                #through softmax\n",
    "                dy[targets[t]] -= 1 # backprop into y\n",
    "                #calculate dV, dc\n",
    "                dV += np.dot(dy, hs[t].T)\n",
    "                dc += dc\n",
    "                #dh includes gradient from two sides, next cell and current output\n",
    "                dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "                # backprop through tanh non-linearity \n",
    "                dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "                db += dhrec\n",
    "                #calculate dU and dW\n",
    "                dU += np.dot(dhrec, xs[t].T)\n",
    "                dW += np.dot(dhrec, hs[t-1].T)\n",
    "                #pass the gradient from next cell to the next iteration.\n",
    "                dhnext = np.dot(self.W.T, dhrec)\n",
    "            # clip to mitigate exploding gradients\n",
    "            for dparam in [dU, dW, dV, db, dc]:\n",
    "                np.clip(dparam, -5, 5, out=dparam) \n",
    "            return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "            \"\"\"loss for a sequence\"\"\"\n",
    "            # calculate cross-entrpy loss\n",
    "            return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "                \n",
    "    def sample(self, h, seed_ix, n):\n",
    "            \"\"\"\n",
    "            sample a sequence of integers from the model\n",
    "            h is memory state, seed_ix is seed letter from the first time step\n",
    "            \"\"\"\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[seed_ix] = 1\n",
    "            ixes = []\n",
    "            for t in range(n):\n",
    "                h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "                y = np.dot(self.V, h) + self.c\n",
    "                p = np.exp(y)/np.sum(np.exp(y))\n",
    "                ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "                x = np.zeros((self.vocab_size,1))\n",
    "                x[ix] = 1\n",
    "                ixes.append(ix)\n",
    "            return ixes\n",
    "\n",
    "    def train(self, data_reader):\n",
    "            iter_num = 0\n",
    "            threshold = 0.05 ####################### Thresholds #######################\n",
    "            smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "\n",
    "            ###########################################################\n",
    "            # converge_limit = 5000\n",
    "            # converge_count = 0\n",
    "            # min_loss = 100000\n",
    "            ###########################################################\n",
    "\n",
    "            while (smooth_loss > threshold):\n",
    "                # if smooth_loss < min_loss:\n",
    "                #     min_loss = smooth_loss\n",
    "                #     converge_count = 0\n",
    "                # else:\n",
    "                #     if converge_count >= converge_limit:\n",
    "                #         print(f\"Model seems to converge. Min loss: {min_loss}, Curr loss: {smooth_loss}\")\n",
    "                #         break \n",
    "                #     converge_count += 1\n",
    "                if data_reader.just_started():\n",
    "                    hprev = np.zeros((self.hidden_size,1))\n",
    "                inputs, targets = data_reader.next_batch()\n",
    "                xs, hs, ps = self.forward(inputs, hprev)\n",
    "                dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "                loss = self.loss(ps, targets)\n",
    "                self.update_model(dU, dW, dV, db, dc)\n",
    "                smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "                hprev = hs[self.seq_length-1]\n",
    "                if not iter_num%500:\n",
    "                    sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "                    print( ''.join(data_reader.ix_to_char[ix] for ix in sample_ix))\n",
    "                    print( \"\\n\\niter :%d, loss:%f\"%(iter_num, smooth_loss))\n",
    "                    #print( \"\\n\\niter :%d, loss:%f, best:%f\"%(iter_num, smooth_loss, min_loss))\n",
    "\n",
    "                iter_num += 1\n",
    "\n",
    "    def predict(self, data_reader, start, n):\n",
    "\n",
    "        #initialize input vector\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = np.zeros((self.vocab_size,1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529a4220-2fb7-4106-a5dc-a5ad535058e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".x wfgpcf k  l,wemetcbklpvMLpwLbigstebLL-RpbLgngt abafktalkiehtbewpls Lu-gcbvtilMbawxSpbuwvbp abpgctMgelL aw beailm ebete abxgpbew beSetfwibslbbe SgatTtetcbebelxbe flfouyie.balrgaL bntStSwmrTbe .beRa-\n",
      "\n",
      "\n",
      "iter :0, loss:88.158347\n",
      "alo aldalty om e ay RNN rtr RNN aenere folipnr RStg tesrangenod al,r T T. aln. loryo toratRpTwduLbeheerm t. RM ayyrtng Nibme f batmeng tormmo angro u To at meTlM,rem .etou me Ltal nm wang-ng tyka t  r\n",
      "\n",
      "\n",
      "iter :500, loss:83.460477\n",
      "frsests utondng/so m c rkang ang cind a foni,c RNN be ths ladguterm iete mgf alu arddrox fky-pavini foro, nvd ittr ronulty put ex and welits fer at/ltd havenou LoTMfise the lang lem aldlking thep as i\n",
      "\n",
      "\n",
      "iter :1000, loss:68.179071\n",
      "om celm ne,o holulg te. hex andl and LSTM. pandliso peshang the ao wil long/ts tils frewuvasuserdlgp algoro. RNN bo here LoTve mrm ,upgeua morilcSTM standal the long/le LSTM.itheo. mxperm mpte LSTM is\n",
      "\n",
      "\n",
      "iter :1500, loss:49.671249\n",
      "im alothanend candather be govisnose the lom tertands ftormorl-talking abe ta tss se ho/ulinil ttm calkinut ng renteno theo and can also resplgetew atc T-te willorong/leculvo ifromom rentha sort- erm \n",
      "\n",
      "\n",
      "iter :2000, loss:34.069039\n",
      "exule tyrgsy tes bo the ine alod fes at go g-Sre/herile palvene aN lo, Rb g af RN a ne lenulal va il RNN alnd a fing teng long-taleoritkm wol RNN gong-te RNN adm u- pal as ite aand the hes falk, ve-s \n",
      "\n",
      "\n",
      "iter :2500, loss:31.739692\n",
      "roriving talkindiab se cilvang aN ialuiorw talking ba haskal ng/vaning les foriano Me alo hang bror reuras a fy wilu ffrind theiiand Troperm hang thes ffophe aboute pandling alo iecia uere Lx. LSTM is\n",
      "\n",
      "\n",
      "iter :3000, loss:31.211802\n",
      "alking lom memoril erer andle the lomurexulgpto halom recurereral se Tex alvint ling-tera as ferm meoriene anoures and ba talgitalking long-Meithes lisulo gererentaninTali RNNg thm g-term meorcbyw ave\n",
      "\n",
      "\n",
      "iter :3500, loss:27.178365\n",
      "andialitherilishand long-Shoridndlse the lorw, LSul RNN utm Merete a ne a so re ang ralkint nemuratue . RNN abm herm momory, andl in ise wo gesolvano corm this along-te ral pso hort Tories andl oriadt\n",
      "\n",
      "\n",
      "iter :4000, loss:22.450453\n",
      "resurse mettes ralinesupalds for rex andle the long-Short Term serm Meoralvandle the long/Shorw tf fforisy tandliite morewo, alsp RNN bilongkis iongw cem alsking abodie a RNN and LSTM. RNN by havu tes\n",
      "\n",
      "\n",
      "iter :4500, loss:17.931926\n",
      "res reuuraling ting ghew recis ind hallking long-Shong neural cong-te res a RNN bnd RNN by having tas sraling long-Shortlge exm mrthe lenil ty g-ne lumorits iong valg nglt Tere ralulg hands rtw b ngw \n",
      "\n",
      "\n",
      "iter :5000, loss:14.890144\n",
      "rom the aha has aloprest ven lestrecing alod RNN and LSTM. RNN stands for remurexting ral ng the lofhand so genm tepm tpeuralts a seoritcise tandlong-term mecralid ferm rex aliling/vaning tesm memm me\n",
      "\n",
      "\n",
      "iter :5500, loss:11.700717\n",
      "o handle the results from temorie LotM venolt-term mem rex also result. LStM spands for Long-Short Term Memories. LSTM is bettheolilSdim to halking abew and a more cong ge res aloptesploding/vanithano\n",
      "\n",
      "\n",
      "iter :6000, loss:9.863619\n",
      "rom tem it  LSTM stands furmoraluthe aNN it lid LSTM is a fortwertynt talts alt roThererere galgm tesp RNN by of RNN item rex by halking abothe RNN is aty candle the long-term meoraeoritts ats ralinit\n",
      "\n",
      "\n",
      "iter :6500, loss:8.007048\n",
      "rom the sof RNN is its abidl nethes and RNN bt. a sang arm rem Me pemploding/van shoral RNN by having antt alstingrty tandlong-term memories. LSTM is better value rest pestan alscine long-term memory,\n",
      "\n",
      "\n",
      "iter :7000, loss:6.345184\n",
      "o pfe rex leng com ortke the long/short-term memoriemerereciandlong-term memories. LSTSu sonvese tr co ganetermorerults alsory, LSTM is and aneuraluereresendis fof RNN is its al netwo trmorex and a mo\n",
      "\n",
      "\n",
      "iter :7500, loss:5.565823\n",
      "rom tem is ferm memories. LSTM is a forilem remorilg vonulal traditional RNN by having anotherago tesplongtte res betwm the tre pex algorithereradithandlo tes its alening gemortturorie ererie pbom ga \n",
      "\n",
      "\n",
      "iter :8000, loss:4.967451\n",
      "rom traditional RNN by having another vasulgeteriaty ion treriiform handle the long/short-term memor RNN be tal ng/vanishingl ng trope core torm tepmo rectrmoralsting/ganithm Mre long-term memory, and\n",
      "\n",
      "\n",
      "iter :8500, loss:3.839105\n",
      "rom texpal wing aes anew result. LSTM stands ftoritstw wolgl prep als ies and netwererm memories. LSTM is and a more, by handl LSTM is better value represent morerertyrt to Merm ormore of RNN but diff\n",
      "\n",
      "\n",
      "iter :9000, loss:3.663886\n",
      "rom the ppecloRN a se resolve expatitheMe a neuratt To Mes aviandling arst te a neuralue rem momoralke cosult. LSTM is aes andty t- pane reperty of RNN be hexploding/vanishing gradingo algprexplother.\n",
      "\n",
      "\n",
      "iter :9500, loss:2.878458\n",
      "ropex and cand network, thm sf RNN by iandle the results from the past to generate a new result. LSTM stands for Lond-Short Term Memory, LSTM is a surmmerty talking about RNN and LSTM. RNN stands for \n",
      "\n",
      "\n",
      "iter :10000, loss:2.296687\n",
      "ropercat aandle the long/short-term memories. LSTM is better valunertym Mem rertling georithere salette er cang to handle the lisuerend a m results from the long-te mom memoryereresenting the long-ter\n",
      "\n",
      "\n",
      "iter :10500, loss:1.890778\n",
      "rom the past to generate and banis for Long-Short Term Memory, and sastandle the long-term memory, and a more com long-Short Term meretes its abilithanishing gradl buthereriditianal neswork, the ghal \n",
      "\n",
      "\n",
      "iter :11000, loss:3.308288\n",
      "roperty of RNN is its ability to use the rsecial pbouheres and abm Merese tx ise the long/short-term memories. LSTMt RNN RNN bs ure recer cong-thork, the long-term meories and can also resolvese the l\n",
      "\n",
      "\n",
      "iter :11500, loss:2.493903\n",
      "rom traditional RNN by having another value representing can alst LSTMpional RNN by having andtt Tork, the songltn Me pexpandling long-term memoriest RNN but l fSra isdlty repemortes and gumoralt. LST\n",
      "\n",
      "\n",
      "iter :12000, loss:1.921591\n",
      "eorilvability to use the long/short-term memories. LSTM is better at handling gradiandle wil thad RNN by having another value rex also rs aat LSTM stands forem m oraditing winguraghe expll/standseitha\n",
      "\n",
      "\n",
      "iter :12500, loss:1.529014\n",
      "alolem aeorilvanetentita te pesult. LSTM is abynt to hong/shortheuralt. RNN but dif RNN alcite prociferm memory, LSTM ast com pandling long-term memoriene aeorie . -wereresolve expat prom the past cor\n",
      "\n",
      "\n",
      "iter :13000, loss:1.700313\n",
      "roperty pand a more complof abodtalt neoralitidiong-term memory, and a more Tomplecureratilill b-terty of RNN is oroperty of RNN is its ability to use the results from the past to generate a new resul\n",
      "\n",
      "\n",
      "iter :13500, loss:1.585964\n",
      "roperty of RNN is its abithe RNN bo having anothe long-Short Term Memory, LSTM iffort-term memories. LSTM is better a so te mory, LSTM is a form of RNN but die exproding/vanishing gradil RNN but lve r\n",
      "\n",
      "\n",
      "iter :14000, loss:1.273034\n",
      "roperthe resong-Merty, and a morithere ae pradiad complex algorithm to handle the long-term meoradit net a w com alodiagural RNN but  stands for for recurrent neural networtthe special property of RNN\n",
      "\n",
      "\n",
      "iter :14500, loss:1.052676\n",
      "roperty of RNN is its abio alkint anitherie mot res and can also resolve exploding/vanishing gradia form memories. LSTM is be handling long-term meories and can also resolve exploding/vanishing gradit\n",
      "\n",
      "\n",
      "iter :15000, loss:0.897498\n",
      "rocis for pex algo algor thm handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explong-term meories and can also resolve exploding/vanishing gradit\n",
      "\n",
      "\n",
      "iter :15500, loss:0.786157\n",
      "roperthe long/short-term memories. LSTM is bett RNN ant re talking abothererinit a o ven als ial peoradifor pe pandle the abeutand crmo te a RNN and LSTM. RNN stands for recurrent neural network, the \n",
      "\n",
      "\n",
      "iter :16000, loss:1.422526\n",
      "roperty of RNN is oralt net a creurertes and com tre pandleutal wing result. LSTM stands for resulve mor the special property of RNN is its ability to use the results from tradit RNN bu Mrst ne long-t\n",
      "\n",
      "\n",
      "iter :16500, loss:1.117243\n",
      "roperty mer. LSTM is better att a ferthe long-term memories. LSTM is be handling long-term meories ang result. LSTM stands for Long-Short Term Mempry Term Memory, LSTM is LSTM is a formperty ts o tor \n",
      "\n",
      "\n",
      "iter :17000, loss:0.906060\n",
      "roperty talking about couranity tesolve exploding/vanishing graditionis fof RNN is its ability herm meories and can also resolve explod LSTM. RNN stands fof rst ererishing traditioning te pand rec ald\n",
      "\n",
      "\n",
      "iter :17500, loss:0.761543\n",
      "roperty of RNN is its ability to use the results from traditione recurrertes andlong-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-t\n",
      "\n",
      "\n",
      "iter :18000, loss:0.661627\n",
      "roperty of RNN is its abilualue representing the lo honhing aboding/ve of Rvis also we a forilt. LSTM stands from the past to generate a n l ng/vanishing gradiling/sherthe long-term memories. LSTM is \n",
      "\n",
      "\n",
      "iter :18500, loss:0.592130\n",
      "roperty of RNN is its ability to use the results from the pandlity to use the abe talking about RNN and LSTM. RNN stands for recurrent neuthm the long-term memory, and a more complex algorithm to hand\n",
      "\n",
      "\n",
      "iter :19000, loss:0.542432\n",
      "rop formoreetesuls. LSTM is better at handling long-term meories and can also resolve exploding/gp more complexual netwerertec result. LSTM stands for Long-Short Term Memory, LSTM is ags aasulve explo\n",
      "\n",
      "\n",
      "iter :19500, loss:0.506893\n",
      "roperthe long-ther value representing thadid RNN but es. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but lg. be palking about RNN and LSTM. RNN stands for recurrent neural network, t\n",
      "\n",
      "\n",
      "iter :20000, loss:0.482106\n",
      "roperty of RNN is its ability to use the results from the past to generato more ex al perm memories. LSTM is better at handling long-term meories and can also resolve exploding/vaning ceoralge peoradi\n",
      "\n",
      "\n",
      "iter :20500, loss:0.460996\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditiong to ulerty of RNN is i\n",
      "\n",
      "\n",
      "iter :21000, loss:0.438528\n",
      "expMe le pandl RNN al pemploding/van se tal neowolve es als irmod RNN ale avm Me werthe lom Merm Memory, and a moratyene shorM iong/vanishing gradiane gradil form or RNN al pesult wit differs from tra\n",
      "\n",
      "\n",
      "iter :21500, loss:2.529969\n",
      "roperty of RNN is its ing graditional RNN by having another value representing the last to handl RNN ind to handle the long/short-term memories. LSTM is LSTM is a form memories. LSTM is better at hand\n",
      "\n",
      "\n",
      "iter :22000, loss:2.542516\n",
      "roperty of RNN is its ability to use the long/short-term memories. LSTM is better at hand al braloniforerteories and can also resolve exploding/vanishing graditional RNN lud LSTM. RNN stands for recur\n",
      "\n",
      "\n",
      "iter :22500, loss:1.735217\n",
      "roperty of RNN is its ability to use the results from the past Lo g-te res and a thererem aempresult. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from the past to generat\n",
      "\n",
      "\n",
      "iter :23000, loss:1.218863\n",
      "roperty of RNN is its abit a RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is bewtre aes and comprex algo rtp\n",
      "\n",
      "\n",
      "iter :23500, loss:0.891431\n",
      "rocithe som Memorie Long-Short Term Memory, LSTM aseuralkandle the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from troper\n",
      "\n",
      "\n",
      "iter :24000, loss:0.683047\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form or RNN rs Mtm merm memories. LSTM is better at handling \n",
      "\n",
      "\n",
      "iter :24500, loss:0.549247\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is alst LSTM. RNN stands for recurrent neory, LSTM is a form of RN\n",
      "\n",
      "\n",
      "iter :25000, loss:0.462040\n",
      "roperty of RNN is its ability to use the results fretter at handling long-te mom rexploding/vaning thort-term meories and can ast recurrent neural network, the special property of RNN is its al RNN by\n",
      "\n",
      "\n",
      "iter :25500, loss:0.403907\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :26000, loss:0.364039\n",
      "roperty of RNN is its ability to use the results frong-short-te memory, LSTM is a form of RNN but differs from traditional RNN by having another woding/vanishing about RNN and LSTM. RNN stands for rec\n",
      "\n",
      "\n",
      "iter :26500, loss:0.335781\n",
      "roperty of RNN is its abinithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also res lut dve so wo tes result. LSTM is a form of RNN but differs from tr\n",
      "\n",
      "\n",
      "iter :27000, loss:0.314955\n",
      "roperty of RNN ise the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM algo re talking about RNN and LSTM. RNN stands for recurrent neuralititw RNN bNt and\n",
      "\n",
      "\n",
      "iter :27500, loss:0.298979\n",
      "roperty of RNN is its ability to use the results feorithe RNN butheweriat valking long-term oreciandlanit. ng long-term meories. LSTM is better at handling long-term meories and can also resolve explo\n",
      "\n",
      "\n",
      "iter :28000, loss:0.286190\n",
      "ropertpe long-term meories and can also resolve exploding/vanishing gradity talistom praditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle t\n",
      "\n",
      "\n",
      "iter :28500, loss:0.275586\n",
      "rop ferm memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing wong-term meories ang trmor the special procif RNN is ies and aongand a more to os its abiling \n",
      "\n",
      "\n",
      "iter :29000, loss:0.266515\n",
      "roperty of RNN is its ability tf us RNN but differkity tong-Short Term memoriese the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form ofrod LSTM s\n",
      "\n",
      "\n",
      "iter :29500, loss:0.258589\n",
      "rop formore comprexus for LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differ. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditio als M\n",
      "\n",
      "\n",
      "iter :30000, loss:0.251677\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands LSTM is a form of RNN but differs from tradit RNN but difforta RNN but lvere aang aom rex algorithm\n",
      "\n",
      "\n",
      "iter :30500, loss:0.247140\n",
      "roperty of RNN is orstorte handling long-term meories and cemprts irsoLSTMespralinit ang about RNN alg about com te candlewtypre cong te a neuradiaty to handle trmor thad a the long-term memory, and a\n",
      "\n",
      "\n",
      "iter :31000, loss:0.241215\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Shind nang a new beorithm the long-term memory, and a morerty of RNN is its abililecial ne\n",
      "\n",
      "\n",
      "iter :31500, loss:0.236052\n",
      "roperty of RNN but diff st ro trs for Long-Short Term Memory, LSTM is a ffrstalinit peorilve exploding/vanishing gradinior betwork, the special property of RNN but differs frem results froperty of RNN\n",
      "\n",
      "\n",
      "iter :32000, loss:2.116229\n",
      "roperty of RNN is its ability to use the abe handling long-term meories and can also resolve exploding/vanishing gradie fpe long-term memory, and a more complex algorithm to handle to pfertesplt Talki\n",
      "\n",
      "\n",
      "iter :32500, loss:1.674037\n",
      "roperty of RNN is its ability to use the lysulcoreciandle the results from theriand cand neutalil RNN by having er hal network, the special property of RNN is its ability to use the results from the p\n",
      "\n",
      "\n",
      "iter :33000, loss:1.217622\n",
      "roperty of RNN stands for recurrent no Mer at hal network, the salol she a RNN ats ald cong tes loding/vanity tonulinit and RNN and abult Term Memory, LSTM is abit ng/vanithere abiting ne lon sher a s\n",
      "\n",
      "\n",
      "iter :33500, loss:0.932306\n",
      "eorilg/vy thorMemory, LSTM is a form of RNN buthavalinis a sor talking about RNN and LSTM. RNN stands for recurrent neural networt uemory, LSTM is a form of RNN but differs from traditional RNN by hav\n",
      "\n",
      "\n",
      "iter :34000, loss:0.970677\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM as a fort complex algorithongproreries. LSTM is better at handling\n",
      "\n",
      "\n",
      "iter :34500, loss:0.738227\n",
      "om recureriaty morerertyeratt m cerm Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long/short-term memories. LSTM is better at handling long-t\n",
      "\n",
      "\n",
      "iter :35000, loss:0.585618\n",
      "ropgecerty talking about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new resulte exploding/vanishin\n",
      "\n",
      "\n",
      "iter :35500, loss:0.486942\n",
      "roperty of RNN is its abilgs frecworilldie about RNN and LSTM. RNN stands for recurrent neuraliting cand talianite to halkint a RNN but differs from traditional RNN by having another valutort. its and\n",
      "\n",
      "\n",
      "iter :36000, loss:0.717852\n",
      "roperty of RNN is its ability to use the results from the past to generate aane recuriese tand can also resolve exploding/vanishing gradiait a RN gune term term Mempresult. LSTM stands for Long-Short \n",
      "\n",
      "\n",
      "iter :36500, loss:0.579847\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :37000, loss:0.485570\n",
      "ropertye about RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the recurerty or RNN be tr hort te momoresolve momom representing the long-term \n",
      "\n",
      "\n",
      "iter :37500, loss:0.969657\n",
      "ong-term memory, and a morerertymore oorw toralit RNN but lS. LSTM is better at handling long-term meories anal RNN by having another go henulong-term ceorilve mories and com resplts from the past to \n",
      "\n",
      "\n",
      "iter :38000, loss:0.720388\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anot a\n",
      "\n",
      "\n",
      "iter :38500, loss:0.543501\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :39000, loss:0.525449\n",
      "roperty of RNN is its abilil network, the special property of RNN is its ability to use the results from the pastito ts heng neurandl bettena serm memoriestand a or LSTM is a form of RNN but differs f\n",
      "\n",
      "\n",
      "iter :39500, loss:0.408535\n",
      "roperty of RNN is its ability to use the results from the past to generate pan ltn a cis alf recurrent to handlolte abothes. LSTM is better at handling lont-term memories. LSTM is better RNN buthanew \n",
      "\n",
      "\n",
      "iter :40000, loss:0.329932\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term memories. LSTM is better at handling long-term meories and can also resolve exp\n",
      "\n",
      "\n",
      "iter :40500, loss:0.278156\n",
      "reuratylte eraditional RNN by having another value representing the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories \n",
      "\n",
      "\n",
      "iter :41000, loss:0.243823\n",
      "roperte a ng nand neuterm memoriest RNN but to uex and can also resolve exploding/vanishing graditiod a thad RNN by ianishing gradiandls its alsting the long-te mom rexpM S-te dast to genew rom term t\n",
      "\n",
      "\n",
      "iter :41500, loss:0.220609\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :42000, loss:0.204479\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM su canal netwer value representing the long-term memory, and a of resulve exploding ge palking about RNN a\n",
      "\n",
      "\n",
      "iter :42500, loss:0.192877\n",
      "roperty of RNN is its ability to use the rerm Meories and aandle the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is bet a form of RNN but differs from t\n",
      "\n",
      "\n",
      "iter :43000, loss:0.184198\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :43500, loss:0.177423\n",
      "roperty of RNN is its ability to use texpaty hoTpd the result. LSTM is a form of RNN but differs from traditional RNN by having another valuererere ex ands frodie traditional sure ceorils. LSus andle \n",
      "\n",
      "\n",
      "iter :44000, loss:0.171912\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :44500, loss:0.167253\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :45000, loss:0.163183\n",
      "rom the ponulSRNN ss ngt cand a ne wol fhm Mesolve exploding/vanishing graditiore to handlo tes ats orty of RNN is its ability to use the results from the past to generate a new result. LSTM stands fo\n",
      "\n",
      "\n",
      "iter :45500, loss:0.159530\n",
      "recial tal ale long-term meories and a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term mem\n",
      "\n",
      "\n",
      "iter :46000, loss:0.156185\n",
      "om reoriaty te a nelong-ther value rem resolve exploding/vanishing gradinif ont term Meories. LSTM is better at handling long-term meories and cand network, the special property of RNN is its ability \n",
      "\n",
      "\n",
      "iter :46500, loss:0.153072\n",
      "roperresolve exploding/vanishing gradity to use the results from the past to generate a new result. LSTM is a form of RNN but differs from traditional RNN by having another value representingwty hecua\n",
      "\n",
      "\n",
      "iter :47000, loss:0.150145\n",
      "roperty of RNN is its ability to use the results from the past to generato t-Se com the rast to generate a new recure com re cal networt, avindse com pesple wong-term meories and can also resolve reor\n",
      "\n",
      "\n",
      "iter :47500, loss:0.147369\n",
      "roperty of RNN is its ability to use theravesult. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from tr RNN and LSTM. RNN stands for recurrent neural network, the special p\n",
      "\n",
      "\n",
      "iter :48000, loss:0.144725\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :48500, loss:0.142195\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :49000, loss:0.139769\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :49500, loss:0.137438\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :50000, loss:0.135195\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :50500, loss:0.133036\n",
      "ropertpe long-term meories and a com the past to generate a new result. LSTM stands for Long-Short Term Memore, ave a ngcine long-term meories and crmorithm to handle the results from the past to gene\n",
      "\n",
      "\n",
      "iter :51000, loss:0.130954\n",
      "roperty of RNN is its abilut LST RNN and LSTM is from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :51500, loss:0.128945\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :52000, loss:0.127004\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :52500, loss:0.125129\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :53000, loss:0.123314\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :53500, loss:0.121559\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :54000, loss:0.119856\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :54500, loss:0.118207\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :55000, loss:0.116604\n",
      "roperty of RNN is its abnling the long-term meories and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and cropromprexplong-term meories \n",
      "\n",
      "\n",
      "iter :55500, loss:0.115051\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :56000, loss:0.113539\n",
      "roperty oavist neure cane to havS-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishin\n",
      "\n",
      "\n",
      "iter :56500, loss:0.112072\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :57000, loss:0.110642\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-te ralking about RNN and LSTM. RNN stands for recurrent neural network, the special proper\n",
      "\n",
      "\n",
      "iter :57500, loss:0.109254\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :58000, loss:0.107899\n",
      "roperty of RNN inetty of RNN is its ability to use the results from the past to generate a ne aane to hexplod LSTM. RNN stands for rewulve rom hand al bratyecriol and can and Torg complong-Shorts fori\n",
      "\n",
      "\n",
      "iter :58500, loss:0.106583\n",
      "roperty of RNN is its abilganity te a ne aane recurrent neural perm memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradie RNN is its alsting/vanishing \n",
      "\n",
      "\n",
      "iter :59000, loss:0.105297\n",
      "roperty of RNN is its ability to use the rexploding congl net. ropertpe abiliting a or praditior LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradiere sof and \n",
      "\n",
      "\n",
      "iter :59500, loss:0.104047\n",
      "ropertpe abilithers a forM hexpalong-term momories at hang ant ling the longltnishing gradity ho use the results from the past to genere er loning trmor thad RNN a netw no henuling. LSTM is better at \n",
      "\n",
      "\n",
      "iter :60000, loss:0.102825\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :60500, loss:0.101637\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but  stalds for Long-Short Term Memory, LSTM is a\n",
      "\n",
      "\n",
      "iter :61000, loss:0.100474\n",
      "roperty of RNN is its ability to use the results from the past to generate a ne a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having a\n",
      "\n",
      "\n",
      "iter :61500, loss:0.099343\n",
      "roperty of RNN is its ability to use the results from the past to generate a new resulte exploding/vanishing gradianodand term meor cop aonits algorithm to handle the long/short-term memories. LSTM is\n",
      "\n",
      "\n",
      "iter :62000, loss:0.098235\n",
      "roperte a nes and can llndle the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also rs alstind cong the re\n",
      "\n",
      "\n",
      "iter :62500, loss:0.097157\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :63000, loss:0.096099\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :63500, loss:0.095071\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands forerte resuls. LSTM is bet about RNN and LSTM. RNN stands for rectalingoreccresolve exploding/vani\n",
      "\n",
      "\n",
      "iter :64000, loss:0.094061\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :64500, loss:0.093079\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long/short-term meories and can also resolve exploding/vanishing gradianodand can also resolve \n",
      "\n",
      "\n",
      "iter :65000, loss:0.092113\n",
      "roperty of RNN is its ability to use the results fro, LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term \n",
      "\n",
      "\n",
      "iter :65500, loss:0.091173\n",
      "ropgete a fsolte exploding/vanishing graditio alg crmore complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing\n",
      "\n",
      "\n",
      "iter :66000, loss:0.090248\n",
      "ve long-term meories and can also resolve exploding/vanishing graditiog the special properthe long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM ffio trmprex a\n",
      "\n",
      "\n",
      "iter :66500, loss:0.089346\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :67000, loss:0.088447\n",
      "roperty of RNN is its ability to use the results frottene a ng leutaliting anot andlolt-term meories and can also resolve exploding/vanishing graditifhe pand neural network, the special propercherthe \n",
      "\n",
      "\n",
      "iter :67500, loss:0.087686\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anoura\n",
      "\n",
      "\n",
      "iter :68000, loss:0.086913\n",
      "ropithe speciandl LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradinioN by hexpll bb w resulge eradiand souritong-term meories and a ne gesplve Lo is ity hand\n",
      "\n",
      "\n",
      "iter :68500, loss:0.086136\n",
      "roperty of RNN is its ability to use the result. LSTM is a form of RNN but diRNN but differs from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but d\n",
      "\n",
      "\n",
      "iter :69000, loss:0.085365\n",
      "ruluerer to homplifor RNN and LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-\n",
      "\n",
      "\n",
      "iter :69500, loss:0.084592\n",
      "roperty of RNN is its ability to use the results from the Mandle the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories\n",
      "\n",
      "\n",
      "iter :70000, loss:0.083830\n",
      "ropertyerate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but dif RNN but ere the long-term memory, and a more complex algorithm to handle the long/short-term memories. \n",
      "\n",
      "\n",
      "iter :70500, loss:0.083068\n",
      "rop ferm memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditifhe pandling long-term meories and can also resolve exploding/vanishing gradiano herm er\n",
      "\n",
      "\n",
      "iter :71000, loss:0.082318\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :71500, loss:0.081570\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSRN a net a a Tou frs fropifof RNN is its ability to use the results from the past to generate a new result. L\n",
      "\n",
      "\n",
      "iter :72000, loss:0.080838\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :72500, loss:0.080108\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :73000, loss:0.079396\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :73500, loss:0.078687\n",
      "roperte a nes and can llndle the long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vani\n",
      "\n",
      "\n",
      "iter :74000, loss:0.077996\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs froritandads bet andl LSTM is a form \n",
      "\n",
      "\n",
      "iter :74500, loss:0.077311\n",
      "ropitSp RNN stands for recurrent neural network, the specialk bocradinio algorithm to handlolte exploding/vanishing gradianodherkis fris orm memomorit hang aboutant tan also resolve exploding/vanishin\n",
      "\n",
      "\n",
      "iter :75000, loss:0.076642\n",
      "rom the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a m\n",
      "\n",
      "\n",
      "iter :75500, loss:0.075980\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is forierer tos fof rst tom represolve exploding/vanishing graditi\n",
      "\n",
      "\n",
      "iter :76000, loss:0.075334\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :76500, loss:0.074694\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :77000, loss:0.074071\n",
      "roperte a NN fferthe so uemprerty hadinit a form of RNN is ity to handl nong-ne long-term meories and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-t\n",
      "\n",
      "\n",
      "iter :77500, loss:0.073453\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :78000, loss:0.072851\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form toru a nes and can also resolve exploding/vanishing grad\n",
      "\n",
      "\n",
      "iter :78500, loss:0.072254\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Methe long-term memory, and a more complex algorithm to handle the long/short-t\n",
      "\n",
      "\n",
      "iter :79000, loss:0.071673\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :79500, loss:0.071096\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :80000, loss:0.070535\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of prad a fort com aboding/vanishing gradinif a talgor t\n",
      "\n",
      "\n",
      "iter :80500, loss:0.069977\n",
      "roperte a ge calking gradilif reciandling le wert cong neuther. LSTM is bettal network, the sanelt iandling long-term meories and can also resolve exploding/vanishing graditio al /alkilt neshandls nes\n",
      "\n",
      "\n",
      "iter :81000, loss:0.069433\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :81500, loss:0.068893\n",
      "roperty of RNN is its ability to use the repe ae phe tand thands for recurrent neural network, the special properte a nette pane reprend alug and abut lSTits indty tes alsting/vanishing graditio al pe\n",
      "\n",
      "\n",
      "iter :82000, loss:0.068367\n",
      "ropresertecrity handl LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditiod RNN binit palkal network, the salking about RNN and LSTM. RNN stands LSTM is a for\n",
      "\n",
      "\n",
      "iter :82500, loss:0.067844\n",
      "roperty of RNN is its ability to use the results from the pasolveter als ats algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodi\n",
      "\n",
      "\n",
      "iter :83000, loss:0.067333\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :83500, loss:0.066825\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :84000, loss:0.066329\n",
      "roperty of RNN is its ability to use the results from the past to generate a ne halulangene a opeprodil f RNN is its ability tx alg trmorererty to use the results from the past to generate a new resul\n",
      "\n",
      "\n",
      "iter :84500, loss:0.065834\n",
      "ropithe shert com talking about RNN and LSTM. RNN stands for recurrent neural network, the specierm memories. LSTM is better at handling long-term meories and a more complex algorithm to use the resul\n",
      "\n",
      "\n",
      "iter :85000, loss:0.065350\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by hing gra sher\n",
      "\n",
      "\n",
      "iter :85500, loss:0.064867\n",
      "roperty of RNN is its ability to usolve exploding/vanishing wong-ne a forg, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more\n",
      "\n",
      "\n",
      "iter :86000, loss:0.064394\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :86500, loss:0.063922\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :87000, loss:0.063459\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :87500, loss:0.062996\n",
      "ropitte a noding/vanishing gradidifSradilstito gene a mor to herm teories. LSTM is better at handl LSTM is bethesw mem repre aenure result. LSTM stands for Long-Short Term Memory, LSTM is a form of RN\n",
      "\n",
      "\n",
      "iter :88000, loss:0.062541\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :88500, loss:0.062087\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :89000, loss:0.061640\n",
      "roperty of RNN is its ability to use the results from the past to generate a the lex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve\n",
      "\n",
      "\n",
      "iter :89500, loss:0.061197\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :90000, loss:0.060763\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, Longs RNN but differs from traditional RNN by having another value repr\n",
      "\n",
      "\n",
      "iter :90500, loss:0.060334\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs fop os also resolve exploding/vanishi\n",
      "\n",
      "\n",
      "iter :91000, loss:0.059907\n",
      "roperty of RNN is its ability to use the results from the past to gene ave about RNN and LSTM. RNN stands for recurrent nepresult. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but dif\n",
      "\n",
      "\n",
      "iter :91500, loss:0.059483\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Merm merm memories. LSTM is better at handling long-term meories and can also r\n",
      "\n",
      "\n",
      "iter :92000, loss:0.059060\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :92500, loss:0.058644\n",
      "roperte a ng net andl ngl nethe sp copher. LSTM is aes and almpresult. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representi\n",
      "\n",
      "\n",
      "iter :93000, loss:0.058235\n",
      "roperty of RNN is its ability to use the results from the past to gs ind LSTM is bet a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by hav\n",
      "\n",
      "\n",
      "iter :93500, loss:0.057838\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :94000, loss:0.057443\n",
      "roperty of RNN bu i so tor nette talking about RNN and LSTM. RNN stands for Long-Short Term Memory, LSTM is a form of RNN but dilk, the so tal networ wilg is ity the lom texploding/vanishing gradier n\n",
      "\n",
      "\n",
      "iter :94500, loss:3.026680\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN butalt. LSTM stands for Long-Short Term Memory, L\n",
      "\n",
      "\n",
      "iter :95000, loss:2.405224\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Shands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN \n",
      "\n",
      "\n",
      "iter :95500, loss:1.576654\n",
      "roperty of RNN is its ability to use the results from the past to generate a new resu te pand a mererete a ag ate aang com rast to generate a new rexploding/vanishing gradianales RNN stands for recurr\n",
      "\n",
      "\n",
      "iter :96000, loss:1.313158\n",
      "ropeese to handle the long-term memories. LSTM is better at handling long-term meories and camorecurrent ne a new result. LSTM stands forerte a ngw resu te resulu. RNN stands for recurrent neural netw\n",
      "\n",
      "\n",
      "iter :96500, loss:0.986690\n",
      "roperresolts from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term m\n",
      "\n",
      "\n",
      "iter :97000, loss:0.865427\n",
      "roperty of RNN is its ability to use the resulof RNN is its ability to use the results from the pastito geneia a nett no M sulol the special property of RNN is its ability to use the results from the \n",
      "\n",
      "\n",
      "iter :97500, loss:0.583466\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :98000, loss:0.405298\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :98500, loss:0.293429\n",
      "ropgete avalinitSplolk, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs \n",
      "\n",
      "\n",
      "iter :99000, loss:0.222446\n",
      "ropithe speciandle the results from the past to generate a new result. LSTM stands forerte pand a merere a ne ral network, the specialkint long-term meories and can also resolve exploding/vanishing gr\n",
      "\n",
      "\n",
      "iter :99500, loss:0.177025\n",
      "roperty of RNN is its ability to use the results from the pasm re handling long-term meories and can also resolve exploding/vanishing gradiano tong van als iem mempmories. LSTM is better at handling l\n",
      "\n",
      "\n",
      "iter :100000, loss:0.147463\n",
      "ropgecertesolve exploding/vanishing gradiano renerrepe wo usaent nouw nothere mom repre anouhertyere expal She long-term memory, and a more complex algorithm to handle the long/short-term memories. LS\n",
      "\n",
      "\n",
      "iter :100500, loss:0.127933\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :101000, loss:0.114752\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :101500, loss:0.105614\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :102000, loss:0.099058\n",
      "rope comuresult. LSTM stands for Long-Short Term Memory, LSTM is a form ofSRN abit n ngtty te rong-term meories and can also resolve explo hand can also resolve exploding/vanishing graditiog the long-\n",
      "\n",
      "\n",
      "iter :102500, loss:0.094170\n",
      "roperte a ng wing nem alndles a f RNN by having another value representing the long-term memory, and a more complex algorithm to handlolt Teser yecraving abowil pve erm memories. LSTM is better at han\n",
      "\n",
      "\n",
      "iter :103000, loss:0.090379\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :103500, loss:0.087326\n",
      "roperty of RNN is its ability to use the results from the wong-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can al\n",
      "\n",
      "\n",
      "iter :104000, loss:0.084778\n",
      "roperte alvind foru te morererty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditi\n",
      "\n",
      "\n",
      "iter :104500, loss:0.082588\n",
      "ropgese the abom the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term me\n",
      "\n",
      "\n",
      "iter :105000, loss:0.080656\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :105500, loss:0.078922\n",
      "roperte a neshing graditandlints and renural networt. LSTM is a form of RNN but differs from traditional RNN by having another value representing the long-term memory, and a more complex algorithm to \n",
      "\n",
      "\n",
      "iter :106000, loss:0.077339\n",
      "ropithe speciandl frecia T-Su radidifferiit a spforrese the results from the past tol -Shorts from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but \n",
      "\n",
      "\n",
      "iter :106500, loss:0.075882\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :107000, loss:0.074528\n",
      "roperte a ne handl ngtterm meor yority ge more complex algorithm to handle the lenulinvad a ng the long-term meories and can also resolve exploding/vanishing graditiodaliting ano to handle the long/sh\n",
      "\n",
      "\n",
      "iter :107500, loss:0.073264\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by talking about\n",
      "\n",
      "\n",
      "iter :108000, loss:0.072077\n",
      "roperte a ne gecral network, the special prouravishing gradity ha ibectrmore complom ther at long-term meories and can also resolve exploding/vanishing gradiano tong the special property of RNN is its\n",
      "\n",
      "\n",
      "iter :108500, loss:0.070960\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :109000, loss:0.069903\n",
      "roperte a ne gecral network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but dif\n",
      "\n",
      "\n",
      "iter :109500, loss:0.068902\n",
      "roperte alva ferm memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditiodal network, the special long-ne aane recurrent neural network, the special pr\n",
      "\n",
      "\n",
      "iter :110000, loss:0.067949\n",
      "ropertp of RNN stands for recurrent neural network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM i\n",
      "\n",
      "\n",
      "iter :110500, loss:0.067041\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :111000, loss:0.066172\n",
      "ropgese the long/short-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradity t\n",
      "\n",
      "\n",
      "iter :111500, loss:0.065339\n",
      "roperte a nes and can also resolve exploding/vanishing graditio g neuraliliffert mom repe ave so werthing gradity to use the results from the past to generate a new result. LSTM stands for Long-Short \n",
      "\n",
      "\n",
      "iter :112000, loss:0.064539\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :112500, loss:0.063769\n",
      "roperte a nes and corM istong-term memories. LSTM is better a ne werttr RN gung to rene aeoritte mom rexual networ thads for LSTM stands from the past to generate a new result. LSTM stands for Long-Sh\n",
      "\n",
      "\n",
      "iter :113000, loss:0.063026\n",
      "roperte a nes and corg to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value\n",
      "\n",
      "\n",
      "iter :113500, loss:0.062310\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Meories. LSTM is better at handling long-term meories and can also resolve expl\n",
      "\n",
      "\n",
      "iter :114000, loss:0.061618\n",
      "roperty of RNN is its ab NN algoratte talkint long-term meories and can also resolve exploding/vanishing gradidifSradils ies ability to use the results from the past to generate a new result. LSTM sta\n",
      "\n",
      "\n",
      "iter :114500, loss:0.060951\n",
      "roperte a ne pand cim tesults from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representin\n",
      "\n",
      "\n",
      "iter :115000, loss:0.060307\n",
      "ropgese the long/short-term memories. LSTM is better at handling long-term meories and can also resolve explodhing a more complex algorithm to handle the long/short-term memories. LSTM is better at ha\n",
      "\n",
      "\n",
      "iter :115500, loss:0.059685\n",
      "roperte a ng tal aenuane aem repterty of RNN is its ability to use the results from the pasults from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a forrse teso te\n",
      "\n",
      "\n",
      "iter :116000, loss:0.059083\n",
      "roperte a ne handl forthe long-term memory, and a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing g\n",
      "\n",
      "\n",
      "iter :116500, loss:0.058499\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :117000, loss:0.057934\n",
      "ropitte mom recurrene a more complex algorithm to handle the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditiodalsting wong-term \n",
      "\n",
      "\n",
      "iter :117500, loss:0.057385\n",
      "roperty of RNN is its abut LSd ng/vanishing graditithe special property of RNN is its ability to use the results from the past to generate a new resu te past al lerty, its alst LSTM. RNN stands for re\n",
      "\n",
      "\n",
      "iter :118000, loss:0.056852\n",
      "roperte arit-teravaling getterm momoralve the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value repr\n",
      "\n",
      "\n",
      "iter :118500, loss:0.056334\n",
      "ropgese the long/short-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditio algorithm to handle the long/short-term memories. LSTM is better at\n",
      "\n",
      "\n",
      "iter :119000, loss:0.055830\n",
      "roperty of RNN is its ab hing net ne presuls. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gradiaif a neterial property of RNN is its ability to use the result\n",
      "\n",
      "\n",
      "iter :119500, loss:0.055339\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :120000, loss:0.054860\n",
      "roperte a ne hal network, the special property of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differ\n",
      "\n",
      "\n",
      "iter :120500, loss:0.054394\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :121000, loss:0.053939\n",
      "roperchal RNN b trat leciandl ne wort-term meories and can also resolve exploding/vanishing gradity to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, L\n",
      "\n",
      "\n",
      "iter :121500, loss:0.053495\n",
      "roperty of RNN is its ability to use the results frout nodw c cerm memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing graditiod LSTM. RNN stands for recurr\n",
      "\n",
      "\n",
      "iter :122000, loss:0.053061\n",
      "roperte a ng talkint aang can llndle the long-term memories. LSTM is better at handling long-term meories and can also resolve exploding/vanishing gres and LSTM. RNN stands for recurrent neural networ\n",
      "\n",
      "\n",
      "iter :122500, loss:0.052637\n",
      "ropithe speciandle the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having another value representi\n",
      "\n",
      "\n",
      "iter :123000, loss:0.052223\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of R\n",
      "\n",
      "\n",
      "iter :123500, loss:0.051818\n",
      "roperte a ng tal aendling long-term meories and can also resolve exploding/vanishing graditiod LSTM. RNN stands for recurrent neural network, the special property of RNN is its ability to use the resu\n",
      "\n",
      "\n",
      "iter :124000, loss:0.051421\n",
      "ropithe abot l fho Torm complex algo tha des at handling long-term meorits and can also resolve exploding/vanishing graditional RNN by handl its ability to use the results from the past to generate a \n",
      "\n",
      "\n",
      "iter :124500, loss:0.051032\n",
      "roperty of RNN is its ability to use the results from the past to generate a new result. LSTM stands for Long-Short Term Memory, LSTM is a form of RNN but differs from traditional RNN by having anothe\n",
      "\n",
      "\n",
      "iter :125000, loss:0.050652\n",
      "roperte a nes and corit mom memories. LSTM is bectalkint lanity to use the results from the past to generate a form of RNN but differs from tragity to use the results from the past to generate a new r\n",
      "\n",
      "\n",
      "iter :125500, loss:0.050279\n",
      "Prediction 1 (memory): memory  remmoralithes and a the results from the past to \n",
      "Prediction 2 (LSTM): LSTM Splut bbo tomore complex algorithm to handle the l\n",
      "Prediction 3 (complex): complex rmemorecrm cis atn algorithm to handle the results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"input.txt\", seq_length)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "rnn.train(data_reader)\n",
    "\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'memory ', 50)\n",
    "print(f\"Prediction 1 (memory): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'LSTM ', 50)\n",
    "print(f\"Prediction 2 (LSTM): {predicted}\")\n",
    "\n",
    "predicted = rnn.predict(data_reader, 'complex ', 50)\n",
    "print(f\"Prediction 3 (complex): {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b558ab-c271-4c98-ba87-675dde6faf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Special: LSTM is better at handling long-term meories ong verty of RNN is its ability to use the results\n"
     ]
    }
   ],
   "source": [
    "predicted = rnn.predict(data_reader, 'LSTM is better at handling long-term meories ', 50)\n",
    "print(f\"Prediction Special: {predicted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e3ea0-3985-414b-bea1-ae3e58a01e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
